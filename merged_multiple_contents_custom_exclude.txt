====================
文件: .\.deepwiki.yml
====================

# FILE: .deepwiki.yml
# Deep-Wiki Configuration for Awesome-LLM-ZeroToScratch

# 项目名称，将显示在Deep-Wiki页面上
name: "Awesome LLM ZeroToScratch"

# 项目的简短描述
description: "An end-to-end, code-driven tutorial for pre-training and fine-tuning LLMs and VLMs, designed to be both nanny-level accessible and industrial-grade robust."

# 定义文档的根文件，通常是README
root: "README.md"

# 指定代码和文档的路径，帮助Deep-Wiki聚焦于最重要的部分
sources:
  - path: "src/"
    description: "Core source code for models, trainers, inference, and evaluation logic."
  - path: "docs/"
    description: "Detailed step-by-step tutorial documentation."
  - path: "configs/"
    description: "YAML configuration files for all experiments (data, models, training)."
  - path: "scripts/"
    description: "Shell scripts to launch training and inference jobs."

# 指定需要排除的路径，避免扫描不必要的文件
exclude:
  - "data/"
  - "checkpoints/"
  - "wandb/"
  - ".idea/"
  - ".vscode/"
  - "__pycache__/"
  - "*.egg-info/"
  - "/autodl-tmp/"

# (可选) 为项目添加标签，方便分类和搜索
tags:
  - "LLM"
  - "VLM"
  - "Pre-training"
  - "Fine-tuning"
  - "PyTorch"
  - "DeepSpeed"
  - "Tutorial"

====================
文件: .\create_project_structure.py
====================

# FILE: create_project_structure.py
"""
Bedrock Protocol v: Absolute Provenance Edition
Project: Awesome-LLM-ZeroToScratch
Task: Automated generation of the complete project directory and file skeleton.

This script is a self-contained utility to bootstrap the project structure.
It creates all necessary directories and placeholder files as defined in the
Bedrock Workflow, ensuring a clean, consistent, and reproducible starting point.

This script embodies the Mandate of Intentionality: its sole, deliberate
purpose is to construct the project's foundational scaffold.
"""

import os
import sys
from pathlib import Path
from typing import List

# --- CORE CONFIGURATION ---

# Mandate of Intentionality: The root directory name is explicitly defined.
ROOT_DIR_NAME: str = "Awesome-LLM-ZeroToScratch"

# Mandate of Structural Integrity: The entire project structure is defined
# declaratively. Each entry is a deliberate component of the grand design.
PROJECT_STRUCTURE: List[str] = [
    ".gitignore",
    "README.md",
    "LICENSE",  # Added LICENSE here to ensure it's created.
    "requirements.txt",
    "setup.sh",
    "configs/data/text_pretrain.yaml",
    "configs/data/vlm_pretrain.yaml",
    "configs/model/0.5B_dense.yaml",
    "configs/model/0.8B_moe.yaml",
    "configs/model/llama2-7b.yaml",
    "configs/training/pretrain_llm.yaml",
    "configs/training/finetune_sft.yaml",
    "configs/training/finetune_dpo.yaml",
    "data_processing/download_and_reproduce.py",
    "data_processing/process_text.py",
    "data_processing/process_vlm.py",
    "data_processing/build_tokenizer.py",
    "data_processing/__init__.py",
    "src/__init__.py",
    "src/models/__init__.py",
    "src/models/attention/__init__.py",
    "src/models/attention/standard_attention.py",
    "src/models/attention/flash_attention.py",
    "src/models/ffn.py",
    "src/models/moe.py",
    "src/models/language_model.py",
    "src/trainers/__init__.py",
    "src/trainers/pretrain_trainer.py",
    "src/trainers/sft_trainer.py",
    "src/trainers/dpo_trainer.py",
    "src/utils/__init__.py",
    "src/utils/data_formatters.py",  # Added for data formatting utils
    "src/inference/__init__.py",  # Added for completeness in structure generation
    "src/inference/inference.py",  # Added for completeness in structure generation
    "src/evaluation/__init__.py",  # Added for completeness in structure generation
    "src/evaluation/evaluate_llm.py",  # Added for completeness in structure generation
    "scripts/run_pretrain.sh",
    "scripts/run_sft.sh",
    "scripts/run_dpo.sh",
    "scripts/run_inference.sh",  # Added for completeness in structure generation
    "scripts/run_evaluation.sh",  # Added for completeness in structure generation
    "docs/01_environment_setup.md",
    "docs/02_data_pipeline.md",
    "docs/03_track_a_pretraining.md",
    "docs/04_track_b_finetuning.md",
    "docs/05_deployment_and_evaluation.md",
]

# Mandate of Zero Ambiguity: Content for critical files is defined explicitly,
# leaving no room for misinterpretation.
GITIGNORE_CONTENT: str = """
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDEs
.idea/
.vscode/
*.swp
*.sublime-workspace
*.sublime-project

# AutoDL / Cloud specific
.autodl/
/autodl-tmp/
/autodl-fs/

# Jupyter Notebook
.ipynb_checkpoints

# Data files
# Raw, processed, or tokenized data should not be in git.
# Use download scripts and mount network storage instead.
*.bin
*.pt
*.arrow
*.parquet
/data/
/datasets/
/wudao_200g/
*.jsonl

# Trained models & logs
# Checkpoints, models, and logs are artifacts, not source code.
/models/
/checkpoints/
/runs/
/logs/

# W&B local artifacts
/wandb/

# Tokenizer files
*.model
*.vocab
"""

README_CONTENT: str = """
# Awesome-LLM-ZeroToScratch

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An end-to-end, code-driven tutorial for pre-training and fine-tuning
Large Language Models (LLMs) and Vision Language Models (VLMs).

This repository is built following the **Bedrock Protocol v: Absolute Provenance Edition**,
ensuring theoretical and practical robustness, clarity, and reproducibility.

## Core Philosophy

This project is designed with a dual-purpose philosophy:

*   **保姆级 (Nanny-Level):** Providing meticulously detailed, step-by-step guidance with ready-to-run code and commands, specifically tailored for platforms like AutoDL. It's designed for users who have a theoretical understanding but lack hands-on experience with large-scale model training.
*   **工业级 (Industrial-Grade):** The codebase is highly modular, configuration-driven (using YAML), and built for scalability. It clearly demonstrates the key architectural and strategic adjustments needed to scale from a small 0.5B parameter model to a 70B+ giant (e.g., parallelism strategies, memory optimization).

## Project Structure

The repository is organized into a clear, modular structure:
Use code with caution.
Python
/Awesome-LLM-ZeroToScratch
|-- README.md
|-- LICENSE
|-- requirements.txt
|-- setup.sh
|-- configs/ # YAML configs for data, models, and training
|-- data_processing/ # Scripts for data download, processing, tokenization
|-- docs/ # Detailed markdown tutorials
|-- scripts/ # Shell scripts to launch training jobs
`-- src/ # Core, modular source code for models and trainers
Generated code
## Getting Started

To begin your journey, please start with the first document which details the complete environment setup process on the AutoDL platform.

**➡️ Start here: [docs/01_environment_setup.md](./docs/01_environment_setup.md)**

## License

This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.
"""

LICENSE_CONTENT: str = """
MIT License

Copyright (c) 2025 Bedrock

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

DATA_FORMATTERS_CONTENT: str = """
# FILE: src/utils/data_formatters.py
\"\"\"
Bedrock Protocol: Centralized Data Formatting Utilities.
This module provides reusable functions for formatting datasets for various
training paradigms (SFT, DPO, ORPO, etc.), promoting code reuse and
decoupling trainers.
\"\"\"
from transformers import AutoTokenizer

def format_dpo_dataset_factory(tokenizer: AutoTokenizer):
    \"\"\"
    Creates a formatting function for preference datasets (DPO, ORPO, RM).
    This function is designed to work with datasets that follow the TRL
    chat format, where 'chosen' and 'rejected' fields contain a list of
    message dictionaries.

    Args:
        tokenizer: The tokenizer to use for applying the chat template.

    Returns:
        A function that takes a dataset example and returns a dictionary
        with 'prompt', 'chosen', and 'rejected' as formatted strings.
    \"\"\"
    def format_dpo_dataset(example: dict) -> dict:
        # The prompt is the conversation history *before* the final turn.
        prompt_messages = example['chosen'][:-1]
        # The chosen response is the full conversation including the final preferred response.
        chosen_messages = example['chosen']
        # The rejected response is the full conversation including the final rejected response.
        rejected_messages = example['rejected']

        prompt_str = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)
        chosen_str = tokenizer.apply_chat_template(chosen_messages, tokenize=False)
        rejected_str = tokenizer.apply_chat_template(rejected_messages, tokenize=False)

        return {
            "prompt": prompt_str,
            "chosen": chosen_str,
            "rejected": rejected_str,
        }
    return format_dpo_dataset
"""


def main() -> None:
    """
    Main execution function to generate the project skeleton.
    """
    print("--- Bedrock Protocol: Initiating Project Scaffolding ---")

    # Mandate of Proactive Defense: Check for existing directory to prevent
    # accidental data loss.
    root_path = Path(ROOT_DIR_NAME)
    if root_path.exists():
        print(f"Error: Directory '{ROOT_DIR_NAME}' already exists.")
        print("Scaffolding aborted to prevent overwriting existing work.")
        sys.exit(1)

    print(f"Creating root directory: {root_path}")
    root_path.mkdir()

    # Create all files and directories
    for file_path_str in PROJECT_STRUCTURE:
        # Mandate of Type Purity: Use pathlib for robust, OS-agnostic path handling.
        file_path = root_path / file_path_str

        # Ensure parent directories exist
        print(f"  - Ensuring path: {file_path.parent}")
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # Create empty file
        print(f"    - Creating file: {file_path}")
        file_path.touch()

    # --- POPULATE SPECIAL FILES ---
    # Mandate of Zero Ambiguity: Write predefined content to essential files.

    print("\nPopulating special files with initial content...")

    # Write .gitignore
    gitignore_path = root_path / ".gitignore"
    gitignore_path.write_text(GITIGNORE_CONTENT.strip())
    print(f"  - Wrote content to {gitignore_path}")

    # Write README.md
    readme_path = root_path / "README.md"
    readme_path.write_text(README_CONTENT.strip())
    print(f"  - Wrote content to {readme_path}")

    # Write LICENSE
    license_path = root_path / "LICENSE"
    license_path.write_text(LICENSE_CONTENT.strip())
    print(f"  - Wrote content to {license_path}")

    # Write data_formatters.py
    data_formatters_path = root_path / "src/utils/data_formatters.py"
    data_formatters_path.write_text(DATA_FORMATTERS_CONTENT.strip())
    print(f"  - Wrote content to {data_formatters_path}")

    # Fill __init__.py files to make packages importable
    for file_path_str in PROJECT_STRUCTURE:
        if file_path_str.endswith("__init__.py"):
            init_path = root_path / file_path_str
            # Mandate of Intentionality: Add a clear comment to __init__.py
            init_path.write_text("# Bedrock: This file makes the directory a Python package.\n")
            print(f"  - Populated package marker in {init_path}")

    print("\n--- Project Scaffolding Complete ---")
    print(f"The project structure for '{ROOT_DIR_NAME}' has been successfully created.")
    print("Each file is a testament to our commitment to absolute provenance.")


if __name__ == "__main__":
    # The script is invoked, beginning the process of creation.
    main()

# END OF FILE: create_project_structure.py

====================
文件: .\datecheck.py
====================

# FILE: diagnose_dataset.py
"""
Bedrock Protocol: Dataset Structure Diagnostic Tool.

This script loads a specified dataset from Hugging Face and prints the
structure of the first example. It's a simple but powerful tool to
verify data formats before writing complex processing logic.
"""

from datasets import load_dataset
import sys


def diagnose(dataset_name: str, split: str = "train", num_samples_to_show: int = 1):
    """
    Loads a dataset and prints detailed information about the first few samples.

    Args:
        dataset_name: The name of the dataset on the Hugging Face Hub.
        split: The dataset split to inspect (e.g., "train", "test").
        num_samples_to_show: The number of samples to print.
    """
    print(f"--- [Bedrock] Starting Dataset Diagnosis for: {dataset_name} ---")

    try:
        # Load the dataset
        print(f"\nAttempting to load dataset '{dataset_name}', split '{split}'...")
        dataset = load_dataset(dataset_name, split=split)
        print("Dataset loaded successfully.")

        if len(dataset) < num_samples_to_show:
            print(
                f"Warning: Dataset has only {len(dataset)} samples, which is less than requested {num_samples_to_show}.")
            num_samples_to_show = len(dataset)

        print(f"\nInspecting the first {num_samples_to_show} sample(s)...")
        print("=" * 50)

        for i in range(num_samples_to_show):
            # Get the first sample
            sample = dataset[i]

            print(f"\n--- SAMPLE #{i + 1} ---")

            # List all keys in the sample
            print(f"Available keys: {list(sample.keys())}")

            # Inspect the fields relevant to DPO
            dpo_keys = ["prompt", "chosen", "rejected"]

            for key in dpo_keys:
                if key in sample:
                    value = sample[key]
                    value_type = type(value)

                    print(f"\nField: '{key}'")
                    print(f"  - Type: {value_type}")
                    print(f"  - Content:\n---\n{value}\n---")
                else:
                    print(f"\nField: '{key}' - NOT FOUND IN SAMPLE.")

            print("=" * 50)

    except Exception as e:
        print(f"\nAn error occurred: {e}")
        print("Please check if the dataset name and split are correct.")


if __name__ == "__main__":
    # You can change the dataset name here for other diagnostics
    # Defaulting to the one causing issues in DPO.
    target_dataset = "trl-internal-testing/hh-rlhf-trl-style"

    if len(sys.argv) > 1:
        target_dataset = sys.argv[1]

    diagnose(target_dataset)

# END OF FILE: diagnose_dataset.py

====================
文件: .\getAllCode.py
====================

import os


def merge_files_to_txt(folder_path, target_extensions, output_filename, excluded_folders=None):
    """
    将指定文件夹（包括子文件夹）下所有指定后缀文件的内容合并到一个 TXT 文件中。

    参数:
    folder_path (str): 要遍历的文件夹路径。
    target_extensions (list): 要查找的文件的后缀名列表（例如：['.txt', '.py', '.md']）。
    output_filename (str): 输出的 TXT 文件的名称。
    excluded_folders (list, optional): 要跳过的文件夹名称列表。默认为 None (不跳过任何文件夹)。
    """
    if excluded_folders is None:
        excluded_folders = []  # 如果没有提供，则初始化为空列表

    try:
        # 确保每个目标后缀都以 '.' 开头
        # 使用列表推导式更简洁地处理
        processed_extensions = [ext if ext.startswith('.') else '.' + ext for ext in target_extensions]

        # 获取输出文件的完整路径
        output_filepath = os.path.join(folder_path, output_filename)

        # 记录实际跳过的文件夹
        actual_excluded_dirs = []

        with open(output_filepath, 'w', encoding='utf-8') as outfile:
            # os.walk() 会遍历指定目录下的所有文件夹和文件
            # dirnames 是当前目录下的子目录列表
            for dirpath, dirnames, filenames in os.walk(folder_path):
                # ==========================================================
                # 关键修改：根据 excluded_folders 参数排除文件夹
                # 需要在这里创建一个 dirnames 的副本，因为直接修改 dirnames 列表会影响 os.walk 的遍历行为
                # 或者更直接的方式是修改 dirnames 自身，让 os.walk 不再进入这些目录
                # 我们直接从 dirnames 中移除，以跳过这些目录

                # 创建一个需要移除的目录列表，避免在遍历的同时修改列表
                dirs_to_remove = [d for d in dirnames if d in excluded_folders]
                for d in dirs_to_remove:
                    dirnames.remove(d)
                    actual_excluded_dirs.append(os.path.join(dirpath, d))  # 记录完整路径

                # ==========================================================

                for filename in filenames:
                    # 检查文件是否以任一目标后缀结尾
                    # 同时确保空后缀（无后缀文件）也能被正确处理
                    is_match = False
                    if '' in processed_extensions and '.' not in filename:  # 如果包含空后缀且文件无后缀
                        is_match = True
                    elif any(filename.endswith(ext) for ext in processed_extensions):  # 如果有匹配的后缀
                        is_match = True

                    if is_match:
                        filepath = os.path.join(dirpath, filename)

                        # 写入文件分隔符和文件名，方便区分不同文件的内容
                        outfile.write(f'{"=" * 20}\n')
                        outfile.write(f'文件: {filepath}\n')
                        outfile.write(f'{"=" * 20}\n\n')

                        try:
                            with open(filepath, 'r', encoding='utf-8') as infile:
                                outfile.write(infile.read())
                                outfile.write('\n\n')  # 在每个文件内容后添加空行，增加可读性
                        except Exception as e:
                            outfile.write(f'读取文件 {filepath} 时出错: {e}\n\n')

        print(f'成功！所有 {", ".join(processed_extensions)} 文件的内容已合并到 {output_filepath}')
        if actual_excluded_dirs:
            print(f'注意：已跳过以下文件夹及其内容:')
            for d in set(actual_excluded_dirs):  # 使用set去重，避免打印重复路径
                print(f'- {d}')
        else:
            print(f'没有指定或实际跳过任何文件夹。')

    except FileNotFoundError:
        print(f'错误：文件夹 "{folder_path}" 不存在。')
    except Exception as e:
        print(f'发生未知错误: {e}')


# --- 使用示例 ---
if __name__ == '__main__':
    # 1. 设置要遍历的文件夹路径
    source_folder = '.'  # '.' 表示当前脚本所在的文件夹

    # 2. 设置要查找的文件后缀名列表
    file_extensions = ['.py', 'sh', 'yaml', 'yml']

    # 3. 设置输出的 TXT 文件名
    output_file = 'merged_multiple_contents_custom_exclude.txt'

    # 4. 设置要跳过的文件夹名称列表
    #    这里可以添加任何你不想遍历的文件夹名称
    folders_to_exclude = ['.venv', '__pycache__', 'node_modules', '.git']

    # 5. 调用函数
    merge_files_to_txt(source_folder, file_extensions, output_file, folders_to_exclude)

====================
文件: .\setup.sh
====================

#!/bin/bash

# ==============================================================================
# Bedrock Protocol: One-Click Environment Setup Script
# ==============================================================================
# This script provides a deterministic, robust, and fast setup for the project
# environment. It embodies the "保姆级" (nanny-level) philosophy by handling
# potential issues like network instability and missing tools.
#
# The script will exit immediately if any command fails.
set -e

echo "--- [Bedrock] Starting Environment Setup ---"

# --- Helper Function to Ensure Command Exists ---
# Before we use any tool, we make sure it's installed.
ensure_command() {
    if ! command -v "$1" &> /dev/null; then
        echo "--- Command '$1' not found. Attempting to install..."
        # 修正：根据不同的命令，安装包名可能不同
        if [ "$1" == "pip" ]; then
            apt-get update -y && apt-get install -y python3-pip
        elif [ "$1" == "uv" ]; then
            # uv 会在后面通过 pip 安装，这里不直接 apt-get
            echo "--- 'uv' will be installed via pip in the next step."
        elif [ "$1" == "aria2c" ]; then
            apt-get update -y && apt-get install -y aria2
        else
            apt-get update -y && apt-get install -y "$2" # 尝试使用提供的第二个参数作为包名
        fi
        echo "--- '$1' installed successfully."
    else
        echo "--- Command '$1' is already installed."
    fi
}


# --- Step 1: Verify and Install Essential Tools ---
echo "[1/5] Verifying essential tools (pip, aria2c)..."
ensure_command pip python3-pip # 确保 pip 已安装
ensure_command aria2c aria2 # 确保 aria2c 已安装

# --- Step 2: Install/Upgrade uv using pip ---
# We use pip for the initial install of uv, our high-speed package manager.
# A mirror is configured first to speed up this step itself.
echo "[2/5] Installing/Upgrading uv..."
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip install -U uv
echo "--- uv installed/updated successfully."


# --- Step 3: Install All Dependencies EXCEPT Flash Attention ---
# We install everything else first. flash-attn is handled separately
# due to its large size and potential for network-related download failures.
echo "[3/5] Installing all packages from requirements.txt (except flash-attn)..."
# Define mirror arguments for uv. This is crucial for speed.
# NOTE: Ensure 'cu121' matches your target environment's CUDA version.
UV_MIRROR_ARGS="--extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple --extra-index-url https://download.pytorch.org/whl/cu121"

# 我们使用 uv 从 requirements.txt 安装，比 pip 快得多。
# 我们明确排除 flash-attn，在下一步单独处理。
# 修正：增加 --index-strategy unsafe-best-match 参数
uv pip install -r requirements.txt $UV_MIRROR_ARGS --index-strategy unsafe-best-match
echo "--- Core dependencies installed successfully."

# --- [CRITICAL FIX] Step 3.5: Upgrade TRL to get latest features like GRPO ---
echo "[3.5/5] Upgrading TRL to the latest version..."
uv pip install -U trl
echo "--- TRL upgraded successfully."


# --- Step 4: Robustly Download Flash Attention ---
# This is the most critical step. We download the large pre-compiled wheel
# file using a robust downloader BEFORE trying to install it. This is the
# key to overcoming network timeouts.
echo "[4/5] Robustly downloading Flash Attention wheel..."
# This URL is for torch 2.3.0 and CUDA 12.1/12.2/12.4. CHANGE IF YOUR ENV IS DIFFERENT.
# 修正：确保 URL 匹配 PyTorch 2.3.0 + CUDA 12.1
FLASH_ATTN_WHEEL_URL="https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu121torch2.3cxx11abiTRUE-cp310-cp310-linux_x86_64.whl"
FLASH_ATTN_WHEEL_FILE=$(basename "$FLASH_ATTN_WHEEL_URL")

# Use aria2c for fast, multi-connection download.
# The -c flag allows resuming if the download is interrupted.
aria2c -c -x 16 -s 16 -o "$FLASH_ATTN_WHEEL_FILE" "$FLASH_ATTN_WHEEL_URL"
echo "--- Flash Attention wheel downloaded successfully."


# --- Step 5: Install Flash Attention From Local File ---
# Now that the file is local, installation is instant and immune to network issues.
echo "[5/5] Installing Flash Attention from local wheel file..."
uv pip install "$FLASH_ATTN_WHEEL_FILE"
echo "--- Flash Attention installed successfully."


# --- Finalization ---
echo ""
echo "================================================="
echo "✅ [Bedrock] Environment Setup Complete!"
echo "All packages, including the tricky ones, are now installed."
echo "You are ready to proceed with your project."
echo "================================================="

# END OF FILE

====================
文件: .\configs\data\text_pretrain.yaml
====================

# FILE: configs/data/text_pretrain.yaml

# Bedrock 协议：文本预训练数据管道的集中配置文件。
# 所有与数据相关的路径和超参数都写在这里，体现“工业级”配置即代码的理念。
# 本文件只关心“数据从哪里来、要存到哪里、怎么加工”，与模型结构无关。

# ── 数据集指定 ───────────────────────────────
# Hugging Face 上的数据集 ID（含版本号，确保可复现）。
dataset_name: wikitext
dataset_config_name: wikitext-2-raw-v1

# ── 输出与缓存目录 ──────────────────────────
# 生产环境请用绝对路径，这里用相对路径只是方便在不同机器上开箱即用。
# 若跑在 AutoDL 等云实例，建议把 base_output_dir 指向 /root/autodl-tmp 这类网络挂载盘。
base_output_dir: ./data

# 原始数据下载后落盘的位置。
raw_data_dir: ${base_output_dir}/raw/wikitext-2-raw-v1

# 清洗/去重/过滤后的正式数据集存放目录。
processed_data_dir: ${base_output_dir}/processed/wikitext-2-raw-v1

# 训练好的 SentencePiece 分词器保存路径。
tokenizer_output_path: ${base_output_dir}/tokenizers/wikitext_spm

# ── 数据处理细节 ───────────────────────────
# Hugging Face 数据集中真正包含文本的列名。
text_column: text

# ── 分词器训练超参数 ────────────────────────
# 训练分词器时所用“纯文本大文件”的路径（由 processed_data_dir 生成）。
tokenizer_training_corpus: ${processed_data_dir}/corpus.txt
vocab_size: 8000            # 演示用小词表，真正大模型常用 32k/50k/100k。
model_type: unigram         # SentencePiece 官方推荐，兼顾压缩率与解码速度。
character_coverage: 1.0     # 英文场景 1.0 即可；多语言场景通常设 0.9995。

# ── 调试/快速验证专用 ──────────────────────
# 如果 >0，则只加载数据集的前 N 条样本，方便冒烟测试；设为 0 就全量加载。
dataset_subset_size: 1000

# END OF FILE: configs/data/text_pretrain.yaml

====================
文件: .\configs\data\vlm_pretrain.yaml
====================

# FILE: configs/data/vlm_pretrain.yaml
# Bedrock Protocol: Configuration for the VLM pre-training data pipeline.

# --- Dataset Specification ---
# 更换为完全公开且易于加载的 COCO 验证集子集
dataset_name: "lmms-lab/COCO-Caption2017" # 更换为 lmms-lab/COCO-Caption2017
# 使用一个较小的样本数进行演示，以确保快速运行
num_samples_to_process: 3 # 增加到500个样本，更具代表性

# --- Output & Cache Directories ---
base_output_dir: ./data

# Subdirectory for the raw downloaded dataset.
raw_data_dir: ${base_output_dir}/raw/coco_val_demo # 更改目录名以匹配新数据集

# Subdirectory for the processed and cleaned dataset.
processed_data_dir: ${base_output_dir}/processed/coco_val_demo # 更改目录名

# --- Data Processing Parameters ---
# 根据 nielsr/coco-2017-val 数据集调整列名
image_column: image
text_column: caption # 该数据集的文本列名为 'caption'

# --- Conceptual Data Distillation ---
distillation:
  enabled: false # 默认为关闭，防止产生 API 费用
  max_prompts: 5
  prompt_template: "Provide a detailed, high-quality caption for this image."

# --- VLM 数据增强配置 (概念性) ---
augmentation:
  enabled: false # 设置为 true 以启用概念性增强
  # 例如，你可以添加更多参数来控制未来实际增强的类型和强度
  # image_transforms: ["random_crop", "color_jitter"]
  # text_transforms: ["back_translation"]

# END OF FILE : configs/data/vlm_pretrain.yaml

====================
文件: .\configs\model\0.5B_dense.yaml
====================

# FILE: configs/model/0.5B_dense.yaml
# Bedrock Protocol: Configuration for a 0.5 Billion (approx) Dense LLM.
# This defines the architectural hyperparameters for from-scratch pre-training.

model_type: "DenseLLM" # Custom identifier for our internal model builder

# --- Core Transformer Architecture Parameters ---
# Calculated to achieve ~0.5B parameters.
vocab_size: 32000          # Vocabulary size. Should match your tokenizer.
hidden_size: 1024          # Dimensionality of the embeddings and Transformer blocks (d_model).
intermediate_size: 4096    # Dimensionality of the feed-forward layer (d_ffn). Typically 4 * hidden_size.
num_hidden_layers: 4      # Number of Transformer blocks (L).
num_attention_heads: 16    # Number of attention heads (h). hidden_size must be divisible by num_attention_heads.
num_key_value_heads: 16    # For Multi-Head Attention (MHA), this is equal to num_attention_heads.
                             # For Grouped-Query Attention (GQA), it's a smaller number.
hidden_act: "silu"         # Activation function for the FFN (e.g., "gelu", "relu", "silu", "swiglu").
max_position_embeddings: 2048 # Maximum sequence length the model can handle.

# --- Attention Specifics ---
# Determines which attention implementation to use.
# "standard" uses PyTorch native Scaled Dot-Product Attention (SDPA) or custom.
# "flash" uses FlashAttention (if available and compatible).
attention_type: "standard" # or "flash"

# --- Tokenizer Configuration (for model loading context) ---
# This links to the tokenizer used for pre-training.
tokenizer_path: "./data/tokenizers/wikitext_spm" # Path to the trained SentencePiece tokenizer.

# --- VLM Specifics (if used with a vision encoder) ---
# For pure LLM pre-training, these are not used.
# vision_encoder_model_name_or_path: "openai/clip-vit-large-patch14"
# vision_projector_hidden_size: 2048 # MLP hidden size for mapping vision features to text embedding space.
# vision_projector_num_layers: 2     # Number of MLP layers for the vision projector.

# END OF FILE: configs/model/0.5B_dense.yaml

====================
文件: .\configs\model\0.8B_moe.yaml
====================

# FILE: configs/model/0.8B_moe.yaml
# Bedrock Protocol: Configuration for a 0.8 Billion (approx) Mixture-of-Experts (MoE) LLM.
# This defines the architectural hyperparameters, showcasing MoE integration.

model_type: "MoELLM" # Custom identifier for our internal MoE model builder

# --- Core Transformer Architecture Parameters (Base for MoE) ---
vocab_size: 32000
hidden_size: 1024
# For MoE, intermediate_size defines the size of each *expert's* FFN.
intermediate_size: 4096
num_hidden_layers: 4
num_attention_heads: 16
num_key_value_heads: 16
hidden_act: "silu"
max_position_embeddings: 2048

# --- Attention Specifics ---
attention_type: "standard" # or "flash"

# --- MoE Specific Parameters ---
num_experts: 8             # Total number of experts in each MoE layer.
num_experts_per_tok: 2     # Number of experts to activate per token. (k)
router_aux_loss_coef: 0.001 # Coefficient for the router auxiliary loss (load balancing).

# --- Tokenizer Configuration (for model loading context) ---
tokenizer_path: "./data/tokenizers/wikitext_spm"

# END OF FILE: configs/model/0.8B_moe.yaml

====================
文件: .\configs\model\llama2-7b.yaml
====================

# FILE: configs/model/llama2-7b.yaml
# Bedrock Protocol: Model configuration template for a LLaMA-class model.
# While we load most configurations directly from the Hugging Face hub,
# this file serves as a template for specifying overrides or custom parameters.

model_name_or_path: "meta-llama/Llama-2-7b-hf" # Placeholder, will be overridden by training config
model_type: "LlamaForCausalLM"

# --- Key Architectural Parameters ---
# These are typically defined by the pre-trained model and not changed during fine-tuning.
vocab_size: 32000
hidden_size: 4096
intermediate_size: 11008
num_hidden_layers: 32
num_attention_heads: 32
num_key_value_heads: 32
hidden_act: "silu"
max_position_embeddings: 4096 # Can be extended via RoPE scaling, etc.
rope_theta: 10000.0
tie_word_embeddings: false

# --- Quantization Configuration ---
# Example for loading with bitsandbytes for lower memory usage.
quantization_config:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16" # torch.bfloat16
  bnb_4bit_use_double_quant: true

# --- Attention Implementation ---
# Use 'flash_attention_2' for significant speedup and memory savings on compatible hardware.
attn_implementation: "flash_attention_2" # or "eager" or "sdpa"

# END OF FILE: configs/model/llama2-7b.yaml

====================
文件: .\configs\training\finetune_dpo.yaml
====================

# FILE: configs/training/finetune_dpo.yaml
# Bedrock Protocol: Configuration for Direct Preference Optimization (DPO).
# MODIFIED FOR CPU-ONLY EXECUTION & SPEED & MINIMAL MEMORY

# --- Model & Tokenizer ---
# DPO starts from a supervised fine-tuned (SFT) model.
# This path should point to the SFT model trained on CPU.
model_name_or_path: "./checkpoints/sft-tinyllama-guanaco-cpu/final_model" # Points to the CPU-trained adapter
use_fast_tokenizer: true

# --- Dataset ---
dataset_name: "trl-internal-testing/hh-rlhf-trl-style"
dataset_subset_size: 10 # [MODIFIED] Reduced to absolute minimum for memory test

# --- DPO Specific Parameters ---
beta: 0.1

# --- Training Arguments ---
output_dir: "./checkpoints/dpo-tinyllama-guanaco-cpu" # Changed output dir for CPU run
num_train_epochs: 1
per_device_train_batch_size: 1      # [MODIFIED] Reduced for CPU memory
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1      # [MODIFIED] Reduced to absolute minimum
optim: "adamw_torch"                # [MODIFIED] Switched to standard CPU-compatible optimizer
learning_rate: 5e-5
weight_decay: 0.001
fp16: false                         # [MODIFIED] Disabled for CPU
bf16: false                         # [MODIFIED] Disabled for CPU
max_grad_norm: 1.0
logging_steps: 1                    # Log every step
# [MODIFIED FOR CPU SPEED] Stop after only 2 training steps.
max_steps: 2
max_length: 256                     # Reduced for memory
max_prompt_length: 128              # Reduced for memory
max_target_length: 128              # Reduced for memory
warmup_ratio: 0.1
lr_scheduler_type: "linear"

# --- PEFT is still used for DPO ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Experiment Tracking ---
report_to: "none" # Set to "none" for quick CPU test runs.
run_name: "dpo-tinyllama-guanaco-demo-cpu"

# END OF FILE: configs/training/finetune_dpo.yaml

====================
文件: .\configs\training\finetune_grpo.yaml
====================

# FILE: configs/training/finetune_grpo.yaml
# Bedrock Protocol: Configuration for Group Relative Policy Optimization (GRPO).

# --- Model & Tokenizer ---
# GRPO also starts from a supervised fine-tuned (SFT) model.
model_name_or_path: "./checkpoints/sft-tinyllama-guanaco-cpu/final_model" # Points to the CPU-trained adapter
use_fast_tokenizer: true

# --- Dataset ---
# GRPO uses a prompt dataset, similar to PPO.
dataset_name: "imdb"
dataset_text_field: "text"
dataset_subset_size: 20 # Keep it small for a quick CPU run

# --- GRPO Specific Parameters (TRL GRPOConfig) ---
# This is the core of GRPO: for each prompt, generate this many responses to form a group.
num_generations: 4
# Total optimization steps. Should be multiple of dataset_subset_size / batch_size.
grpo_steps: 5
learning_rate: 1e-5
batch_size: 1 # Number of prompts per batch
gradient_accumulation_steps: 1
beta: 0.1 # KL divergence coefficient
epsilon: 0.2 # PPO-like clipping parameter

# --- Generation Parameters for GRPO Rollouts ---
max_new_tokens: 32
min_length: -1 # TRL default
temperature: 0.7
top_k: 0.0
top_p: 1.0

# --- General Training & Experiment Tracking ---
output_dir: "./checkpoints/grpo-tinyllama-imdb-cpu"
seed: 42

# --- PEFT Configuration (LoRA for GRPO Actor) ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Experiment Tracking ---
run_name: "grpo-tinyllama-imdb-demo-cpu"
report_to: "none"

# END OF FILE: configs/training/finetune_grpo.yaml

====================
文件: .\configs\training\finetune_orpo.yaml
====================

# FILE: configs/training/finetune_orpo.yaml
# Bedrock Protocol: Configuration for Odds Ratio Preference Optimization (ORPO).

# --- Model & Tokenizer ---
# ORPO starts from a base model (does not require SFT first like DPO/PPO)
model_name_or_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
use_fast_tokenizer: true

# --- Dataset ---
# ORPO uses a mixed SFT + Preference dataset or just preference data.
# We'll use the same preference dataset as DPO for consistency in this demo.
dataset_name: "trl-internal-testing/hh-rlhf-trl-style"
dataset_subset_size: 20 # For quick CPU test

# --- ORPO Specific Parameters ---
beta: 0.1 # ORPO's odds ratio beta parameter

# --- Training Arguments ---
output_dir: "./checkpoints/orpo-tinyllama-mixed"
num_train_epochs: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
optim: "adamw_torch"
learning_rate: 5e-5
weight_decay: 0.001
max_grad_norm: 1.0
logging_steps: 1
max_steps: 2 # For quick CPU test
lr_scheduler_type: "linear"
report_to: "none"
run_name: "orpo-tinyllama-mixed-demo"

# --- Tokenization Parameters ---
max_length: 256
max_prompt_length: 128

# --- PEFT Configuration (LoRA for ORPO) ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# END OF FILE: configs/training/finetune_orpo.yaml

====================
文件: .\configs\training\finetune_ppo.yaml
====================

# FILE: configs/training/finetune_ppo.yaml
# Bedrock Protocol: Configuration for Proximal Policy Optimization (PPO).
# MODIFIED FOR CLEAN LOGS & CPU EXECUTION

# --- Model & Tokenizer ---
model_name_or_path: "./checkpoints/sft-tinyllama-guanaco-cpu/final_model" # Points to the CPU-trained adapter
use_fast_tokenizer: true

# --- Dataset ---
dataset_name: "imdb"
dataset_text_field: "text"
# [MODIFIED] Reduced subset size to accommodate larger batch size in RAM
dataset_subset_size: 20

# --- PPO Specific Parameters (TRL PPOConfig) ---
# [MODIFIED] Increased steps slightly for more meaningful run
ppo_steps: 4
learning_rate: 1.41e-5
batch_size: 2
# [FIX] Set mini_batch_size to be >= 2 to avoid 'var: nan' in logs.
# It should be a divisor of batch_size.
mini_batch_size: 2
gradient_accumulation_steps: 1
ppo_epochs: 4
init_kl_coef: 0.2
target_kl: 0.1
adap_kl_ctrl: true

# --- Generation Parameters for PPO Rollouts ---
max_new_tokens: 32
min_output_length: 8
max_output_length: 64

# --- General Training & Experiment Tracking ---
output_dir: "./checkpoints/ppo-tinyllama-guanaco-cpu"
seed: 42
bf16: false
fp16: false

# --- PEFT Configuration (LoRA for PPO Actor) ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Experiment Tracking ---
run_name: "ppo-tinyllama-guanaco-demo-cpu"

# END OF FILE: configs/training/finetune_ppo.yaml

====================
文件: .\configs\training\finetune_sft.yaml
====================

# FILE: configs/training/finetune_sft.yaml
# Bedrock Protocol: Configuration for Supervised Fine-Tuning (SFT) using LoRA.
# MODIFIED FOR CPU-ONLY EXECUTION & SPEED

# --- Model & Tokenizer ---
model_name_or_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
use_fast_tokenizer: true

# --- Dataset ---
dataset_name: "mlabonne/guanaco-llama2-1k"
dataset_text_field: "text"
# [MODIFIED FOR CPU SPEED] Add this new parameter to limit dataset size for fast CPU runs.
dataset_subset_size_cpu: 16 # Use only 16 samples for a quick run.
max_seq_length: 512

# --- PEFT / LoRA Configuration ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Training Arguments ---
output_dir: "./checkpoints/sft-tinyllama-guanaco-cpu"
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
optim: "adamw_torch"
save_steps: 10 # Save every 10 steps to have a checkpoint
logging_steps: 1 # Log every step to see progress
learning_rate: 2e-4
weight_decay: 0.001
fp16: false
bf16: false
max_grad_norm: 1.0
max_steps: 5 # [MODIFIED FOR CPU SPEED] Stop after only 5 training steps. This is the most important change.
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# --- Experiment Tracking ---
report_to: "none"
run_name: "sft-tinyllama-guanaco-demo-cpu"

# END OF FILE: configs/training/finetune_sft.yaml

====================
文件: .\configs\training\pretrain_llm.yaml
====================

# FILE: configs/training/pretrain_llm.yaml
# Bedrock Protocol: Comprehensive configuration for LLM/VLM from-scratch pre-training.
# This file centralizes all training-related hyperparameters and strategies.

# --- Global Training Settings ---
seed: 42
output_dir: "./checkpoints/pretrain_llm_demo"
# Checkpoint interval. Set to -1 to only save at the end.
save_steps: 1
logging_steps: 10
eval_steps: 10 # Evaluate every X steps.
max_steps: 10 # Total training steps. Set to -1 to train for num_train_epochs.
num_train_epochs: -1 # If max_steps > 0, this is ignored. Set to 1 for a quick demo epoch.

# --- Model Configuration ---
# Path to the specific model architecture YAML (e.g., 0.5B_dense.yaml, 0.8B_moe.yaml).
model_config_path: "configs/model/0.5B_dense.yaml" # Change this to switch models.

# --- Data Configuration ---
# Path to the processed text data or VLM data.
# This should point to the directory where data_processing/download_and_reproduce.py saved the data.
dataset_dir: "./data/processed/wikitext"
# Name of the column containing text data in the dataset.
dataset_text_field: "text"
# Max sequence length for tokenization and model input.
max_seq_length: 1024

# --- Optimizer ---
optimizer: "adamw_torch" # "adamw_torch", "adamw_8bit", "lion"
learning_rate: 3e-4
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8
max_grad_norm: 1.0

# --- Learning Rate Scheduler ---
lr_scheduler_type: "cosine" # "linear", "cosine", "polynomial", "constant"
warmup_ratio: 0.01 # Percentage of total steps for linear warmup.

# --- Batching & Parallelism ---
per_device_train_batch_size: 2 # Batch size per GPU.
gradient_accumulation_steps: 16 # Accumulate gradients over X steps. Effective batch size = per_device_train_batch_size * gradient_accumulation_steps * num_gpus.
# Mixed precision training. "bf16" is recommended for Ampere+ GPUs (A100, RTX30/40 series).
# "fp16" is for older GPUs or when bf16 is not available.
mixed_precision: "bf16" # "no", "fp16", "bf16"

# --- DeepSpeed Configuration ---
# Path to a DeepSpeed config JSON. Leave empty to use Accelerate's default DDP or FSDP.
# For ZeRO-3 or Pipeline Parallelism, a DeepSpeed config JSON is essential.
deepspeed_config: "" # e.g., "configs/deepspeed/zero3_config.json"

# --- Memory Optimization (If not using DeepSpeed config) ---
# These are Accelerate's native FSDP (Fully Sharded Data Parallel) options.
# DeepSpeed Zero-3 offers similar benefits and is often preferred.
# use_fsdp: false
# fsdp_strategy: "SHARD_GRAD_OP" # "FULL_SHARD", "SHARD_GRAD_OP"
# fsdp_cpu_offload: false # Offload optimizer states to CPU.

# --- Efficiency Optimizations ---
# torch.compile: Accelerates PyTorch code. Highly recommended.
use_torch_compile: true
# FlashAttention: Leveraged within model architecture (language_model.py) based on attention_type.

# --- Experiment Tracking ---
report_to: "wandb" # "wandb", "tensorboard", "none"
run_name: "pretrain-0.5b-dense-demo" # Unique name for your W&B run.

# END OF FILE: configs/training/pretrain_llm.yaml

====================
文件: .\configs\training\pretrain_llm_resume.yaml
====================

# FILE: configs/training/pretrain_llm_resume.yaml
# Bedrock Protocol: Configuration for resuming LLM/VLM pre-training.
# This file centralizes all training-related hyperparameters and strategies.

# --- Global Training Settings ---
seed: 42
output_dir: "./checkpoints/pretrain_llm_resume_demo" # Output directory for this resumed run
save_steps: 1 # Checkpoint interval. Set to -1 to only save at the end.
logging_steps: 1 # Log every X steps.
eval_steps: 1 # Evaluate every X steps.
max_steps: 5 # Total training steps. Set to -1 to train for num_train_epochs.
num_train_epochs: -1 # If max_steps > 0, this is ignored. Set to 1 for a quick demo epoch.

# --- Model Configuration ---
# Path to the specific model architecture YAML used for the initial pre-training.
# This is used if not resuming from a full model checkpoint.
model_config_path: "configs/model/0.5B_dense.yaml"

# [NEW] Resume from checkpoint
# Specify the path to the model checkpoint you want to resume from.
# This should be a directory containing `config.json` and `model.safetensors` (or `pytorch_model.bin`).
# Example: "./checkpoints/pretrain_llm_demo/final_model" or "./checkpoints/pretrain_llm_demo/step_X"
resume_from_checkpoint: "./checkpoints/pretrain_llm_demo/final_model"
# If resuming, 'model_config_path' is ignored for model loading, but used for tokenizer.
# model_name_or_path points to the tokenizer, which usually comes with the checkpoint.
model_name_or_path: "${resume_from_checkpoint}" # Tokenizer path for resumed models.

# --- Data Configuration ---
dataset_dir: "./data/processed/wikitext"
dataset_text_field: "text"
max_seq_length: 1024

# --- Optimizer ---
optimizer: "adamw_torch"
learning_rate: 1e-4 # Often lower for resumed training
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8
max_grad_norm: 1.0

# --- Learning Rate Scheduler ---
lr_scheduler_type: "cosine"
warmup_ratio: 0.01

# --- Batching & Parallelism (CPU friendly) ---
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
mixed_precision: "no" # "no" for CPU

# --- Experiment Tracking ---
report_to: "none"
run_name: "pretrain-0.5b-dense-resume-demo"

# END OF FILE: configs/training/pretrain_llm_resume.yaml

====================
文件: .\configs\training\train_reward_model.yaml
====================

# FILE: configs/training/train_reward_model.yaml
# Bedrock Protocol: Configuration for Training a Reward Model (RM).

# --- Model & Tokenizer ---
# Base model for the Reward Model. Can be a pre-trained LLM (e.g., TinyLlama)
# or an SFT-tuned model. Usually a smaller model is sufficient for RM.
model_name_or_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # Start with a base model or SFT model.
use_fast_tokenizer: true

# --- Dataset ---
# Preference dataset for RM training (same as DPO)
dataset_name: "trl-internal-testing/hh-rlhf-trl-style"
dataset_subset_size: 200 # For quick CPU test, use a larger subset for better RM training

# --- Tokenization ---
# [FIX] Added max_length for the tokenizer preprocessing step.
max_length: 512 # Max sequence length for prompt + response.

# --- Training Arguments for RewardTrainer ---
output_dir: "./checkpoints/reward-model-tinyllama-hh"
num_train_epochs: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
optim: "adamw_torch"
learning_rate: 1e-5 # Typically smaller for RM
weight_decay: 0.001
max_grad_norm: 1.0
logging_steps: 10
max_steps: 10 # For quick CPU test
lr_scheduler_type: "cosine"
report_to: "none" # Can change to "wandb" for tracking
run_name: "reward-model-tinyllama-hh-demo"

# --- PEFT Configuration (LoRA for RM) ---
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
# Target modules for LoRA in a sequence classification model
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  # The linear layer for classification is typically named "score" by TRL
  # or can be the last linear layer of the model if not PEFT-tuned before.
  - "score"

# END OF FILE: configs/training/train_reward_model.yaml

====================
文件: .\data_processing\build_tokenizer.py
====================

# FILE: data_processing/build_tokenizer.py
"""
Bedrock Protocol: Module for training a SentencePiece tokenizer. (Upgraded for Robustness)

This script provides a single, focused function to train a tokenizer from a
corpus file. It now includes a dynamic fallback mechanism to automatically
adjust the vocabulary size if the initial request is too large for the corpus,
ensuring the pipeline doesn't crash on smaller datasets.
"""

import sentencepiece as spm
from pathlib import Path
import argparse
import re
import logging  # 导入 logging 模块
import shutil  # 用于文件复制
import json  # 用于生成 JSON 配置
import sys  # 修正: 导入 sys
from transformers import PreTrainedTokenizerFast, AutoTokenizer, LlamaTokenizerFast

# 修正: 配置日志, 确保脚本独立运行时也能正常输出
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)  # 默认输出到控制台
    ]
)
# 获取日志器
logger = logging.getLogger(__name__)


def train_tokenizer(
        output_path_prefix: str,
        corpus_path: str,
        vocab_size: int,
        model_type: str,
        character_coverage: float,
        add_special_tokens: bool = True
) -> None:
    """
    Trains a SentencePiece tokenizer and saves the model.
    Includes a fallback to automatically reduce vocab_size if it's too high.
    """
    # 将传入的字符串路径转换为 Path 对象，以便使用 .parent 等属性
    output_path_prefix = Path(output_path_prefix)
    corpus_path = Path(corpus_path)

    # Ensure the output directory exists
    output_dir = output_path_prefix.parent
    output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"分词器输出目录已准备: {output_dir}")

    # --- Initial Training Attempt ---
    try:
        logger.info(f"--- [Bedrock] 启动 SentencePiece 训练 (尝试 1) ---")
        logger.info(f"请求词汇量大小: {vocab_size}")
        # SentencePiece Trainer 需要字符串路径
        _run_sentencepiece_training(str(output_path_prefix), str(corpus_path), vocab_size, model_type,
                                    character_coverage, add_special_tokens)

    except RuntimeError as e:
        logger.warning(f"首次训练失败。这在小数据集上很常见。错误详情: {e}")

        # --- Dynamic Vocab Size Adjustment Logic ---
        match = re.search(r'Please set it to a value <= (\d+)', str(e))
        if match:
            suggested_vocab_size = int(match.group(1))
            logger.info(f"--- [Bedrock] 正在尝试使用建议词汇量: {suggested_vocab_size}")

            try:
                _run_sentencepiece_training(str(output_path_prefix), str(corpus_path), suggested_vocab_size, model_type,
                                            character_coverage, add_special_tokens)
            except RuntimeError as e2:
                logger.error(f"致命错误: 使用建议词汇量重试训练仍失败。错误详情: {e2}")
                return
        else:
            logger.error("致命错误: 无法从错误信息中解析建议词汇量。已中止。")
            return

    logger.info("\n--- [Bedrock] SentencePiece 训练完成 ---")

    # --- 保存为 Hugging Face Transformers 格式 (最新的鲁棒逻辑) ---
    logger.info(f"--- [Bedrock] 正在生成 Hugging Face 分词器文件... ---")
    hf_tokenizer_output_dir = output_dir / f"{output_path_prefix.name}_hf"
    hf_tokenizer_output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"Hugging Face 分词器目标目录: {hf_tokenizer_output_dir}")

    spm_model_file = output_path_prefix.with_suffix('.model')

    try:
        # 修正：直接使用 LlamaTokenizerFast 加载 .model 文件，这是最稳健的方式
        # 它会正确生成所有必要的文件，包括 tokenizer.json
        logger.info(f"尝试使用 LlamaTokenizerFast 直接从 '{spm_model_file}' 加载...")
        tokenizer = LlamaTokenizerFast(vocab_file=str(spm_model_file))

        # 确保特殊 token 被正确设置
        if add_special_tokens:
            special_tokens_dict = {
                'bos_token': '[BOS]',
                'eos_token': '[EOS]',
                'unk_token': '[UNK]',
                'pad_token': '[PAD]'
            }
            tokenizer.add_special_tokens(special_tokens_dict)
            logger.info(f"已为分词器添加特殊 token: {list(special_tokens_dict.keys())}")

        if tokenizer.pad_token is None and tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info(f"已设置分词器的 pad_token 为 eos_token。")

        # 保存为 Hugging Face 格式
        tokenizer.save_pretrained(str(hf_tokenizer_output_dir))
        logger.info(f"Hugging Face 格式分词器已成功保存到: {hf_tokenizer_output_dir}")

    except Exception as e:
        logger.error(f"致命错误: 尝试生成 Hugging Face 分词器文件失败。错误详情: {e}")
        return


def _run_sentencepiece_training(
        output_path_prefix: str,
        corpus_path: str,
        vocab_size: int,
        model_type: str,
        character_coverage: float,
        add_special_tokens: bool
):
    """Helper function to construct and run the SentencePiece training command."""
    cmd_parts = [
        f'--input={corpus_path}',
        f'--model_prefix={output_path_prefix}',
        f'--vocab_size={vocab_size}',
        f'--model_type={model_type}',
        f'--character_coverage={character_coverage}',
    ]

    if add_special_tokens:
        cmd_parts.extend([
            f'--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3',
            f'--pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]',
            f'--user_defined_symbols=<0x0A>'
        ])

    command = " ".join(cmd_parts)
    spm.SentencePieceTrainer.Train(command)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train a SentencePiece tokenizer with dynamic vocab size fallback.")
    parser.add_argument(
        "--output_path_prefix", type=str, required=True,
        help="The base path and prefix for the output files (e.g., './data/tokenizers/my_spm')."
    )
    parser.add_argument(
        "--corpus_path", type=str, required=True,
        help="Path to the text file corpus to train on."
    )
    parser.add_argument(
        "--vocab_size", type=int, default=8000,
        help="The desired total number of tokens in the vocabulary."
    )
    parser.add_argument(
        "--model_type", type=str, default="unigram", choices=["unigram", "bpe", "char", "word"],
        help="The tokenizer model type ('unigram' is recommended)."
    )
    parser.add_argument(
        "--character_coverage", type=float, default=1.0,
        help="The percentage of characters in the corpus to be covered."
    )
    parser.add_argument(
        "--add_special_tokens", type=bool, default=True,
        help="Whether to add default special tokens."
    )
    args = parser.parse_args()

    train_tokenizer(
        output_path_prefix=args.output_path_prefix,
        corpus_path=args.corpus_path,
        vocab_size=args.vocab_size,
        model_type=args.model_type,
        character_coverage=args.character_coverage,
        add_special_tokens=args.add_special_tokens
    )

# END OF FILE: data_processing/build_tokenizer.py

====================
文件: .\data_processing\download_and_reproduce.py
====================


# FILE: data_processing/download_and_reproduce.py
"""
基石协议：数据管道执行主脚本。

此脚本作为下载、处理和准备教程所需数据集的入口点。
它通过使用版本化数据集和确定性处理步骤，确保数据管道的完全可复现性。

设计为运行一次以设置所有必要的数据工件。
"""

import sys
import os
from pathlib import Path
import yaml
from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict  # 导入 Dataset 类
import logging
import datetime
import shutil  # 导入 shutil 模块用于文件操作
from PIL import Image  # 导入 Image 用于图像加载

# 配置日志
log_dir = Path("./logs")
log_dir.mkdir(parents=True, exist_ok=True)
log_filename = log_dir / f"data_pipeline_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,  # 默认日志级别为 INFO
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),  # 输出到文件
        logging.StreamHandler(sys.stdout)  # 输出到控制台
    ]
)

# 获取主脚本的日志器
logger = logging.getLogger(__name__)

# 从 data_processing 包导入相关函数
from data_processing.process_text import clean_text_dataset, deduplicate_dataset, augment_text_dataset
from data_processing.process_vlm import process_vlm_dataset, deduplicate_vlm_dataset, augment_vlm_dataset
from data_processing.build_tokenizer import train_tokenizer


def run_text_pipeline(config_path: str) -> None:
    """
    执行用于文本预训练的整个数据管道。
    """
    logger.info("--- [基石] 启动文本数据处理流水线 ---")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    logger.info(f"已加载配置文件: {config_path}")

    # --- 路径解析（已修正，严格遵循配置文件） ---
    base_output_dir = Path(config['base_output_dir'])

    # 使用字符串替换来处理配置文件中的 ${variable} 占位符
    raw_data_dir = Path(config['raw_data_dir'].replace('${base_output_dir}', str(base_output_dir)))
    processed_data_dir = Path(config['processed_data_dir'].replace('${base_output_dir}', str(base_output_dir)))
    tokenizer_output_prefix = Path(config['tokenizer_output_path'].replace('${base_output_dir}', str(base_output_dir)))
    tokenizer_corpus_path = Path(
        config['tokenizer_training_corpus'].replace('${processed_data_dir}', str(processed_data_dir)))

    # 确保输出目录存在
    processed_data_dir.mkdir(parents=True, exist_ok=True)
    tokenizer_output_prefix.parent.mkdir(parents=True, exist_ok=True)
    logger.info(f"数据输出目录已准备: {processed_data_dir}")
    logger.info(f"分词器输出前缀已准备: {tokenizer_output_prefix}")

    logger.info(f"1. 正在加载数据集 '{config['dataset_name']}'...")
    # --- 健壮的数据集加载和子集选择逻辑 ---
    # 首先，加载数据集的元信息来检查可用的分割
    full_dataset = load_dataset(
        config['dataset_name'],
        config['dataset_config_name'],
        cache_dir=str(raw_data_dir),
    )
    logger.info(f"完整数据集包含以下分割: {list(full_dataset.keys())}")

    # 从配置文件获取数据集子集大小。如果为0或未指定，则加载整个数据集。
    subset_size = config.get('dataset_subset_size')

    if subset_size is not None and subset_size > 0:
        logger.info(f"--- [基石] 正在为所有可用分割加载子集: 前 {subset_size} 个样本。 ---")

        processed_splits = {}
        for split_name, split_dataset in full_dataset.items():
            # 为每个分割选择子集
            # 为了保持分割间的比例，可以为验证集和测试集选择更小的子集
            if split_name == 'train':
                size = subset_size
            else:  # validation, test, etc.
                size = int(subset_size * 0.1) if int(subset_size * 0.1) > 0 else 1

            # 确保请求的样本数不超过该分割的总样本数
            if size > len(split_dataset):
                logger.warning(
                    f"请求的子集大小 {size} 大于 '{split_name}' 分割的可用样本数 {len(split_dataset)}。将使用所有可用样本。")
                size = len(split_dataset)

            processed_splits[split_name] = split_dataset.select(range(size))

        dataset = DatasetDict(processed_splits)
        logger.info(f"子集加载完成，包含分割: {list(dataset.keys())}")
    else:
        logger.info(f"--- [基石] 正在使用完整数据集。 ---")
        dataset = full_dataset
    logger.info("数据集加载成功。")

    logger.info("2. 正在进行文本清洗和质量过滤...")
    cleaned_dataset = clean_text_dataset(dataset, text_column=config['text_column'])
    logger.info("文本清洗和质量过滤完成。")

    logger.info("3. 正在进行数据去重...")
    deduplicated_dataset = deduplicate_dataset(cleaned_dataset, text_column=config['text_column'])
    logger.info("数据去重完成。")

    logger.info("4. 正在进行数据增强...")
    augmented_dataset = augment_text_dataset(deduplicated_dataset, text_column=config['text_column'])
    logger.info("数据增强步骤完成。")

    logger.info(f"5. 正在保存处理后的数据集到 '{processed_data_dir}'...")
    augmented_dataset.save_to_disk(str(processed_data_dir))
    logger.info(f"处理后的数据集已保存到: {processed_data_dir}")

    logger.info(f"6. 正在准备分词器训练语料到 '{tokenizer_corpus_path}'...")
    # 假设文本数据管道加载的数据集包含 'train', 'validation', 'test' 分割
    # 确保这些分割存在，或者根据实际情况调整
    full_text_dataset = concatenate_datasets([
        augmented_dataset['train'],
        augmented_dataset['validation'],
        augmented_dataset['test']
    ])
    with open(tokenizer_corpus_path, "w", encoding="utf-8") as f:
        for example in full_text_dataset:
            text = example[config['text_column']]
            if text:  # 确保写入非空文本
                f.write(text + "\n")
    logger.info("分词器训练语料已准备。")

    logger.info(f"7. 正在训练 SentencePiece 分词器并保存为 Hugging Face 格式...")
    train_tokenizer(
        output_path_prefix=str(tokenizer_output_prefix),  # 确保传递字符串
        corpus_path=str(tokenizer_corpus_path),  # 确保传递字符串
        vocab_size=config['vocab_size'],
        model_type=config['model_type'],
        character_coverage=config['character_coverage']
    )
    logger.info(f"分词器训练和保存完成到 '{tokenizer_output_prefix}_hf'.")

    logger.info("--- [基石] 文本数据处理流水线全部完成 ---")


def run_vlm_pipeline(config_path: str) -> None:
    """
    执行用于 VLM 预训练的数据管道。
    已重构，以更稳健地处理 COCO 数据下载和加载。
    """
    logger.info("\n--- [基石] 启动 VLM 数据处理流水线 ---")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    logger.info(f"已加载配置文件: {config_path}")

    base_output_dir = Path(config['base_output_dir'])
    # 对于 VLM 数据，通常目录名为 'coco_demo' 或 'coco_val_demo'
    # 使用 Path(config['raw_data_dir']).name 来从配置文件中提取目录名部分
    raw_data_dir = base_output_dir / 'raw' / Path(config['raw_data_dir']).name
    processed_data_dir = base_output_dir / 'processed' / Path(config['processed_data_dir']).name

    # 确保输出目录存在
    processed_data_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"VLM 数据输出目录已准备: {processed_data_dir}")

    # --- 健壮的 VLM 数据集加载和子集选择逻辑 ---
    dataset_name = config['dataset_name']
    logger.info(f"1. 正在加载数据集 '{dataset_name}'...")

    # 首先，加载数据集的元信息来检查可用的分割
    full_dataset = load_dataset(
        dataset_name,
        cache_dir=str(raw_data_dir),
    )
    logger.info(f"完整数据集包含以下分割: {list(full_dataset.keys())}")

    # 从配置文件获取 num_samples_to_process。如果为0或未指定，则加载整个数据集。
    num_samples_to_process = config.get("num_samples_to_process")

    if num_samples_to_process is not None and num_samples_to_process > 0:
        logger.info(f"--- [基石] 正在为所有可用分割加载子集: 前 {num_samples_to_process} 个样本。 ---")

        processed_splits = {}
        for split_name, split_dataset in full_dataset.items():
            # 确保请求的样本数不超过该分割的总样本数
            size = num_samples_to_process
            if size > len(split_dataset):
                logger.warning(
                    f"请求的子集大小 {size} 大于 '{split_name}' 分割的可用样本数 {len(split_dataset)}。将使用所有可用样本。")
                size = len(split_dataset)

            processed_splits[split_name] = split_dataset.select(range(size))

        dataset = DatasetDict(processed_splits)
        logger.info(f"子集加载完成，包含分割: {list(dataset.keys())}")
    else:
        logger.info(f"--- [基石] 正在使用完整数据集。 ---")
        dataset = full_dataset
    logger.info("数据集加载成功。")
    # --- VLM 数据下载和初始加载逻辑结束 ---

    logger.info("2. 正在处理 VLM 数据 (图像转换, 文本清洗等)...")
    processed_dataset = process_vlm_dataset(
        dataset,
        image_column=config['image_column'],
        text_column=config['text_column'],
        distillation_config=config['distillation']
    )
    logger.info("VLM 数据处理完成。")

    logger.info("3. 正在进行 VLM 数据去重...")
    deduplicated_dataset = deduplicate_vlm_dataset(
        processed_dataset,
        image_column=config['image_column'],  # 原始列名，仅为函数签名兼容性
        text_column='cleaned_captions'  # 确保使用清洗后的字幕进行去重
    )
    logger.info("VLM 数据去重完成。")

    logger.info("4. 正在进行 VLM 数据增强...")
    augmented_vlm_dataset = augment_vlm_dataset(
        deduplicated_dataset,
        image_column=config['image_column'],
        text_column='cleaned_captions',
        augmentation_config=config.get('augmentation', {})
    )
    logger.info("VLM 数据增强步骤完成。")

    logger.info(f"5. 正在保存处理后的数据集到 '{processed_data_dir}'...")
    augmented_vlm_dataset.save_to_disk(str(processed_data_dir))
    logger.info(f"处理后的 VLM 数据集已保存到: {processed_data_dir}")

    logger.info("--- [基石] VLM 数据处理流水线全部完成 ---")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="运行 Awesome-LLM-ZeroToScratch 的数据处理管道。")
    parser.add_argument(
        "pipeline",
        type=str,
        choices=["text", "vlm", "all"],
        help="要运行的管道: 'text', 'vlm', 或 'all'。"
    )
    args = parser.parse_args()

    PIPELINE_TO_RUN = args.pipeline

    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    text_config = project_root / "configs/data/text_pretrain.yaml"
    vlm_config = project_root / "configs/data/vlm_pretrain.yaml"

    if PIPELINE_TO_RUN in ["text", "all"]:
        run_text_pipeline(str(text_config))

    if PIPELINE_TO_RUN in ["vlm", "all"]:
        run_vlm_pipeline(str(vlm_config))

    logger.info("\n[基石] 所有选定的流水线已成功执行。")

# END OF FILE: data_processing/download_and_reproduce.py


====================
文件: .\data_processing\process_text.py
====================

# FILE: data_processing/process_text.py
"""
Bedrock Protocol: Module for text data cleaning and processing.

This module contains functions that perform deterministic, rule-based cleaning
on text datasets. The goal is to improve data quality, which is a cornerstone
of building robust models.
"""

import os
import logging
from typing import Dict, Any
from datasets import Dataset, DatasetDict
import re

# 配置日志
logger = logging.getLogger(__name__)


def clean_text(text: str) -> str:
    """
    对单个文本字符串应用一系列确定性的清洗规则。
    此函数旨在标准化文本格式，移除无关内容，为后续处理步骤做准备。

    Args:
        text: 原始的文本字符串。

    Returns:
        清洗后的文本字符串。如果输入无效，则返回空字符串。
    """
    if not isinstance(text, str):
        logger.warning(f"检测到非字符串类型的数据，无法执行清洗，已跳过。数据类型: {type(text)}")
        return ""

    # 规则 1: 标准化空白字符。将多个空格、换行符、制表符替换为单个空格，并移除首尾空格。
    cleaned_text = re.sub(r'\s+', ' ', text).strip()

    # 规则 2: 移除维基百科文章中常见的元数据标题行 (例如, "= = = Section Title = = =")。
    # 这条规则是针对 Wikitext 数据集格式的特定优化。
    if cleaned_text.startswith('=') and cleaned_text.endswith('='):
        logger.debug(f"移除被识别为元数据行的文本: {cleaned_text[:80]}...")
        return ""  # 返回空字符串，该行将在后续的质量过滤中被移除

    return cleaned_text


def quality_filter(text: str) -> bool:
    """
    对文本字符串应用一套启发式规则，以评估其数据质量。
    此函数用于过滤掉低质量或无意义的文本。

    Args:
        text: 需要检查的文本字符串。

    Returns:
        如果文本通过所有质量检查，则返回 True，否则返回 False。
    """
    if not text:
        logger.debug("质量过滤：过滤掉空文本。")
        return False

    # 过滤规则 1: 最小长度（按词计数）。此规则可以有效过滤掉过短或无意义的行。
    # 这里的 "10" 是一个经验值，可以根据具体任务进行调整。
    if len(text.split()) < 10:
        logger.debug(f"质量过滤：因少于10个词而过滤掉的短文本: {text[:80]}...")
        return False

    # 过滤规则 2: 文本必须包含至少一个英文字母。此规则可以过滤掉纯数字或纯符号的行。
    if not re.search(r'[a-zA-Z]', text):
        logger.debug(f"质量过滤：因不含任何英文字母而被过滤的文本: {text[:80]}...")
        return False

    return True


def clean_text_dataset(dataset: DatasetDict, text_column: str) -> DatasetDict:
    """
    将文本清洗和质量过滤函数应用于 Hugging Face DatasetDict 的所有分割。
    这是一个高级封装函数，它协调 `clean_text` 和 `quality_filter` 的执行。

    Args:
        dataset: 原始的 DatasetDict 对象。
        text_column: 包含待清洗文本的列名。

    Returns:
        一个新的 DatasetDict，其中包含了经过清洗和过滤的数据。
    """
    logger.info("开始对整个数据集进行文本清洗和质量过滤...")
    # 使用多进程以加速处理，进程数可以根据 CPU 核心数调整
    num_processes = os.cpu_count() if os.cpu_count() is not None else 1
    logger.info(f"使用 {num_processes} 个进程进行数据映射操作。")

    # 第一步: 应用 clean_text 函数进行初步清洗
    cleaned_dataset = dataset.map(
        lambda example: {text_column: clean_text(example[text_column])},
        num_proc=num_processes,
        desc="执行文本初步清洗"
    )
    logger.info("文本初步清洗完成。")

    # 第二步: 应用 quality_filter 函数进行质量过滤
    # 过滤掉在 clean_text 中可能返回空字符串的行，以及不符合质量标准的行
    filtered_dataset = cleaned_dataset.filter(
        lambda example: quality_filter(example[text_column]),
        num_proc=num_processes,
        desc="执行数据质量过滤"
    )
    
    # 记录每个分割的样本数量变化
    for split_name in dataset.keys():
        original_count = len(dataset[split_name])
        filtered_count = len(filtered_dataset[split_name])
        logger.info(f"'{split_name}' 分割：原始样本数 = {original_count}, 过滤后样本数 = {filtered_count} (移除了 {original_count - filtered_count} 个样本)")

    logger.info("数据集的文本清洗和质量过滤全部完成。")
    return filtered_dataset


def deduplicate_dataset(dataset: DatasetDict, text_column: str) -> DatasetDict:
    """
    使用高效的、基于哈希的向量化方法，从 Hugging Face DatasetDict 的所有分割中删除精确的重复文本条目。

    Args:
        dataset: 要去重的 DatasetDict 对象。
        text_column: 包含要进行去重操作的文本的列名。

    Returns:
        一个新的 DatasetDict，其中重复的条目已被移除。
    """
    logger.info("开始对数据集进行高效的精确去重...")
    # 使用多进程以加速处理，进程数可以根据 CPU 核心数调整
    num_processes = os.cpu_count() if os.cpu_count() is not None else 1
    logger.info(f"使用 {num_processes} 个进程进行去重。")

    deduplicated_dataset = DatasetDict()
    for split_name, split_dataset in dataset.items():
        original_count = len(split_dataset)
        logger.info(f"正在处理 '{split_name}' 分割，共 {original_count} 个样本...")

        # 使用 map 方法高效地计算每个文本的哈希值
        # with_indices=True 会为每个样本提供一个唯一的索引，用于后续的筛选
        hashes = split_dataset.map(
            lambda example: {'hash': hash(example[text_column])},
            num_proc=num_processes,
            desc=f"为 '{split_name}' 分割计算哈希值"
        )

        seen_hashes = set()
        unique_indices = []

        # 迭代哈希值（这比迭代整个数据集要快得多）
        for i, h in enumerate(hashes['hash']):
            if h not in seen_hashes:
                seen_hashes.add(h)
                unique_indices.append(i)

        # 使用计算出的唯一索引来选择样本
        deduplicated_split = split_dataset.select(unique_indices)
        deduplicated_dataset[split_name] = deduplicated_split
        
        final_count = len(deduplicated_split)
        logger.info(
            f"'{split_name}' 分割去重完成。原始样本数: {original_count}, 去重后样本数: {final_count} (移除了 {original_count - final_count} 个重复项)"
        )

    logger.info("数据集精确去重完成。")
    return deduplicated_dataset


def augment_text_dataset(dataset: DatasetDict, text_column: str) -> DatasetDict:
    """
    用于文本数据增强的概念性占位符函数。

    在真实的生产环境中，这里可以集成各种数据增强技术，例如：
    - **回译 (Back-translation)**: 将文本翻译到另一种语言再翻译回来，以产生新的表述。
    - **释义 (Paraphrasing)**: 使用语言模型生成与原文意思相同但措辞不同的句子。
    - **同义词替换**: 使用词库（如 WordNet）替换句子中的部分词语。
    - **生成式增强**: 使用大型语言模型根据原文生成新的、相关的文本样本。

    Args:
        dataset: 需要进行增强的 DatasetDict 对象。
        text_column: 包含待增强文本的列名。

    Returns:
        返回原始的 DatasetDict。这是一个占位符，需要实现具体逻辑才能实际增强数据。
    """
    logger.info("开始执行概念性文本数据增强 (当前为占位符逻辑)。")
    # -------------------------------------------------------------------
    # 警告: 下方是数据增强的示例代码，默认被注释掉。
    # 若要启用，请取消注释并安装所需库 (例如 `pip install nlpaug`)。
    # -------------------------------------------------------------------
    # from nlpaug.augmenter.word import SynonymAugmenter
    # aug = SynonymAugmenter(aug_src='wordnet')
    #
    # def _augment_example(example: Dict[str, Any]) -> Dict[str, Any]:
    #     original_text = example[text_column]
    #     # nlpaug 可能返回单个字符串或列表，确保处理逻辑的健壮性
    #     augmented_text_list = aug.augment(original_text)
    #     augmented_text = augmented_text_list[0] if isinstance(augmented_text_list, list) else augmented_text_list
    #     return {text_column: augmented_text if augmented_text else original_text}
    #
    # logger.info("正在应用同义词替换增强...")
    # num_processes = os.cpu_count() if os.cpu_count() is not None else 1
    # augmented_dataset = dataset.map(_augment_example, num_proc=num_processes, desc="应用数据增强")
    # logger.info("文本数据增强完成。")
    # return augmented_dataset
    # -------------------------------------------------------------------

    logger.warning("数据增强功能当前为占位符，未对数据进行实际修改。若需启用，请在 `augment_text_dataset` 函数中实现具体逻辑。")
    return dataset  # 按当前设计，返回未经修改的原始数据集

# END OF FILE: data_processing/process_text.py

====================
文件: .\data_processing\process_vlm.py
====================

# FILE: data_processing/process_vlm.py
"""
Bedrock Protocol: Module for Vision-Language Model (VLM) data processing.

Contains functions for handling image-text pairs, including cleaning, resizing,
and a conceptual implementation of data distillation.
Now includes deduplication and conceptual augmentation.
"""

import os
import logging # 导入 logging 模块
from typing import Dict, Any, List
from datasets import Dataset, DatasetDict
from PIL import Image
import torch
import torch.nn as nn
from torchvision.transforms import ToTensor, Resize, Compose
import hashlib # 用于图像哈希


# 配置日志
logger = logging.getLogger(__name__)


# --- Conceptual Implementation of GPT-4V Distillation ---
# Mandate of Proactive Defense: This function is designed to be safe. It will
# not run and incur costs unless explicitly enabled and configured with an
# API key via environment variables.

def conceptual_gpt4v_distillation(
        image: Image.Image,
        config: Dict[str, Any]
) -> List[str]:
    """
    一个概念性的、非执行的示例，演示如何使用像 GPT-4V 这样的多模态大模型
    为图像生成高质量的描述（即数据蒸馏）。

    警告: 此函数被设计为安全的占位符。除非在配置文件中明确启用并通过环境变量
    设置了 API 密钥，否则它不会执行实际的 API 调用，从而避免意外的费用产生。

    Args:
        image: 一个 PIL.Image.Image 对象。
        config: 包含蒸馏相关配置的字典，例如 `enabled` 和 `max_prompts`。

    Returns:
        一个包含生成描述的列表。在此示例中，仅返回一个硬编码的占位符描述。
    """
    logger.info("正在执行概念性 GPT-4V 数据蒸馏（当前为占位符逻辑）。")
    
    # 在实际应用中，您需要取消注释并完善此处的逻辑。
    if not config.get("enabled", False):
        logger.debug("数据蒸馏功能在配置中未启用。")
        return ["数据蒸馏功能未启用（概念性实现）。"]

    # 从环境变量中获取 API 密钥，这是管理密钥的安全实践。
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        logger.warning("警告: 环境变量 `OPENAI_API_KEY` 未设置。无法执行实际的蒸馏操作。")
        return ["数据蒸馏已启用但缺少 API 密钥（概念性实现）。"]

    # -------------------------------------------------------------------
    # 警告: 下方是实际 API 调用的示例代码，默认被注释掉。
    # 若要启用，请确保已安装 openai 库 (`pip install openai`) 并已设置 API 密钥。
    # -------------------------------------------------------------------
    # from openai import OpenAI
    # client = OpenAI(api_key=api_key)
    # import base64
    # from io import BytesIO
    #
    # # 将 PIL 图像转换为 Base64 编码的字符串
    # buffered = BytesIO()
    # image.save(buffered, format="PNG")
    # img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    #
    # # 构建发送给 GPT-4V 的请求
    # response = client.chat.completions.create(
    #     model="gpt-4-vision-preview",
    #     messages=[
    #         {
    #             "role": "user",
    #             "content": [
    #                 {"type": "text", "text": config.get("prompt", "请详细描述这张图片。")},
    #                 {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_str}"}},
    #             ],
    #         }
    #     ],
    #     max_tokens=300,
    # )
    # distilled_caption = response.choices[0].message.content
    # return [distilled_caption]
    # -------------------------------------------------------------------

    # 在本教程中，我们返回一个硬编码的占位符字符串。
    logger.info("概念性 GPT-4V 数据蒸馏完成。返回占位符描述。")
    return [f"这是一张为 {image.mode} 图像生成的、概念性的高质量描述（蒸馏功能已激活）。"]


def process_vlm_dataset(
        dataset: Dataset,
        image_column: str,
        text_column: str,
        distillation_config: Dict[str, Any]
) -> Dataset:
    """
    对 VLM 数据集进行系统化处理，包括图像验证、转换、文本清洗和概念性数据蒸馏。

    Args:
        dataset: 原始的 Hugging Face Dataset 对象。
        image_column: 包含图像的列名。
        text_column: 包含文本描述的列名。
        distillation_config: 用于概念性数据蒸馏的配置字典。

    Returns:
        处理后的 Dataset，其中包含了新的列（如 `processed_image_tensor`）并过滤了无效样本。
    """
    logger.info("开始处理 VLM 数据集，包括图像验证、转换和文本清洗...")

    # 定义标准的图像转换流程，这是 VLM 模型输入的常见预处理步骤
    image_transform = Compose([
        Resize((224, 224)),  # 将所有图像尺寸统一调整为 224x224
        ToTensor(),          # 将 PIL.Image 对象转换为 PyTorch 张量
    ])

    # 使用闭包变量来限制蒸馏 API 的调用次数，以控制成本
    prompt_count = 0

    def process_example(example: Dict[str, Any], idx: int) -> Dict[str, Any]:
        """处理 VLM 数据集中的单个样本。"""
        nonlocal prompt_count
        image = example[image_column]
        captions = example[text_column]

        # --- 图像验证与处理 ---
        # 健壮性检查：确保图像是有效的 PIL.Image 对象
        if not isinstance(image, Image.Image) or image is None:
            logger.warning(f"在索引 {idx} 处检测到无效或空的图像数据 (类型: {type(image)})，将跳过此样本。")
            return {"processed_image_tensor": None, "cleaned_captions": [], "distilled_captions": [], "is_valid": False}

        # 确保图像是 RGB 格式，这是大多数视觉模型的标准输入格式
        if image.mode != "RGB":
            try:
                image = image.convert("RGB")
            except Exception as e:
                logger.warning(f"在索引 {idx} 处无法将图像转换为 RGB 格式，将跳过此样本。错误: {e}")
                return {"processed_image_tensor": None, "cleaned_captions": [], "distilled_captions": [], "is_valid": False}

        # 应用图像转换流程
        processed_image_tensor = image_transform(image)

        # --- 文本清洗 ---
        cleaned_captions = []
        if isinstance(captions, list):
            # COCO 数据集的字幕是字典列表，例如: [{'caption': '...'}]
            cleaned_captions = [cap['caption'].strip() for cap in captions if isinstance(cap, dict) and 'caption' in cap and cap['caption'].strip()]
        elif isinstance(captions, str):
            # 处理字幕是单个字符串的情况
            cleaned_captions = [captions.strip()] if captions.strip() else []
        else:
            logger.warning(f"在索引 {idx} 处检测到非预期的文本数据类型 (类型: {type(captions)})，文本将被视为空。")

        # --- 数据蒸馏步骤 ---
        distilled_captions = []
        if distillation_config.get("enabled", False) and prompt_count < distillation_config.get("max_prompts", 0):
            distilled_captions = conceptual_gpt4v_distillation(image, distillation_config)
            prompt_count += 1
            if prompt_count >= distillation_config.get("max_prompts", 0):
                logger.info(f"已达到概念性蒸馏的最大调用次数 ({distillation_config.get('max_prompts', 0)})。后续样本将不再进行蒸馏。")

        return {
            "processed_image_tensor": processed_image_tensor,
            "cleaned_captions": cleaned_captions,
            "distilled_captions": distilled_captions,
            "is_valid": True  # 标记为有效样本，用于后续过滤
        }

    # VLM 数据处理涉及图像操作和潜在的 API 调用，这些操作在多进程中容易出错（例如 PIL/CUDA 上下文问题）。
    # 因此，使用单进程 (num_proc=1) 是更安全、更稳健的选择。
    processed_dataset = dataset.map(
        process_example,
        with_indices=True, # 传递样本索引给 process_example 以便日志记录
        num_proc=1,        # 保持为 1 以确保稳定性
        desc="处理 VLM 样本（图像转换、文本清洗）"
    )
    logger.info("VLM 数据集初步处理完成。")

    # --- 过滤无效样本 ---
    # 移除在处理过程中被标记为无效的样本（例如，损坏的图像）
    initial_count = len(processed_dataset)
    processed_dataset = processed_dataset.filter(lambda x: x["is_valid"], desc="过滤无效的 VLM 样本")
    processed_dataset = processed_dataset.remove_columns("is_valid") # 移除临时的 is_valid 标记列
    final_count = len(processed_dataset)
    logger.info(f"VLM 样本过滤完成。原始样本数: {initial_count}, 过滤后样本数: {final_count} (移除了 {initial_count - final_count} 个无效样本)")

    return processed_dataset


def deduplicate_vlm_dataset(dataset: Dataset, image_column: str, text_column: str) -> Dataset:
    """
    使用高效的、基于哈希的向量化方法，根据图像内容和清洗后的字幕组合，移除重复的 VLM 条目。

    Args:
        dataset: 待去重的 Dataset 对象 (应已包含 'processed_image_tensor' 和 'cleaned_captions' 列)。
        image_column: 原始图像列的名称 (仅用于日志和上下文)。
        text_column: 原始文本列的名称 (仅用于日志和上下文)。

    Returns:
        一个新的 Dataset，其中重复的条目已被移除。
    """
    logger.info("开始对 VLM 数据集进行高效去重...")
    original_count = len(dataset)
    
    # VLM 的哈希计算相对复杂，涉及 I/O 和计算，因此单进程映射更稳定
    logger.info("使用单进程为 VLM 数据集计算组合哈希值...")

    def _calculate_hash(example: Dict[str, Any]) -> Dict[str, str]:
        """为单个样本计算图像和文本的组合哈希值。"""
        processed_image_tensor = example.get("processed_image_tensor")
        cleaned_captions = example.get("cleaned_captions")

        if processed_image_tensor is None or not cleaned_captions:
            return {"combined_hash": None}

        # 确保张量在 CPU 上并且是确定性的数据类型，以便哈希
        image_bytes = torch.tensor(processed_image_tensor).cpu().float().numpy().tobytes()
        image_hash = hashlib.md5(image_bytes).hexdigest()

        # 对字幕进行排序，确保哈希值与字幕顺序无关
        caption_string = " ".join(sorted(cleaned_captions))
        text_hash = hashlib.md5(caption_string.encode('utf-8')).hexdigest()
        
        return {"combined_hash": f"{image_hash}_{text_hash}"}

    # 使用 map 方法高效地为每个样本添加一个哈希值列
    hashed_dataset = dataset.map(
        _calculate_hash,
        num_proc=1, # VLM 哈希计算建议使用单进程以保证稳定性
        desc="为 VLM 样本计算组合哈希值"
    )

    seen_hashes = set()
    unique_indices = []
    # 遍历速度更快的哈希值列来识别唯一项
    for i, h in enumerate(hashed_dataset['combined_hash']):
        if h is not None and h not in seen_hashes:
            seen_hashes.add(h)
            unique_indices.append(i)

    # 根据唯一索引选择样本
    deduplicated_dataset = dataset.select(unique_indices)
    final_count = len(deduplicated_dataset)

    logger.info(f"VLM 数据集去重完成。原始样本数: {original_count}, 去重后样本数: {final_count} (移除了 {original_count - final_count} 个重复项)")
    return deduplicated_dataset


def augment_vlm_dataset(dataset: Dataset, image_column: str, text_column: str, augmentation_config: Dict[str, Any]) -> Dataset:
    """
    用于 VLM 数据增强的概念性占位符函数。

    在真实的生产环境中，这里可以集成各种数据增强技术，例如：
    - **图像增强**: 应用随机裁剪、旋转、颜色抖动等变换 (例如使用 `torchvision.transforms`)。
    - **文本增强**: 对图像描述进行回译、同义词替换等操作。
    - **生成式增强**: 使用另一个强大的 VLM 根据现有图像生成新的、多样的描述，甚至生成新的图像。

    Args:
        dataset: 需要进行增强的 Dataset 对象。
        image_column: 原始图像列的名称 (仅用于日志和上下文)。
        text_column: 原始文本列的名称 (仅用于日志和上下文)。
        augmentation_config: 包含增强相关配置的字典，例如 `enabled`。

    Returns:
        返回原始的 Dataset。这是一个占位符，需要实现具体逻辑才能实际增强数据。
    """
    logger.info("开始执行概念性 VLM 数据增强 (当前为占位符逻辑)。")

    if not augmentation_config.get("enabled", False):
        logger.info("VLM 数据增强功能在配置中未启用，跳过此步骤。")
        return dataset

    # -------------------------------------------------------------------
    # 警告: 下方是 VLM 数据增强的示例代码，默认被注释掉。
    # 若要启用，请取消注释并安装所需库 (例如 `torchvision`)。
    # -------------------------------------------------------------------
    # from torchvision.transforms import RandomResizedCrop, ColorJitter, ToPILImage, ToTensor
    # 
    # # 定义一个图像增强的变换流程
    # image_augment_transform = Compose([
    #     RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),
    #     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    # ])
    # 
    # def _augment_example(example: Dict[str, Any]) -> Dict[str, Any]:
    #     image_tensor = example.get("processed_image_tensor")
    #     if image_tensor is not None:
    #         # 为了使用 torchvision 的变换，需要先将 Tensor 转回 PIL Image
    #         image_pil = ToPILImage()(torch.tensor(image_tensor))
    #         augmented_image_pil = image_augment_transform(image_pil)
    #         # 将增强后的 PIL Image 转回 Tensor
    #         example["processed_image_tensor"] = ToTensor()(augmented_image_pil)
    #     
    #     # 此处还可以添加对字幕文本的增强逻辑
    #     # example["cleaned_captions"] = some_text_augmenter(example["cleaned_captions"])
    #     
    #     return example
    # 
    # logger.info("正在应用 VLM 数据增强...")
    # augmented_dataset = dataset.map(_augment_example, num_proc=1, desc="执行 VLM 数据增强")
    # logger.info("VLM 数据增强完成。")
    # return augmented_dataset
    # -------------------------------------------------------------------

    logger.warning("VLM 数据增强功能当前为占位符，未对数据进行实际修改。若需启用，请在 `augment_vlm_dataset` 函数中实现具体逻辑。")
    return dataset  # 按当前设计，返回未经修改的原始数据集

# END OF FILE: data_processing/process_vlm.py

====================
文件: .\data_processing\__init__.py
====================

# FILE: data_processing/__init__.py
# Bedrock: This file makes the directory a Python package.

# Mandate of Zero Ambiguity: By importing key functions, we define a clear
# public API for this package, making it easier for other parts of the system
# to use its functionality.

from .process_text import clean_text_dataset
from .process_vlm import process_vlm_dataset, conceptual_gpt4v_distillation
from .build_tokenizer import train_tokenizer

# END OF FILE: data_processing/__init__.py


====================
文件: .\scripts\run_dpo.sh
====================

# FILE: scripts/run_dpo.sh
#!/bin/bash
# Bedrock Protocol: One-click script to launch the DPO process.
set -e

echo "--- [Bedrock] Launching DPO Trainer ---"

# Ensure the SFT model path in the DPO config is correct before running.
# The script assumes the SFT training has already been completed.

accelerate launch src/trainers/dpo_trainer.py configs/training/finetune_dpo.yaml

echo "--- [Bedrock] DPO script finished. ---"
# END OF FILE: scripts/run_dpo.sh

====================
文件: .\scripts\run_evaluation.sh
====================

# FILE: scripts/run_evaluation.sh
#!/bin/bash
# Bedrock Protocol: Script to run model evaluation.
# This script runs our custom evaluation tool which performs qualitative checks
# and provides guidance for quantitative benchmarking.
set -e

echo "--- [Bedrock] Launching Model Evaluation ---"

# Usage: bash scripts/run_evaluation.sh <path_to_model_or_adapter>
# Example: bash scripts/run_evaluation.sh ./checkpoints/dpo-tinyllama-guanaco-cpu/final_model

# Default to the DPO model if no argument is provided, as it's the final stage.
MODEL_PATH=${1:-"./checkpoints/dpo-tinyllama-guanaco-cpu/final_model"}

echo "Evaluating Model Path: $MODEL_PATH"

# Execute the Python evaluation script
python src/evaluation/evaluate_llm.py "$MODEL_PATH"

echo "--- [Bedrock] Evaluation script finished. ---"

# END OF FILE: scripts/run_evaluation.sh

====================
文件: .\scripts\run_grpo.sh
====================

# FILE: scripts/run_grpo.sh
# Bedrock Protocol: One-click script to launch the GRPO process.
set -e

echo "--- [Bedrock] Launching GRPO Trainer ---"

# This script launches the GRPO trainer.
# It uses an SFT model as a starting point.

accelerate launch src/trainers/grpo_trainer.py configs/training/finetune_grpo.yaml

echo "--- [Bedrock] GRPO script finished. ---"

# END OF FILE: scripts/run_grpo.sh

====================
文件: .\scripts\run_inference.sh
====================

# FILE: scripts/run_inference.sh
#!/bin/bash
# Bedrock Protocol: Script to launch the model inference CLI.
set -e

echo "--- [Bedrock] Launching Model Inference CLI ---"

# Usage: bash scripts/run_inference.sh <path_to_model_or_adapter> [max_new_tokens]

MODEL_PATH=${1:-"./checkpoints/sft-tinyllama-guanaco/final_model"} # Default to SFT model
MAX_NEW_TOKENS=${2:-200} # Default max generation length

echo "Model Path: $MODEL_PATH"
echo "Max New Tokens: $MAX_NEW_TOKENS"

# Execute the Python inference script
python src/inference/inference.py "$MODEL_PATH" "$MAX_NEW_TOKENS"

echo "--- [Bedrock] Inference script finished. ---"
# END OF FILE: scripts/run_inference.sh

====================
文件: .\scripts\run_orpo.sh
====================

# FILE: scripts/run_orpo.sh
#!/bin/bash
# Bedrock Protocol: One-click script to launch the ORPO process.
set -e

echo "--- [Bedrock] Launching ORPO Trainer ---"

# This script launches the ORPO trainer.
# Ensure the configuration file points to the correct base model and dataset.

accelerate launch src/trainers/orpo_trainer.py configs/training/finetune_orpo.yaml

echo "--- [Bedrock] ORPO training finished. ---"
# END OF FILE: scripts/run_orpo.sh

====================
文件: .\scripts\run_ppo.sh
====================

# FILE: scripts/run_ppo.sh
#!/bin/bash
# Bedrock Protocol: One-click script to launch the PPO process (Conceptual).
set -e

echo "--- [Bedrock] Launching PPO Trainer (Conceptual) ---"

# This script runs the PPO trainer. The configuration file points to the SFT
# model as its starting point. Ensure SFT has been run successfully first.
# The trainer script is adapted to run on CPU if no GPU is available.

accelerate launch src/trainers/ppo_trainer.py configs/training/finetune_ppo.yaml

echo "--- [Bedrock] PPO script finished. ---"

# END OF FILE: scripts/run_ppo.sh

====================
文件: .\scripts\run_pretrain.sh
====================

# FILE: scripts/run_pretrain.sh
#!/bin/bash
# Bedrock Protocol: One-click script to launch the pre-training process.
# This script uses DeepSpeed, which is highly recommended for large-scale pre-training.

# Exit immediately if a command exits with a non-zero status.
set -e

echo "--- [Bedrock] Launching Pre-training Trainer ---"

# --- Recommended Usage with DeepSpeed ---
# Before running, ensure you have configured Accelerate for DeepSpeed.
# You can do this by running:
# accelerate config
# And selecting DeepSpeed as your distributed training method.
# For example:
# - What processor(s) do you have? `all_cuda`
# - What type of machine are you using? `multi-GPU` (or `multi-node` if applicable)
# - How many processes in total in your distributed setup? (e.g., 4 for 4 GPUs)
# - Do you want to use DeepSpeed? `yes`
# - Do you want to use the BF16 mixed precision? `yes` (if your GPU supports it, e.g., A100, RTX 30/40 series)
# - What DeepSpeed config do you want to use? `all` (for ZeRO-3) or `ZeRO-2`
# - You can also provide a custom DeepSpeed config JSON path.

# Command to run the pre-training script using accelerate.
# It automatically detects your configured DeepSpeed setup.
accelerate launch src/trainers/pretrain_trainer.py configs/training/pretrain_llm.yaml

echo "--- [Bedrock] Pre-training script finished. ---"
# END OF FILE: scripts/run_pretrain.sh

====================
文件: .\scripts\run_reward_model_trainer.sh
====================

# FILE: scripts/run_reward_model_trainer.sh
#!/bin/bash
# Bedrock Protocol: One-click script to launch the Reward Model training process.
set -e

echo "--- [Bedrock] Launching Reward Model Trainer ---"

# This script launches the Reward Model trainer.
# Ensure the configuration file points to the correct base model and dataset.

accelerate launch src/trainers/reward_model_trainer.py configs/training/train_reward_model.yaml

echo "--- [Bedrock] Reward Model training finished. ---"
# END OF FILE: scripts/run_reward_model_trainer.sh

====================
文件: .\scripts\run_sft.sh
====================

# FILE: scripts/run_sft.sh
#!/bin/bash
# Bedrock Protocol: One-click script to launch the SFT process.
set -e

echo "--- [Bedrock] Launching SFT Trainer ---"

# We use `accelerate launch` which is the standard, robust way to handle
# single-node multi-GPU or multi-node training, managed by Hugging Face Accelerate.
# It correctly sets up the distributed environment.

accelerate launch src/trainers/sft_trainer.py configs/training/finetune_sft.yaml

echo "--- [Bedrock] SFT script finished. ---"
# END OF FILE: scripts/run_sft.sh

====================
文件: .\src\__init__.py
====================

# FILE: src/__init__.py
# Bedrock: This file makes the directory a Python package.

# Mandate of Zero Ambiguity: By making this file minimal, we prevent
# unintended side-effects from cascading imports. Specific components
# should be imported directly from their modules, for example:
# `from src.trainers.dpo_trainer import run_dpo`
# This ensures that running one script does not load all other scripts,
# making the system more modular and robust.

# We can still register our custom models here, as this is a safe,
# project-wide operation.
from . import models

====================
文件: .\src\evaluation\evaluate_llm.py
====================

# FILE: src/evaluation/evaluate_llm.py
"""
Bedrock Protocol: LLM Evaluation Script.

This module provides a functional tool for performing qualitative (subjective)
evaluation by generating responses to a set of predefined prompts. It also serves
as a guide for conducting quantitative (objective) evaluation using standard
benchmarks like `lm-evaluation-harness`.
"""

import sys
import yaml
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from peft import PeftModel, PeftConfig
import os
import shutil  # For cleaning up temporary directories
import gc  # For garbage collection


def load_model_for_evaluation(model_path: str):
    """Loads a PEFT adapter or a full model and prepares it for evaluation."""
    print(f"\n[Model Loader] Loading model from: {model_path}")

    # [FIX] Define an offload directory for CPU memory issues during evaluation
    # Create a unique temporary directory for this run's offloading
    evaluation_output_dir = Path("./checkpoints/evaluation_temp_models")
    offload_folder = evaluation_output_dir / "offload_eval_model"
    os.makedirs(offload_folder, exist_ok=True)
    print(f"--> No GPU detected. Configuring for CPU-only execution with offload to: {offload_folder}")

    peft_adapter_dir = Path(model_path)
    is_peft_adapter = (peft_adapter_dir / "adapter_config.json").exists()

    base_model_name_or_path = ""
    if is_peft_adapter:
        try:
            peft_config = PeftConfig.from_pretrained(model_path)
            base_model_name_or_path = peft_config.base_model_name_or_path
            print(f"--> Detected PEFT adapter. Base model: {base_model_name_or_path}")
        except Exception as e:
            print(f"--> WARNING: Could not auto-detect base model from PEFT config ({e}).")
            # Fallback for our project structure
            base_model_name_or_path = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            print(f"--> Using fallback base model for our project: {base_model_name_or_path}")
    else:
        base_model_name_or_path = model_path
        print("--> Detected full model. Loading directly.")

    try:
        # Load the tokenizer first (no offloading needed for tokenizer)
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # [FIX] Load the base model with offloading
        print(f"--> Loading base model '{base_model_name_or_path}' with offloading...")
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name_or_path,
            torch_dtype=torch.float32,  # Use float32 for CPU
            device_map={"": "cpu"},
            trust_remote_code=True,
            attn_implementation="sdpa",
            offload_folder=str(offload_folder),  # Pass the offload directory
        )

        # If it's a PEFT adapter, load and merge it
        if is_peft_adapter:
            print("--> Applying and merging PEFT adapter (with offloading context)...")
            model = PeftModel.from_pretrained(model, model_path,
                                              offload_folder=str(offload_folder))  # Pass offload folder again
            model = model.merge_and_unload()
            print("--> LoRA weights merged into base model.")

        model.eval()
        print("[Model Loader] Model and tokenizer loaded successfully for evaluation.")
        return model, tokenizer, evaluation_output_dir  # Return the temp directory for cleanup
    except Exception as e:
        print(f"FATAL: Error loading model for evaluation: {e}")
        sys.exit(1)


def run_qualitative_evaluation(model, tokenizer):
    """Performs subjective evaluation by generating answers to predefined questions."""
    print("\n--- [Stage 1: Qualitative Evaluation (Manual Check)] ---")
    print("Generating responses for a set of predefined prompts...")

    prompts = [
        "What is the capital of France?",
        "Write a short, three-sentence horror story.",
        "Explain the concept of supervised fine-tuning in simple terms.",
        "Provide a python function to reverse a string.",
    ]

    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    for i, prompt_text in enumerate(prompts):
        print("\n" + "=" * 50)
        print(f"PROMPT {i + 1}/{len(prompts)}: {prompt_text}")
        print("=" * 50)

        chat = [{"role": "user", "content": prompt_text}]
        formatted_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)  # Move to model's device

        print(f"MODEL RESPONSE:")
        with torch.no_grad():
            model.generate(
                **inputs,
                streamer=streamer,
                max_new_tokens=150,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
            )
        print()  # for newline after streaming


def guide_quantitative_evaluation(model_path: str):
    """Provides guidance on how to run objective, benchmark-based evaluation."""
    print("\n--- [Stage 2: Quantitative Evaluation (Benchmark Guide)] ---")
    print("Quantitative evaluation measures model performance on standard academic benchmarks.")
    print("The industry-standard tool for this is `lm-evaluation-harness`.")
    print("\n**Why we don't run it directly here:**")
    print(
        " - Running benchmarks like MMLU or GSM8K is computationally intensive and can take hours or days, even on GPUs.")
    print(" - It requires a specific setup and large benchmark datasets to be downloaded.")

    print("\n**How to run it yourself (on a GPU machine):**")
    print("1. Install the tool:")
    print("   pip install lm-eval")
    print("\n2. Prepare your model:")
    print(
        "   - For our project, you first need a merged model. The qualitative evaluation above already uses a merged, in-memory model.")
    print("   - You would save this merged model to a new directory before running the harness.")

    print("\n3. Run the evaluation command:")
    print("   (This is a template, replace with your actual paths and desired tasks)")
    print(f"   lm_eval --model hf \\")
    print(f"       --model_args pretrained={model_path},dtype=float32 \\")
    print(f"       --tasks mmlu,gsm8k \\")
    print(f"       --device cpu \\")  # Keep CPU for general compatibility guide
    print(f"       --batch_size 1 \\")
    print(f"       --output_path ./evaluation_results.json")

    print("\nThis conceptual script has successfully loaded the model, which is the prerequisite for evaluation.")
    print("Please follow the steps above in a suitable environment to get official benchmark scores.")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run LLM evaluation.")
    parser.add_argument("model_path", type=str, help="Path to the model checkpoint or PEFT adapter.")
    args = parser.parse_args()

    # Main execution flow
    model_for_eval, tokenizer_for_eval, temp_dir = load_model_for_evaluation(args.model_path)
    run_qualitative_evaluation(model_for_eval, tokenizer_for_eval)
    guide_quantitative_evaluation(args.model_path)

    # [FIX] Clean up temporary offload directory
    if temp_dir.exists():
        print(f"\n[Cleanup] Cleaning up temporary evaluation directory: {temp_dir}")
        shutil.rmtree(temp_dir)
        gc.collect()

    print("\n--- [Bedrock] Evaluation Script Finished ---")

# END OF FILE: src/evaluation/evaluate_llm.py

====================
文件: .\src\evaluation\__init__.py
====================

# FILE: src/evaluation/__init__.py
# Bedrock: This file makes the directory a Python package.

# Mandate of Zero Ambiguity: Expose the primary evaluation function.
# [API IMPORT FIX] Removed 'run_evaluation' as it's no longer a direct export.
# The evaluate_llm.py script is meant to be run directly via command line.
# from .evaluate_llm import run_evaluation # Removed

# END OF FILE: src/evaluation/__init__.py

====================
文件: .\src\inference\inference.py
====================

# FILE: src/inference/inference.py
"""
Bedrock Protocol: Model Inference Script.

This script provides a simple command-line interface (CLI) for loading a
fine-tuned or pre-trained language model and interacting with it.
Includes streaming for real-time feedback.
"""

import torch
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

# Ensure our custom model is registered when this module is run
import src.models as models


def run_inference_cli(model_path: str, max_new_tokens: int = 20) -> None:  # <<< MODIFIED: Default tokens reduced to 20
    """
    Loads a model and provides a CLI for interaction with real-time streaming.

    Args:
        model_path: Path to the directory containing the model checkpoint.
        max_new_tokens: Maximum number of tokens to generate per response.
                        Default is low for fast CPU feedback.
    """
    print("--- [Bedrock] Starting Model Inference CLI ---")
    print(f"Attempting to load model from: {model_path}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
        print("Tokenizer loaded.")

        print(f"Loading model with AutoModelForCausalLM from: {model_path}")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16 if 'cuda' in str(
                torch.device("cuda" if torch.cuda.is_available() else "cpu")) and torch.cuda.get_device_capability()[
                                              0] >= 8 else torch.float16,
            trust_remote_code=True,
            device_map="auto"
        )
        print("Model loaded successfully using AutoModel.")

        model.eval()
        print("Model set to evaluation mode. Ready for inference.")

    except Exception as e:
        print(f"Fatal error loading model or tokenizer: {e}")
        print("\nPlease ensure the model path is correct and all dependencies are installed.")
        sys.exit(1)

    # +++ START OF THE FIX: Add a TextStreamer +++
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    # +++ END OF THE FIX +++

    print("\n--- Start Chatting (type 'exit' to quit) ---")
    while True:
        try:
            user_input = input("You: ")
            if user_input.lower() == 'exit':
                break

            inputs = tokenizer(user_input, return_tensors="pt").to(model.device)

            # Use a dictionary for generation arguments for clarity
            generation_kwargs = {
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs["attention_mask"],
                "streamer": streamer,  # <<< MODIFIED: Pass the streamer here
                "max_new_tokens": max_new_tokens,
                "do_sample": True,
                "top_p": 0.9,
                "temperature": 0.7,
                "eos_token_id": tokenizer.eos_token_id,
                "pad_token_id": tokenizer.pad_token_id
            }

            print("Model: ", end="", flush=True)  # Print the prompt for the streamer

            with torch.no_grad():
                # .generate() will now print tokens as they are created
                model.generate(**generation_kwargs)

            # The streamer handles the printing, so we just need a newline
            print()

        except KeyboardInterrupt:
            print("\nExiting chat.")
            break
        except Exception as e:
            print(f"\nAn error occurred during generation: {e}")
            continue


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run LLM inference CLI.")
    parser.add_argument("model_path", type=str, help="Path to the model checkpoint.")
    # MODIFIED: Changed default to a smaller, CPU-friendly value
    parser.add_argument("max_new_tokens", nargs='?', type=int, default=20,
                        help="Maximum number of tokens to generate.")

    args = parser.parse_args()

    run_inference_cli(args.model_path, args.max_new_tokens)

====================
文件: .\src\inference\__init__.py
====================

# FILE: src/inference/__init__.py
# Bedrock: This file makes the directory a Python package.
# Mandate of Zero Ambiguity: Expose the primary inference function.
from .inference import run_inference_cli
# END OF FILE: src/inference/__init__.py

====================
文件: .\src\models\ffn.py
====================

# FILE: src/models/ffn.py
"""
Bedrock Protocol: Feed-Forward Network (FFN) implementations.

This module provides standard FFN layers, including support for various activation
functions, notably SwiGLU, which is common in modern LLMs.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class FFN(nn.Module):
    """
    Implements a standard Feed-Forward Network layer.
    Supports various activation functions including SwiGLU.
    """

    def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str):
        super().__init__()
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.hidden_act = hidden_act

        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

        # Mandate of Intentionality: Dynamically select activation function.
        if hidden_act == "silu":
            self.act_fn = nn.SiLU()
        elif hidden_act == "relu":
            self.act_fn = nn.ReLU()
        elif hidden_act == "gelu":
            self.act_fn = nn.GELU()
        elif hidden_act == "swiglu":
            # For SwiGLU, the gate_proj and up_proj are effectively part of the
            # same block. We'll handle it inside forward.
            self.act_fn = nn.SiLU()  # SiLU is part of SwiGLU
            # The structure for SwiGLU is (input -> gate_proj * act_fn(up_proj) -> down_proj)
            # In Llama, gate_proj and up_proj are parallel, then multiplied.
            # We already defined gate_proj and up_proj as separate layers above.
        else:
            raise ValueError(f"Unsupported activation function: {hidden_act}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the FFN layer.

        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_size).

        Returns:
            Output tensor of shape (batch_size, seq_len, hidden_size).
        """
        if self.hidden_act == "swiglu":
            # SwiGLU implementation: (input * SiLU(input)) @ W_down
            # Llama uses this structure: gate_proj(x) * act_fn(up_proj(x))
            # Where act_fn is SiLU.
            hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)
        else:
            # Standard FFN: act_fn(gate_proj(x)) @ W_down
            hidden_states = self.act_fn(self.gate_proj(x))

        return self.down_proj(hidden_states)

# END OF FILE: src/models/ffn.py

====================
文件: .\src\models\language_model.py
====================

# FILE: src/models/language_model.py
"""
Bedrock Protocol: Core Language Model (LLM/VLM) construction.

This module provides the `BaseLLM` class, which serves as the central factory
for assembling various architectural components (Attention, FFN, MoE) into
a complete Transformer-based language model. It's designed to be highly
configurable via a dictionary, adhering to the "industrial-grade" principle.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, Tuple, List

from transformers import PreTrainedModel, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers.generation import GenerationMixin

from src.models.attention.standard_attention import StandardAttention
from src.models.attention.flash_attention import FlashAttention
from src.models.ffn import FFN
from src.models.moe import MoE


class BaseLLMConfig(PretrainedConfig):
    model_type = "BaseLLM"

    def __init__(
            self,
            vocab_size=32000,
            hidden_size=1024,
            intermediate_size=4096,
            num_hidden_layers=4,
            num_attention_heads=16,
            num_key_value_heads=16,  # Added num_key_value_heads for GQA/MQA support
            hidden_act="silu",
            max_position_embeddings=2048,
            attention_type="standard",
            model_type_llm="DenseLLM",
            tie_word_embeddings=True,
            is_decoder=True,
            is_encoder_decoder=False,
            # MoE Params
            num_experts=8,
            num_experts_per_tok=2,
            router_aux_loss_coef=0.001,
            # VLM Params
            is_vlm=False,
            num_image_tokens=256,
            vision_encoder_output_dim=768,
            **kwargs
    ):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.num_key_value_heads = num_key_value_heads  # Store in config
        self.hidden_act = hidden_act
        self.max_position_embeddings = max_position_embeddings
        self.attention_type = attention_type
        self.model_type_llm = model_type_llm
        self.num_experts = num_experts
        self.num_experts_per_tok = num_experts_per_tok
        self.router_aux_loss_coef = router_aux_loss_coef
        self.is_vlm = is_vlm
        self.num_image_tokens = num_image_tokens
        self.vision_encoder_output_dim = vision_encoder_output_dim

        super().__init__(tie_word_embeddings=tie_word_embeddings, is_decoder=is_decoder,
                         is_encoder_decoder=is_encoder_decoder, **kwargs)


class VisionEncoderDummy(nn.Module):
    def __init__(self, output_dim: int = 768, num_image_tokens: int = 256):
        super().__init__()
        self.output_dim = output_dim
        self.num_image_tokens = num_image_tokens
        self.conv = nn.Conv2d(3, output_dim // 4, kernel_size=16, stride=16)
        self.pool = nn.AdaptiveAvgPool2d((1, num_image_tokens))
        self.linear = nn.Linear(output_dim // 4, output_dim)
        print(f"--- Bedrock: Initialized Conceptual Vision Encoder...")

    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:
        if pixel_values.dim() != 4 or pixel_values.shape[1] != 3:
            # Return dummy output if input is not valid, matching expected shape
            batch_size = pixel_values.shape[0] if pixel_values.dim() >= 1 else 1
            return torch.zeros(batch_size, self.num_image_tokens, self.output_dim,
                               device=pixel_values.device, dtype=pixel_values.dtype)
        features = self.conv(pixel_values)
        features = self.pool(features)
        features = features.squeeze(2).permute(0, 2, 1)  # Permute to (B, S, D)
        features = self.linear(features)
        return features


class TransformerBlock(nn.Module):
    def __init__(self, config: BaseLLMConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.attention_type = config.attention_type
        self.input_layernorm = nn.LayerNorm(self.hidden_size, eps=1e-5)

        if self.attention_type == "flash":
            self.self_attn = FlashAttention(self.hidden_size, config.num_attention_heads, config.num_key_value_heads)
        else:
            self.self_attn = StandardAttention(self.hidden_size, config.num_attention_heads, config.num_key_value_heads)

        self.post_attention_layernorm = nn.LayerNorm(self.hidden_size, eps=1e-5)

        if config.model_type_llm == "MoELLM":
            self.mlp = MoE(hidden_size=self.hidden_size, intermediate_size=config.intermediate_size,
                           hidden_act=config.hidden_act, num_experts=config.num_experts,
                           num_experts_per_tok=config.num_experts_per_tok,
                           router_aux_loss_coef=config.router_aux_loss_coef)
        else:
            self.mlp = FFN(hidden_size=self.hidden_size, intermediate_size=config.intermediate_size,
                           hidden_act=config.hidden_act)

    def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None,
            past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # Added for KV caching
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], Tuple[
        torch.Tensor, torch.Tensor]]:  # Added present_key_value to return tuple

        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # Pass past_key_value and receive present_key_value from attention
        attn_output, present_key_value = self.self_attn(
            hidden_states,
            attention_mask=attention_mask,
            past_key_value=past_key_value
        )

        hidden_states = residual + attn_output
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_output_or_tuple = self.mlp(hidden_states)

        if isinstance(mlp_output_or_tuple, tuple):
            mlp_output, aux_loss_dict = mlp_output_or_tuple
        else:
            mlp_output = mlp_output_or_tuple
            aux_loss_dict = {}

        hidden_states = residual + mlp_output
        return hidden_states, aux_loss_dict, present_key_value  # Return present_key_value


class BaseLLM(PreTrainedModel, GenerationMixin):
    config_class = BaseLLMConfig
    _no_split_modules = ["TransformerBlock"]
    main_input_name = "input_ids"

    def __init__(self, config: BaseLLMConfig):
        super().__init__(config)
        self.config = config
        self.vocab_size = config.vocab_size
        self.hidden_size = config.hidden_size

        self.embed_tokens = nn.Embedding(self.vocab_size, self.hidden_size)
        self.vision_encoder = None
        self.vision_projector = None

        if config.is_vlm:
            self.vision_encoder = VisionEncoderDummy(output_dim=config.vision_encoder_output_dim,
                                                     num_image_tokens=config.num_image_tokens)
            self.vision_projector = nn.Sequential(nn.Linear(config.vision_encoder_output_dim, self.hidden_size),
                                                  nn.SiLU(), nn.Linear(self.hidden_size, self.hidden_size))

        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_hidden_layers)])
        self.norm = nn.LayerNorm(self.hidden_size, eps=1e-5)
        self.lm_head = nn.Linear(self.hidden_size, self.vocab_size, bias=False)

        # Initialize weights and apply tie_word_embeddings
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs) -> Dict[str, Any]:
        """
        Prepares model inputs for the next generation step. This is a crucial method
        for compatibility with the .generate() function.
        """
        # If past_key_values are provided, it means we are in incremental decoding.
        # input_ids will usually only contain the last generated token.
        # We need to ensure that the attention_mask matches the current total sequence length.

        # If pixel_values were previously passed and are now implicitly part of past_key_values,
        # they should not be passed again.
        # For simplicity, assume pixel_values are ONLY passed at the first forward call.

        model_inputs = {"input_ids": input_ids}

        # The `generate` method passes the `attention_mask` inside `kwargs`.
        # This attention_mask is already correctly padded and expanded by the `generate` method
        # to match the combined length of past tokens and current input_ids.
        model_inputs["attention_mask"] = kwargs.get("attention_mask", None)

        # Pass past_key_values to the forward method
        model_inputs["past_key_values"] = kwargs.get("past_key_values", None)

        # Ensure use_cache is passed for generation.
        model_inputs["use_cache"] = kwargs.get("use_cache", self.config.use_cache)

        # For VLM, pixel_values are typically only for the first token.
        # If past_key_values exist, it means we are continuing from a previous step,
        # so pixel_values should not be re-processed.
        if "pixel_values" in kwargs and kwargs["pixel_values"] is not None and kwargs.get("past_key_values") is None:
            model_inputs["pixel_values"] = kwargs["pixel_values"]

        return model_inputs

    def forward(
            self,
            input_ids: Optional[torch.LongTensor] = None,  # Make optional for VLM-only first pass
            attention_mask: Optional[torch.Tensor] = None,  # Mask for the full current sequence
            pixel_values: Optional[torch.Tensor] = None,
            past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
            # List of (key, value) for each layer
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,  # If True, past_key_values will be returned
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
    ) -> CausalLMOutputWithPast:

        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is None and pixel_values is None:
            raise ValueError("You must specify either input_ids or pixel_values (or both).")

        batch_size = input_ids.shape[0] if input_ids is not None else pixel_values.shape[0]
        # Current sequence length of text tokens only (excluding image tokens if VLM)
        current_text_seq_len = input_ids.shape[1] if input_ids is not None else 0

        aux_losses = {}

        # 1. Embed tokens
        if input_ids is not None:
            text_hidden_states = self.embed_tokens(input_ids)
        else:
            text_hidden_states = None  # No text input

        # 2. Process Vision Features (if VLM and first pass)
        # Vision features are prepended only on the very first forward pass.
        # Subsequent calls during generation will have past_key_values containing them.
        vision_features_prepended = False
        if self.config.is_vlm and self.vision_encoder is not None and pixel_values is not None and past_key_values is None:
            image_features = self.vision_encoder(pixel_values)
            projected_image_features = self.vision_projector(image_features)

            if text_hidden_states is not None:
                combined_hidden_states = torch.cat((projected_image_features, text_hidden_states), dim=1)
            else:
                combined_hidden_states = projected_image_features  # VLM-only input

            vision_features_prepended = True

            # Create or extend attention mask to include image tokens for the FIRST pass
            if attention_mask is not None:
                # current attention_mask is for input_ids. Need to prepend image mask.
                image_attention_mask = torch.ones(batch_size, self.config.num_image_tokens,
                                                  dtype=attention_mask.dtype, device=attention_mask.device)
                attention_mask = torch.cat((image_attention_mask, attention_mask), dim=1)
            else:
                # If no text input and no initial mask, create full mask including image tokens.
                attention_mask = torch.ones(batch_size, self.config.num_image_tokens + current_text_seq_len,
                                            dtype=torch.long, device=self.device)
        elif text_hidden_states is not None:
            combined_hidden_states = text_hidden_states
        else:  # Neither text nor initial image input
            raise ValueError("No input (input_ids or pixel_values) provided for model forward pass.")

        # 3. Transformer Layers with KV Caching
        present_key_values = [] if use_cache else None

        # `position_ids` usually handled by `transformers.modeling_utils._get_position_ids` or similar for `generate`.
        # For our custom model, we assume the `attention_mask` is correctly extended by `generate()`
        # or manually constructed here for the full sequence at each step.

        hidden_states_in_blocks = combined_hidden_states
        for i, layer in enumerate(self.layers):
            layer_past_key_value = past_key_values[i] if past_key_values else None

            layer_output, layer_aux_losses, current_layer_present_kv = layer(
                hidden_states_in_blocks,
                attention_mask=attention_mask,
                past_key_value=layer_past_key_value
            )
            hidden_states_in_blocks = layer_output
            aux_losses.update({f"layer_{i}_{k}": v for k, v in layer_aux_losses.items()})
            if use_cache:
                present_key_values.append(current_layer_present_kv)

        final_hidden_states = self.norm(hidden_states_in_blocks)

        # 4. Language Model Head
        # If VLM, the text logits correspond only to the text portion of the sequence.
        if self.config.is_vlm and vision_features_prepended:
            # If vision features were just prepended, the logits should only be calculated
            # for the *text* part of the sequence for causal LM loss.
            logits = self.lm_head(final_hidden_states[:, self.config.num_image_tokens:, :])
        elif self.config.is_vlm and past_key_values is not None:
            # If VLM and using cache (i.e., not first pass), input_ids is likely 1 token.
            # The `final_hidden_states` will be `(B, 1, D)`.
            # The VLM prefix is implicitly handled by the KV cache.
            logits = self.lm_head(final_hidden_states)
        else:  # Pure LLM or VLM's subsequent generation steps without new image input
            logits = self.lm_head(final_hidden_states)

        # 5. Calculate Loss
        loss = None
        if labels is not None:
            # Shift predictions and labels for causal language modeling
            # If VLM and vision_features_prepended, labels should also align to text part.
            if self.config.is_vlm and vision_features_prepended:
                # Labels correspond to text_hidden_states, so need to shift them too.
                # The labels should already be for the text sequence only.
                # Example: labels for [IMG_TOKENS, TEXT_TOKENS] would be [IGNORE, IGNORE, ..., TEXT_LABELS]
                # We expect labels to already be prepared for the text part.
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()  # labels should start from first text token
            else:
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()

            loss = F.cross_entropy(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1),
                                   ignore_index=getattr(self.config, 'pad_token_id', -100))

        if aux_losses:
            total_aux_loss = sum(aux_losses.values())
            if loss is not None:
                loss += total_aux_loss
            else:
                loss = total_aux_loss

        # 6. Return Output
        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=present_key_values,  # Return present_key_values for caching
            hidden_states=None,  # For simplicity, not returning these for now
            attentions=None,  # For simplicity, not returning these for now
        )

# END OF /src/models/language_model.py

====================
文件: .\src\models\moe.py
====================

# FILE: src/models/moe.py
"""
Bedrock Protocol: Mixture-of-Experts (MoE) Layer implementation.

This module provides a basic Sparse MoE layer, including a router and
multiple expert FFNs. It's a key component for building scalable and
efficient large language models.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List

from src.models.ffn import FFN  # MoE layers typically contain FFNs as experts


class MoE(nn.Module):
    """
    Implements a basic Mixture-of-Experts (MoE) layer.

    This layer routes tokens to a subset of experts (Feed-Forward Networks).
    It includes a router for sparse activation and an optional auxiliary loss
    for load balancing.
    """

    def __init__(
            self,
            hidden_size: int,
            intermediate_size: int,
            hidden_act: str,
            num_experts: int,
            num_experts_per_tok: int,
            router_aux_loss_coef: float = 0.001
    ):
        super().__init__()
        # Mandate of Intentionality: All MoE-specific parameters are clearly defined.
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.hidden_act = hidden_act
        self.num_experts = num_experts
        self.num_experts_per_tok = num_experts_per_tok
        self.router_aux_loss_coef = router_aux_loss_coef

        # Router: Maps input tokens to expert probabilities.
        # This is a linear layer that outputs num_experts scores for each token.
        self.gate = nn.Linear(hidden_size, num_experts, bias=False)

        # Experts: A list of independent FFN modules.
        self.experts = nn.ModuleList([
            FFN(hidden_size, intermediate_size, hidden_act)
            for _ in range(num_experts)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Vectorized and efficient forward pass for the MoE layer.
        """
        batch_size, seq_len, hidden_size = x.size()
        flat_x = x.view(-1, hidden_size)
        num_tokens = flat_x.shape[0]

        router_logits = self.gate(flat_x)
        router_weights = F.softmax(router_logits, dim=-1, dtype=torch.float32).to(x.dtype)
        top_k_weights, top_k_indices = torch.topk(router_weights, self.num_experts_per_tok, dim=-1)
        top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)

        load_balancing_loss = self._calculate_load_balancing_loss(router_weights, top_k_indices)

        final_output = torch.zeros_like(flat_x)
        
        # Create a flat list of tokens that are dispatched to experts
        flat_top_k_indices = top_k_indices.flatten()
        
        # Create a mask for combining expert outputs
        expert_mask = torch.nn.functional.one_hot(top_k_indices, num_classes=self.num_experts).permute(2, 0, 1)

        # Loop over experts (this loop is small and acceptable)
        for expert_idx, expert_layer in enumerate(self.experts):
            # Find which tokens are routed to this expert
            # token_indices: a boolean tensor of shape (num_tokens, num_experts_per_tok)
            token_indices = (top_k_indices == expert_idx)
            
            # Get the indices of tokens that go to this expert
            # selected_tokens: a 1D tensor of indices
            selected_tokens = token_indices.any(dim=1).nonzero(as_tuple=True)[0]

            if selected_tokens.numel() > 0:
                # Get the corresponding input tokens
                expert_input = flat_x[selected_tokens]
                
                # Pass through the expert
                expert_output = expert_layer(expert_input)

                # Get the weights for these tokens
                weights = top_k_weights[token_indices]
                
                # Weighted sum of expert outputs
                final_output.index_add_(0, selected_tokens, (expert_output.T * weights).T)

        return final_output.view(batch_size, seq_len, hidden_size), {
            "router_aux_loss": load_balancing_loss * self.router_aux_loss_coef
        }

    def _calculate_load_balancing_loss(self, router_weights: torch.Tensor, top_k_indices: torch.Tensor) -> torch.Tensor:
        """
        Calculates a simplified load balancing loss for the router.
        Encourages experts to receive roughly equal traffic.
        """
        # router_weights: (num_tokens, num_experts) after softmax
        # top_k_indices: (num_tokens, num_experts_per_tok)

        # Probability of a token being routed to an expert
        expert_load = torch.zeros(self.num_experts, device=router_weights.device, dtype=router_weights.dtype)
        for i in range(router_weights.size(0)):  # Iterate through tokens
            for k_idx in range(self.num_experts_per_tok):
                expert_idx = top_k_indices[i, k_idx].item()
                expert_load[expert_idx] += router_weights[i, expert_idx]

        # P_expert: Average probability of routing to an expert
        P_expert = expert_load / router_weights.size(0)

        # P_token_expert: Proportion of tokens routed to each expert
        # Count how many tokens were actually routed to each expert
        tokens_per_expert = torch.bincount(top_k_indices.flatten(), minlength=self.num_experts)
        P_token_expert = tokens_per_expert.float() / top_k_indices.numel()  # num_tokens * num_experts_per_tok

        # Load balancing loss: variance of expert loads
        # This is a simplified version. More complex losses (e.g., in Switch Transformers) exist.
        # It encourages P_expert and P_token_expert to be proportional.
        # This term tries to minimize the product of P_expert and P_token_expert
        # when they are not well-balanced.
        loss = (P_expert * P_token_expert).sum() * self.num_experts

        return loss

# END OF FILE: src/models/moe.py

====================
文件: .\src\models\__init__.py
====================

# FILE: src/models/__init__.py
# Bedrock: This file makes the directory a Python package.

# +++ START OF FINAL ELEGANT FIX +++
# **终极解决方案**: 向 Transformers 框架注册我们的自定义模型。
# 这是让 `AutoModelForCausalLM.from_pretrained()` 能够识别和加载
# 本地自定义 `BaseLLM` 类的最规范、最稳健的方法。
from transformers import AutoConfig, AutoModelForCausalLM
from .language_model import BaseLLM, BaseLLMConfig

# 注册配置类
AutoConfig.register("BaseLLM", BaseLLMConfig)
# 注册模型类
AutoModelForCausalLM.register(BaseLLMConfig, BaseLLM)
# +++ END OF FINAL ELEGANT FIX +++


# Mandate of Zero Ambiguity: Expose key components for direct import.
from .attention.standard_attention import StandardAttention
from .attention.flash_attention import FlashAttention
from .ffn import FFN
from .moe import MoE
from .language_model import BaseLLM

====================
文件: .\src\models\attention\flash_attention.py
====================

# FILE: src/models/attention/flash_attention.py
"""
Bedrock Protocol: FlashAttention wrapper.

This module provides an abstraction layer for FlashAttention. It conditionally
uses the optimized FlashAttention implementation if available and compatible,
otherwise falls back to a standard PyTorch SDPA (Scaled Dot-Product Attention).
This embodies the "Mandate of Proactive Defense" by providing a robust fallback.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple

# Attempt to import FlashAttention. This will only succeed if `flash-attn` is installed
# and the CUDA environment is set up correctly for its compilation.
try:
    from flash_attn import flash_attn_func

    _has_flash_attn = True
except ImportError:
    _has_flash_attn = False
    print("Warning: FlashAttention not found or cannot be imported. Falling back to PyTorch SDPA.")


class FlashAttention(nn.Module):
    """
    Wrapper for FlashAttention, with a fallback to PyTorch's native SDPA.
    Supports KV caching and GQA/MQA.
    """

    def __init__(self, hidden_size: int, num_attention_heads: int, num_key_value_heads: Optional[int] = None):
        super().__init__()
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads  # Default to MHA

        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.num_key_value_heads = num_key_value_heads
        self.head_dim = hidden_size // num_attention_heads

        if hidden_size % num_attention_heads != 0:
            raise ValueError(
                f"hidden_size ({hidden_size}) must be divisible by num_attention_heads ({num_attention_heads})"
            )
        if hidden_size % num_key_value_heads != 0:
            raise ValueError(
                f"hidden_size ({hidden_size}) must be divisible by num_key_value_heads ({num_key_value_heads})"
            )

        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads

        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)

        # Check for FlashAttention availability and CUDA capability (Ampere+ for optimal performance)
        self.has_flash_attn = _has_flash_attn and torch.cuda.is_available() and \
                              torch.cuda.get_device_capability()[0] >= 8  # Check major compute capability >= 8 (Ampere)

        if self.has_flash_attn:
            print(f"Info: FlashAttention will be used on this device ({torch.cuda.get_device_name()}).")
        else:
            print(
                f"Info: Falling back to PyTorch SDPA for attention (FlashAttention not available/compatible or GPU < Ampere).")

    def _split_heads(self, x: torch.Tensor, num_heads: int) -> torch.Tensor:
        """
        Splits the input tensor into multiple heads.
        Input shape: (batch_size, seq_len, hidden_size) or (batch_size, seq_len, num_kv_heads * head_dim)
        Output shape: (batch_size, seq_len, num_heads, head_dim) # This is how flash_attn_func expects it.
        """
        new_shape = x.size()[:-1] + (num_heads, self.head_dim)
        x = x.view(new_shape)
        return x  # (batch_size, seq_len, num_heads, head_dim)

    def _repeat_kv(self, hidden_states: torch.Tensor, num_kv_groups: int) -> torch.Tensor:
        """
        Repeats K/V heads for Grouped-Query Attention (GQA).
        Input shape: (batch_size, seq_len, num_key_value_heads, head_dim)
        Output shape: (batch_size, seq_len, num_attention_heads, head_dim)
        """
        if num_kv_groups == 1:
            return hidden_states
        batch_size, seq_len, num_kv_heads, head_dim = hidden_states.shape
        hidden_states = hidden_states[:, :, None, :, :].expand(batch_size, seq_len, num_kv_heads, num_kv_groups,
                                                               head_dim)
        return hidden_states.reshape(batch_size, seq_len, num_kv_heads * num_kv_groups, head_dim)

    def forward(
            self,
            hidden_states: torch.Tensor,  # (batch_size, q_seq_len, hidden_size)
            attention_mask: Optional[torch.Tensor] = None,
            # (batch_size, current_total_seq_len) from transformers.generate()
            past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # (key_past, value_past) for each layer
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:  # (attn_output, (key_present, value_present))
        """
        Forward pass for FlashAttention (or fallback to PyTorch SDPA) with KV caching.

        Args:
            hidden_states: Input tensor of shape (batch_size, q_seq_len, hidden_size).
                           q_seq_len is typically 1 during incremental decoding.
            attention_mask: Optional padding mask (batch_size, current_total_seq_len).
                            0 for padded tokens, 1 for valid tokens.
            past_key_value: Optional tuple of (key_states, value_states) from previous steps.

        Returns:
            Output tensor of shape (batch_size, q_seq_len, hidden_size), and present_key_value.
        """
        batch_size, q_seq_len, _ = hidden_states.shape

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Split heads: (B, S, D_proj) -> (B, S, H, D_head)
        query_states = self._split_heads(query_states, self.num_attention_heads)
        key_states = self._split_heads(key_states, self.num_key_value_heads)
        value_states = self._split_heads(value_states, self.num_key_value_heads)

        # Handle KV caching: concatenate past and current K/V states
        if past_key_value is not None:
            # past_key_value: (key_past, value_past) where key_past.shape = (B, S_past, H_kv, D_head)
            key_states = torch.cat((past_key_value[0], key_states), dim=1)  # Concat along sequence dimension
            value_states = torch.cat((past_key_value[1], value_states), dim=1)

        # Apply GQA/MQA repetition for keys and values
        key_states = self._repeat_kv(key_states, self.num_key_value_groups)  # (B, S_total, H_attn, D_head)
        value_states = self._repeat_kv(value_states, self.num_key_value_groups)  # (B, S_total, H_attn, D_head)

        # present_key_value for the next step of generation
        present_key_value = (key_states, value_states)

        if self.has_flash_attn:
            # FlashAttention requires QKV to be in (B, S, H, D) format
            # `causal=True` directly enforces causal masking within FlashAttention.
            # key_padding_mask is for (B, S_total) where True means padded.
            fa_key_padding_mask = None
            if attention_mask is not None:
                fa_key_padding_mask = (attention_mask == 0)  # attention_mask: 1 is valid, 0 is padded.
                # fa_key_padding_mask: True is padded.

            attn_output = flash_attn_func(
                query_states,  # (B, S_q, H, D)
                key_states,  # (B, S_total, H, D)
                value_states,  # (B, S_total, H, D)
                dropout_p=0.0,
                causal=True,
                key_padding_mask=fa_key_padding_mask
            )  # (B, S_q, H, D)
        else:
            # Fallback to PyTorch's native Scaled Dot-Product Attention (SDPA)
            # SDPA expects Q, K, V in (B, H, S, D)
            # The _split_heads for standard attention permutes to (B, H, S, D),
            # but for flash attention it keeps (B, S, H, D).
            # So, need to permute for SDPA here.
            query_sdpa = query_states.permute(0, 2, 1, 3)  # (B, H, S_q, D)
            key_sdpa = key_states.permute(0, 2, 1, 3)  # (B, H, S_total, D)
            value_sdpa = value_states.permute(0, 2, 1, 3)  # (B, H, S_total, D)

            additive_attention_mask = None
            if attention_mask is not None:
                # (batch_size, 1, 1, current_total_seq_len) to broadcast.
                # attention_mask: 0 for padded tokens, 1 for valid tokens.
                additive_attention_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * torch.finfo(
                    query_sdpa.dtype).min
                additive_attention_mask = additive_attention_mask.to(query_sdpa.dtype)

            attn_output_sdpa = F.scaled_dot_product_attention(
                query_sdpa,
                key_sdpa,
                value_sdpa,
                attn_mask=additive_attention_mask,
                is_causal=True,
            )  # (B, H, S_q, D)
            attn_output = attn_output_sdpa.permute(0, 2, 1, 3)  # Permute back to (B, S_q, H, D) for merging heads

        # Merge heads
        # attn_output is (batch_size, q_seq_len, num_heads, head_dim)
        attn_output = attn_output.contiguous().view(batch_size, q_seq_len, self.hidden_size)

        # Output projection
        output = self.o_proj(attn_output)
        return output, present_key_value

# END OF FILE src/models/attention/flash_attention.py

====================
文件: .\src\models\attention\standard_attention.py
====================

# FILE: src/models/attention/standard_attention.py
"""
Bedrock Protocol: Standard Scaled Dot-Product Attention implementation.

This module provides a robust and clear implementation of the attention mechanism,
serving as a foundational component for our Transformer models.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple


class StandardAttention(nn.Module):
    """
    Implements standard multi-head self-attention, supporting KV caching and GQA/MQA.
    """

    def __init__(self, hidden_size: int, num_attention_heads: int, num_key_value_heads: Optional[int] = None):
        super().__init__()
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads  # Default to MHA

        if hidden_size % num_attention_heads != 0:
            raise ValueError(
                f"hidden_size ({hidden_size}) must be divisible by num_attention_heads ({num_attention_heads})"
            )
        if hidden_size % num_key_value_heads != 0:
            raise ValueError(
                f"hidden_size ({hidden_size}) must be divisible by num_key_value_heads ({num_key_value_heads})"
            )

        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.num_key_value_heads = num_key_value_heads
        self.head_dim = hidden_size // num_attention_heads
        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads

        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)

    def _split_heads(self, x: torch.Tensor, num_heads: int) -> torch.Tensor:
        """
        Splits the input tensor into multiple heads.
        Input shape: (batch_size, seq_len, hidden_size) or (batch_size, seq_len, num_kv_heads * head_dim)
        Output shape: (batch_size, num_heads, seq_len, head_dim)
        """
        new_shape = x.size()[:-1] + (num_heads, self.head_dim)
        x = x.view(new_shape)
        return x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)

    def _repeat_kv(self, hidden_states: torch.Tensor, num_kv_groups: int) -> torch.Tensor:
        """
        Repeats K/V heads for Grouped-Query Attention (GQA).
        Input shape: (batch_size, num_key_value_heads, seq_len, head_dim)
        Output shape: (batch_size, num_attention_heads, seq_len, head_dim)
        """
        if num_kv_groups == 1:
            return hidden_states
        batch_size, num_kv_heads, seq_len, head_dim = hidden_states.shape
        hidden_states = hidden_states[:, :, None, :, :].expand(batch_size, num_kv_heads, num_kv_groups, seq_len,
                                                               head_dim)
        return hidden_states.reshape(batch_size, num_kv_heads * num_kv_groups, seq_len, head_dim)

    def forward(
            self,
            hidden_states: torch.Tensor,  # (batch_size, seq_len, hidden_size)
            attention_mask: Optional[torch.Tensor] = None,
            # (batch_size, current_total_seq_len) from transformers.generate()
            past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # (key_past, value_past)
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:  # (attn_output, (key_present, value_present))
        """
        Forward pass for standard multi-head self-attention with KV caching.
        """
        batch_size, q_seq_len, _ = hidden_states.shape

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = self._split_heads(query_states, self.num_attention_heads)  # (B, H_attn, S_q, D)
        key_states = self._split_heads(key_states, self.num_key_value_heads)  # (B, H_kv, S_kv, D)
        value_states = self._split_heads(value_states, self.num_key_value_heads)  # (B, H_kv, S_kv, D)

        if past_key_value is not None:
            # past_key_value: (key_past, value_past) where key_past.shape = (B, H_kv, S_past, D)
            key_states = torch.cat((past_key_value[0], key_states), dim=2)
            value_states = torch.cat((past_key_value[1], value_states), dim=2)

        # Apply GQA/MQA repetition for keys and values
        key_states = self._repeat_kv(key_states, self.num_key_value_groups)  # (B, H_attn, S_total, D)
        value_states = self._repeat_kv(value_states, self.num_key_value_groups)  # (B, H_attn, S_total, D)

        # present_key_value for the next step of generation
        present_key_value = (key_states, value_states)

        # Create attention mask.
        # attention_mask from generate() is (batch_size, current_total_seq_len) where 0 is padding.
        # F.scaled_dot_product_attention expects additive_bias (B, 1, S_Q, S_KV) or (B, 1, S_Q, S_KV) for causal.
        # Since is_causal=True handles causality, we only need to convert padding mask.
        additive_attention_mask = None
        if attention_mask is not None:
            # (batch_size, 1, 1, current_total_seq_len)
            additive_attention_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * torch.finfo(
                query_states.dtype).min
            additive_attention_mask = additive_attention_mask.to(query_states.dtype)

        attn_output = F.scaled_dot_product_attention(
            query_states,  # (B, H_attn, S_q, D)
            key_states,  # (B, H_attn, S_kv_total, D)
            value_states,  # (B, H_attn, S_kv_total, D)
            attn_mask=additive_attention_mask,
            is_causal=True
        )  # (B, H_attn, S_q, D)

        attn_output = self._merge_heads(attn_output)  # (B, S_q, hidden_size)
        output = self.o_proj(attn_output)

        return output, present_key_value

# END OF FILE: src/models/attention/standard_attention.py

====================
文件: .\src\models\attention\__init__.py
====================

# Bedrock: This file makes the directory a Python package.

from .flash_attention import FlashAttention
from .standard_attention import StandardAttention

====================
文件: .\src\trainers\dpo_trainer.py
====================

# FILE: src/trainers/dpo_trainer.py
"""
Bedrock Protocol: Direct Preference Optimization (DPO) Trainer.

This script uses the Hugging Face TRL library to perform DPO, a form of
reinforcement learning from human feedback (RLHF) that is more stable and
computationally efficient than traditional PPO.
"""

# [HARDCODED MIRROR] Force Hugging Face Hub downloads to go through a domestic mirror
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
from pathlib import Path
import yaml
import torch
import gc
import shutil
import time
from datasets import load_dataset
from huggingface_hub import list_repo_files, hf_hub_download, HfApi
from huggingface_hub.constants import HF_HUB_CACHE
from huggingface_hub.hf_api import RepoFile
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)
from peft import LoraConfig, PeftModel, PeftConfig
from trl import DPOTrainer, DPOConfig

# Ensure project root is in path for imports if running as a script
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.utils.data_formatters import format_dpo_dataset_factory


def load_dataset_robustly(repo_id: str, split: str):
    """
    [ULTIMATE DATA ENGINE V12.0] Intelligently validates and downloads datasets.
    """
    print(f"\n[Data Engine] Initializing for dataset '{repo_id}'.")

    print("--> Step 1/4: Performing pre-flight check of local cache...")

    local_cache_dir = Path(HF_HUB_CACHE) / f"datasets--{repo_id.replace('/', '--')}"
    is_complete = False

    try:
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        relevant_files = {
            get_filename(f) for f in repo_files_info
            if get_filename(f).endswith(('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt',
                                         '.py')) or "dataset_info.json" in get_filename(
                f) or "README.md" in get_filename(f)
        }

        if not local_cache_dir.exists():
            print("--> STATUS: Local cache directory does not exist. Full download required.")
            files_to_download = list(relevant_files)
        else:
            snapshot_dir = local_cache_dir / 'snapshots'
            if not snapshot_dir.exists():
                print("--> STATUS: Local cache directory exists but is empty. Full download required.")
                files_to_download = list(relevant_files)
            else:
                local_files_in_snapshot = {p.name for p in snapshot_dir.rglob('*') if p.is_file()}
                is_missing = any(
                    Path(f).name not in local_files_in_snapshot for f in relevant_files if not Path(f).is_dir())

                if not is_missing:
                    print("--> STATUS: Cache check passed. All files appear to be present. Skipping download.")
                    is_complete = True
                    files_to_download = []
                else:
                    print(f"--> STATUS: Cache incomplete. Full re-download will be triggered for safety.")
                    files_to_download = list(relevant_files)

    except Exception as e:
        print(f"--> WARNING: Pre-flight check failed. Assuming full download is needed. Error: {e}")
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        files_to_download = [get_filename(info) for info in repo_files_info if get_filename(info).endswith(
            ('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt', '.py')) or "dataset_info.json" in get_filename(
            info) or "README.md" in get_filename(info)]

    if not is_complete:
        print(f"\n--> Step 2/4: Starting intelligent download of {len(files_to_download)} file(s)...")
        max_retries = 5
        initial_wait_time = 2

        for i, filename in enumerate(files_to_download):
            for attempt in range(max_retries):
                try:
                    print(
                        f"    - Downloading file {i + 1}/{len(files_to_download)}: '{filename}' (Attempt {attempt + 1}/{max_retries})...")
                    hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset", resume_download=True)
                    print(f"    - Successfully downloaded '{filename}'.")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = initial_wait_time * (2 ** attempt)
                        print(f"    - FAILED to download '{filename}'. Error: {e}. Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        print(f"    - FATAL: Failed to download '{filename}' after {max_retries} attempts.")
                        raise e
        print("--> Intelligent download complete.")

    try:
        print(f"\n--> Step 3/4: Loading dataset '{repo_id}' from local cache...")
        dataset = load_dataset(repo_id, split=split, download_mode="reuse_dataset_if_exists")
        print(f"\n[Data Engine] Successfully loaded the '{split}' split.")
        print("--> Step 4/4: Data Engine finished.")
        return dataset
    except Exception as e:
        print(
            f"--> FATAL: Failed to load dataset from cache even after download. Cache might be severely corrupted. Error: {e}")
        sys.exit(1)


def run_dpo(config_path: str) -> None:
    print("--- [Bedrock] Initiating Direct Preference Optimization (DPO) ---")
    print(f"--> NOTE: Hugging Face endpoint is set to: {os.environ.get('HF_ENDPOINT')}")

    print(f"\n[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    tokenizer = AutoTokenizer.from_pretrained(config['model_name_or_path'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("Tokenizer loaded successfully.")

    dataset = load_dataset_robustly(config['dataset_name'], split="train")

    if 'dataset_subset_size' in config and int(config['dataset_subset_size']) > 0:
        subset_size = int(config['dataset_subset_size'])
        dataset = dataset.select(range(subset_size))
        print(f"--> Using a subset of {subset_size} samples for this run.")

    formatting_function = format_dpo_dataset_factory(tokenizer)
    dataset = dataset.map(formatting_function)
    print(f"Dataset loaded and formatted with {len(dataset)} samples.")

    print(f"\n[Model Loading] Loading base SFT model for DPO: {config['model_name_or_path']}")
    print("--> Using 'force load into RAM' strategy for robust CPU execution.")

    peft_config_for_base = PeftConfig.from_pretrained(config['model_name_or_path'])
    base_model_name = peft_config_for_base.base_model_name_or_path

    print(f"--> Loading base model '{base_model_name}' fully into RAM for POLICY model...")
    base_model_policy = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float32)
    print("--> Loading PEFT adapter and merging for POLICY model...")
    model = PeftModel.from_pretrained(base_model_policy, config['model_name_or_path'])
    model = model.merge_and_unload()
    model.config.use_cache = False
    print("POLICY model prepared and fully loaded into RAM.")

    # [API UPDATE FIX] REMOVED the manual creation of the reference model.
    # TRL will automatically create an internal reference model from the 'model'
    # before applying the new PEFT adapter for DPO training.
    gc.collect()

    print("\n[Configuration] Configuring PEFT with LoRA for DPO training...")
    peft_config = LoraConfig(
        r=int(config['lora_r']),
        lora_alpha=int(config['lora_alpha']),
        lora_dropout=float(config['lora_dropout']),
        target_modules=config['lora_target_modules'],
        bias="none",
        task_type="CAUSAL_LM",
    )

    print("[Configuration] Setting up training arguments...")
    # [API UPDATE FIX] Use DPOConfig to wrap training arguments for TRL
    training_args = DPOConfig(
        output_dir=config['output_dir'],
        num_train_epochs=int(config['num_train_epochs']),
        per_device_train_batch_size=int(config['per_device_train_batch_size']),
        per_device_eval_batch_size=int(config['per_device_eval_batch_size']),
        gradient_accumulation_steps=int(config['gradient_accumulation_steps']),
        optim=config['optim'],
        learning_rate=float(config['learning_rate']),
        weight_decay=float(config.get('weight_decay', 0.0)),
        fp16=False,
        bf16=False,
        max_grad_norm=float(config['max_grad_norm']),
        logging_steps=int(config['logging_steps']),
        max_steps=int(config['max_steps']),
        lr_scheduler_type=config['lr_scheduler_type'],
        report_to=config['report_to'],
        run_name=config['run_name'],
        remove_unused_columns=False,
        # DPO-specific arguments
        beta=float(config['beta']),
        max_prompt_length=int(config['max_prompt_length']),
        max_length=int(config['max_length']),
    )

    print("\n[Trainer Init] Initializing DPOTrainer...")
    dpo_trainer = DPOTrainer(
        model=model,
        # [API UPDATE FIX] Pass ref_model=None as required by the new TRL API
        # when using PEFT.
        ref_model=None,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        peft_config=peft_config,
    )

    print("\n--- DPO Training Started ---")
    dpo_trainer.train()
    print("\n--- DPO Training Finished ---")

    final_model_path = Path(config['output_dir']) / "final_model"
    os.makedirs(final_model_path, exist_ok=True)

    print(f"\n[Saving] Saving final DPO-tuned adapter model to {final_model_path}...")
    dpo_trainer.save_model(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))
    print("Model and tokenizer saved successfully.")

    print("\n--- [Bedrock] DPO Process Complete ---")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/dpo_trainer.py <path_to_config.yaml>")
        sys.exit(1)
    config_file_path = sys.argv[1]
    run_dpo(config_file_path)

# END OF FILE: src/trainers/dpo_trainer.py

====================
文件: .\src\trainers\grpo_trainer.py
====================

# FILE: src/trainers/grpo_trainer.py
"""
Bedrock Protocol: Group Relative Policy Optimization (GRPO) Trainer.

This script provides a full, runnable implementation for the GRPO algorithm,
a value-free reinforcement learning technique for aligning language models.
"""

# [HARDCODED MIRROR] Force Hugging Face Hub downloads to go through a domestic mirror
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
from pathlib import Path
import yaml
import torch
import gc
import shutil
import time
from datasets import load_dataset
from huggingface_hub import list_repo_files, hf_hub_download, HfApi
from huggingface_hub.constants import HF_HUB_CACHE
from huggingface_hub.hf_api import RepoFile
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)
from peft import LoraConfig, PeftModel, PeftConfig, get_peft_model
# [FIX] Import GRPOConfig from the correct location if needed, and handle potential future changes
try:
    from trl import GRPOConfig, GRPOTrainer, GenerationConfig, AutoModelForCausalLMWithValueHead
except ImportError:
    print("Warning: Could not import GRPOConfig. This might happen with older TRL versions.")
    print("Attempting to fall back to a generic config object if possible, but an upgrade is recommended.")
    # Fallback for older versions, though this may not work perfectly
    from trl import PPOConfig as GRPOConfig
    from trl import GRPOTrainer, GenerationConfig, AutoModelForCausalLMWithValueHead


# Ensure project root is in path for imports
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))


def load_dataset_robustly(repo_id: str, split: str):
    """
    [ULTIMATE DATA ENGINE V12.0] Intelligently validates and downloads datasets.
    """
    print(f"\n[Data Engine] Initializing for dataset '{repo_id}'.")

    print("--> Step 1/4: Performing pre-flight check of local cache...")

    local_cache_dir = Path(HF_HUB_CACHE) / f"datasets--{repo_id.replace('/', '--')}"
    is_complete = False

    try:
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        relevant_files = {
            get_filename(f) for f in repo_files_info
            if get_filename(f).endswith(('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt',
                                         '.py')) or "dataset_info.json" in get_filename(
                f) or "README.md" in get_filename(f)
        }

        if not local_cache_dir.exists():
            print("--> STATUS: Local cache directory does not exist. Full download required.")
            files_to_download = list(relevant_files)
        else:
            snapshot_dir = local_cache_dir / 'snapshots'
            if not snapshot_dir.exists():
                print("--> STATUS: Local cache directory exists but is empty. Full download required.")
                files_to_download = list(relevant_files)
            else:
                local_files_in_snapshot = {p.name for p in snapshot_dir.rglob('*') if p.is_file()}
                is_missing = any(
                    Path(f).name not in local_files_in_snapshot for f in relevant_files if not Path(f).is_dir())

                if not is_missing:
                    print("--> STATUS: Cache check passed. All files appear to be present. Skipping download.")
                    is_complete = True
                    files_to_download = []
                else:
                    print(f"--> STATUS: Cache incomplete. Full re-download will be triggered for safety.")
                    files_to_download = list(relevant_files)


    except Exception as e:
        print(f"--> WARNING: Pre-flight check failed. Assuming full download is needed. Error: {e}")
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        files_to_download = [get_filename(info) for info in repo_files_info if get_filename(info).endswith(
            ('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt', '.py')) or "dataset_info.json" in get_filename(
            info) or "README.md" in get_filename(info)]

    if not is_complete:
        print(f"\n--> Step 2/4: Starting intelligent download of {len(files_to_download)} file(s)...")
        max_retries = 5
        initial_wait_time = 2

        for i, filename in enumerate(files_to_download):
            for attempt in range(max_retries):
                try:
                    print(
                        f"    - Downloading file {i + 1}/{len(files_to_download)}: '{filename}' (Attempt {attempt + 1}/{max_retries})...")
                    hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset", resume_download=True)
                    print(f"    - Successfully downloaded '{filename}'.")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = initial_wait_time * (2 ** attempt)
                        print(f"    - FAILED to download '{filename}'. Error: {e}. Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        print(f"    - FATAL: Failed to download '{filename}' after {max_retries} attempts.")
                        raise e
        print("--> Intelligent download complete.")

    try:
        print(f"\n--> Step 3/4: Loading dataset '{repo_id}' from local cache...")
        dataset = load_dataset(repo_id, split=split, download_mode="reuse_dataset_if_exists")
        print(f"\n[Data Engine] Successfully loaded the '{split}' split.")
        print("--> Step 4/4: Data Engine finished.")
        return dataset
    except Exception as e:
        print(
            f"--> FATAL: Failed to load dataset from cache even after download. Cache might be severely corrupted. Error: {e}")
        sys.exit(1)


def run_grpo(config_path: str) -> None:
    """
    Main function to execute the GRPO process.
    """
    print("--- [Bedrock] Initiating Group Relative Policy Optimization (GRPO) ---")
    print(f"--> NOTE: Hugging Face endpoint is set to: {os.environ.get('HF_ENDPOINT')}")

    print(f"\n[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    dataset = load_dataset_robustly(config['dataset_name'], split="train")

    if 'dataset_subset_size' in config and int(config['dataset_subset_size']) > 0:
        subset_size = int(config['dataset_subset_size'])
        dataset = dataset.select(range(subset_size))
        print(f"--> Using a subset of {subset_size} samples for this run.")

    def format_grpo_dataset(example: dict) -> dict:
        return {"query": example[config['dataset_text_field']]}

    dataset = dataset.map(format_grpo_dataset, remove_columns=dataset.column_names)

    tokenizer = AutoTokenizer.from_pretrained(config['model_name_or_path'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    def tokenize(example):
        return tokenizer(example["query"], truncation=True, max_length=128, padding="max_length")

    dataset = dataset.map(tokenize, batched=False)
    dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "query"])


    print(f"Dataset loaded and formatted with {len(dataset)} prompts.")

    print(f"\n[Model Loading] Loading SFT model as Actor for GRPO: {config['model_name_or_path']}")

    peft_config_for_base = PeftConfig.from_pretrained(config['model_name_or_path'])
    base_model_name = peft_config_for_base.base_model_name_or_path

    print("--> Using 'force load into RAM' strategy for robust CPU execution.")

    print("--> Step 1/2: Loading SFT-tuned model and merging into a clean base model...")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float32)
    sft_merged_model = PeftModel.from_pretrained(base_model, config['model_name_or_path']).merge_and_unload()
    print("--> SFT weights merged successfully.")

    peft_config = LoraConfig(
        r=int(config['lora_r']),
        lora_alpha=int(config['lora_alpha']),
        lora_dropout=float(config['lora_dropout']),
        target_modules=config['lora_target_modules'],
        bias="none",
        task_type="CAUSAL_LM",
    )

    print("--> Step 2/2: Wrapping SFT model with ValueHead AND applying new PEFT adapter for GRPO...")
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        sft_merged_model,
        peft_config=peft_config,
    )
    print("--> Model correctly wrapped with ValueHead and PEFT adapter. Ready for GRPOTrainer.")
    gc.collect()


    print("\n[Configuration] Setting up GRPO training arguments...")
    grpo_config = GRPOConfig(
        output_dir=config['output_dir'],
        num_train_epochs=1,
        max_steps=int(config['grpo_steps']),
        per_device_train_batch_size=int(config['batch_size']),
        gradient_accumulation_steps=int(config['gradient_accumulation_steps']),
        learning_rate=float(config['learning_rate']),
        log_with=config.get('report_to'),
        logging_steps=1,
        run_name=config['run_name'],
        remove_unused_columns=False,
        seed=int(config['seed']),
        beta=float(config['beta']),
        num_generations=int(config['num_generations']),
    )

    print("[Reward Model] Initializing conceptual Reward Model (rewards based on length)...")

    def get_dummy_reward(outputs_text):
        rewards = []
        for text in outputs_text:
            unique_words = len(set(text.split()))
            rewards.append(torch.tensor(float(unique_words)))
        return rewards

    print("\n[Trainer Init] Initializing GRPOTrainer...")
    grpo_trainer = GRPOTrainer(
        model=model,
        ref_model=None, # TRL will handle the reference model
        args=grpo_config,
        tokenizer=tokenizer,
        train_dataset=dataset,
    )

    print("\n--- GRPO Training Started ---")
    generation_config = GenerationConfig(
        min_length=-1,
        top_k=0.0,
        top_p=1.0,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=int(config['max_new_tokens']),
    )

    for i, batch in enumerate(grpo_trainer.dataloader):
        if i >= config['grpo_steps']:
            break

        print(f"--- GRPO Step {i + 1}/{config['grpo_steps']} ---")

        queries_text = batch["query"]
        query_tensors = batch["input_ids"]

        # [FIX] GRPOTrainer.generate expects the tokenized tensors, not raw text.
        responses_text = grpo_trainer.generate(query_tensors, generation_config=generation_config)

        rewards = get_dummy_reward(responses_text)

        # The 'step' method expects lists of strings, so we decode the responses here.
        decoded_responses_text = tokenizer.batch_decode(responses_text, skip_special_tokens=True)

        train_stats = grpo_trainer.step([q for q in queries_text], decoded_responses_text, rewards)

        log_output = f"GRPO Step {i + 1}/{config['grpo_steps']}:"
        for key, value in train_stats.items():
            if isinstance(value, (float, int, torch.Tensor)):
                log_output += f" | {key.replace('grpo/', '')}: {float(value):.4f}"
        print(log_output)

    print("\n--- GRPO Training Finished ---")

    final_model_path = Path(config['output_dir']) / "final_model"
    os.makedirs(final_model_path, exist_ok=True)

    print(f"\n[Saving] Saving final GRPO-tuned adapter model to {final_model_path}...")
    grpo_trainer.save_model(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))

    print("Model and tokenizer saved successfully.")

    print("\n--- [Bedrock] GRPO Process Complete ---")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/grpo_trainer.py <path_to_config.yaml>")
        sys.exit(1)
    config_file_path = sys.argv[1]
    run_grpo(config_file_path)

====================
文件: .\src\trainers\orpo_trainer.py
====================

# FILE: src/trainers/orpo_trainer.py
"""
Bedrock Protocol: Odds Ratio Preference Optimization (ORPO) Trainer.

This script implements the ORPO algorithm, a single-stage alignment technique
that combines SFT and DPO-like preference optimization. It does not require
a separate reference model or explicit reward model.
"""

# [HARDCODED MIRROR] Force Hugging Face Hub downloads to go through a domestic mirror
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
from pathlib import Path
import yaml
import torch
import gc
import shutil
import time
from datasets import load_dataset
from huggingface_hub import list_repo_files, hf_hub_download, HfApi
from huggingface_hub.constants import HF_HUB_CACHE
from huggingface_hub.hf_api import RepoFile
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)
from peft import LoraConfig, get_peft_model
from trl import ORPOTrainer, ORPOConfig

# Ensure project root is in path for imports
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.utils.data_formatters import format_dpo_dataset_factory


def load_dataset_robustly(repo_id: str, split: str):
    """
    [ULTIMATE DATA ENGINE V12.0] Intelligently validates and downloads datasets.
    """
    print(f"\n[Data Engine] Initializing for dataset '{repo_id}'.")

    print("--> Step 1/4: Performing pre-flight check of local cache...")

    local_cache_dir = Path(HF_HUB_CACHE) / f"datasets--{repo_id.replace('/', '--')}"
    is_complete = False

    try:
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        relevant_files = {
            get_filename(f) for f in repo_files_info
            if get_filename(f).endswith(('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt',
                                         '.py')) or "dataset_info.json" in get_filename(
                f) or "README.md" in get_filename(f)
        }

        if not local_cache_dir.exists():
            print("--> STATUS: Local cache directory does not exist. Full download required.")
            files_to_download = list(relevant_files)
        else:
            snapshot_dir = local_cache_dir / 'snapshots'
            if not snapshot_dir.exists():
                print("--> STATUS: Local cache directory exists but is empty. Full download required.")
                files_to_download = list(relevant_files)
            else:
                local_files_in_snapshot = {p.name for p in snapshot_dir.rglob('*') if p.is_file()}
                is_missing = any(
                    Path(f).name not in local_files_in_snapshot for f in relevant_files if not Path(f).is_dir())

                if not is_missing:
                    print("--> STATUS: Cache check passed. All files appear to be present. Skipping download.")
                    is_complete = True
                    files_to_download = []
                else:
                    print(f"--> STATUS: Cache incomplete. Full re-download will be triggered for safety.")
                    files_to_download = list(relevant_files)

    except Exception as e:
        print(f"--> WARNING: Pre-flight check failed. Assuming full download is needed. Error: {e}")
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        files_to_download = [get_filename(info) for info in repo_files_info if get_filename(info).endswith(
            ('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt', '.py')) or "dataset_info.json" in get_filename(
            info) or "README.md" in get_filename(info)]

    if not is_complete:
        print(f"\n--> Step 2/4: Starting intelligent download of {len(files_to_download)} file(s)...")
        max_retries = 5
        initial_wait_time = 2

        for i, filename in enumerate(files_to_download):
            for attempt in range(max_retries):
                try:
                    print(
                        f"    - Downloading file {i + 1}/{len(files_to_download)}: '{filename}' (Attempt {attempt + 1}/{max_retries})...")
                    hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset", resume_download=True)
                    print(f"    - Successfully downloaded '{filename}'.")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = initial_wait_time * (2 ** attempt)
                        print(f"    - FAILED to download '{filename}'. Error: {e}. Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        print(f"    - FATAL: Failed to download '{filename}' after {max_retries} attempts.")
                        raise e
        print("--> Intelligent download complete.")

    try:
        print(f"\n--> Step 3/4: Loading dataset '{repo_id}' from local cache...")
        dataset = load_dataset(repo_id, split=split, download_mode="reuse_dataset_if_exists")
        print(f"\n[Data Engine] Successfully loaded the '{split}' split.")
        print("--> Step 4/4: Data Engine finished.")
        return dataset
    except Exception as e:
        print(
            f"--> FATAL: Failed to load dataset from cache even after download. Cache might be severely corrupted. Error: {e}")
        sys.exit(1)


def run_orpo(config_path: str) -> None:
    """
    Main function to execute the ORPO training process.
    """
    print("--- [Bedrock] Initiating Odds Ratio Preference Optimization (ORPO) ---")
    print(f"--> NOTE: Hugging Face endpoint is set to: {os.environ.get('HF_ENDPOINT')}")

    # 1. Load Configuration
    print(f"\n[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # 2. Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config['model_name_or_path'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    print("Tokenizer loaded successfully.")

    # 3. Load and Format Dataset
    dataset = load_dataset_robustly(config['dataset_name'], split="train")

    if 'dataset_subset_size' in config and int(config['dataset_subset_size']) > 0:
        subset_size = int(config['dataset_subset_size'])
        dataset = dataset.select(range(subset_size))
        print(f"--> Using a subset of {subset_size} samples for this run.")

    formatting_function = format_dpo_dataset_factory(tokenizer)
    dataset = dataset.map(formatting_function)

    print(f"Dataset loaded and formatted with {len(dataset)} samples for ORPO training.")

    # 4. Load Base Model for ORPO Training
    print(f"\n[Model Loading] Loading base model for ORPO: {config['model_name_or_path']}")
    print("--> Using 'force load into RAM' strategy for robust CPU execution.")

    model = AutoModelForCausalLM.from_pretrained(
        config['model_name_or_path'],
        torch_dtype=torch.float32,
    )
    model.config.use_cache = False

    print("Base model loaded successfully.")
    gc.collect()

    # 5. Configure PEFT (LoRA) for ORPO training
    print("\n[Configuration] Configuring PEFT with LoRA for ORPO training...")
    peft_config = LoraConfig(
        r=int(config['lora_r']),
        lora_alpha=int(config['lora_alpha']),
        lora_dropout=float(config['lora_dropout']),
        target_modules=config['lora_target_modules'],
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, peft_config)
    print("PEFT adapter applied for ORPO training. Trainable parameters:")
    model.print_trainable_parameters()

    # 6. Configure Training Arguments using ORPOConfig
    print("[Configuration] Setting up training arguments for ORPO...")
    log_with = config.get('report_to')
    if log_with == "none" or log_with is None:
        log_with = None

    training_args = ORPOConfig(
        output_dir=config['output_dir'],
        num_train_epochs=int(config['num_train_epochs']),
        per_device_train_batch_size=int(config['per_device_train_batch_size']),
        per_device_eval_batch_size=int(config['per_device_eval_batch_size']),
        gradient_accumulation_steps=int(config['gradient_accumulation_steps']),
        optim=config['optim'],
        learning_rate=float(config['learning_rate']),
        weight_decay=float(config.get('weight_decay', 0.0)),
        max_grad_norm=float(config['max_grad_norm']),
        logging_steps=int(config['logging_steps']),
        max_steps=int(config['max_steps']),
        lr_scheduler_type=config['lr_scheduler_type'],
        report_to=log_with,
        run_name=config['run_name'],
        remove_unused_columns=False,
        # ORPO-specific arguments
        beta=float(config['beta']),
        max_length=int(config['max_length']),
        max_prompt_length=int(config['max_prompt_length']),
    )

    # 7. Initialize ORPOTrainer
    print("\n[Trainer Init] Initializing ORPOTrainer...")
    orpo_trainer = ORPOTrainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=dataset,
    )

    print("\n--- ORPO Training Started ---")
    orpo_trainer.train()
    print("\n--- ORPO Training Finished ---")

    # 8. Save Final Model
    final_model_path = Path(config['output_dir']) / "final_model"
    os.makedirs(final_model_path, exist_ok=True)

    print(f"\n[Saving] Saving final ORPO-tuned adapter model to {final_model_path}...")
    orpo_trainer.model.save_pretrained(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))
    print("Model and tokenizer saved successfully.")

    print("\n--- [Bedrock] ORPO Process Complete ---")
    gc.collect()


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/orpo_trainer.py <path_to_config.yaml>")
        sys.exit(1)
    config_file_path = sys.argv[1]
    run_orpo(config_file_path)

# END OF FILE: src/trainers/orpo_trainer.py

====================
文件: .\src\trainers\ppo_trainer.py
====================

# FILE: src/trainers/ppo_trainer.py
"""
Bedrock Protocol: Proximal Policy Optimization (PPO) Trainer (Conceptual).

This script provides a conceptual outline for using the Hugging Face TRL library
to perform PPO, a common reinforcement learning from human feedback (RLHF) algorithm.
"""

# [HARDCODED MIRROR] Force Hugging Face Hub downloads to go through a domestic mirror
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
from pathlib import Path
import yaml
import torch
import gc
import shutil
import time
from datasets import load_dataset
from huggingface_hub import list_repo_files, hf_hub_download, HfApi
from huggingface_hub.constants import HF_HUB_CACHE
from huggingface_hub.hf_api import RepoFile
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig,
)
from peft import LoraConfig, PeftModel, PeftConfig, get_peft_model
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead

# Ensure project root is in path for imports
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))


def load_dataset_robustly(repo_id: str, split: str):
    """
    [ULTIMATE DATA ENGINE V12.0] Intelligently validates and downloads datasets.
    """
    print(f"\n[Data Engine] Initializing for dataset '{repo_id}'.")

    print("--> Step 1/4: Performing pre-flight check of local cache...")

    local_cache_dir = Path(HF_HUB_CACHE) / f"datasets--{repo_id.replace('/', '--')}"
    is_complete = False

    try:
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        relevant_files = {
            get_filename(f) for f in repo_files_info
            if get_filename(f).endswith(('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt',
                                         '.py')) or "dataset_info.json" in get_filename(
                f) or "README.md" in get_filename(f)
        }

        if not local_cache_dir.exists():
            print("--> STATUS: Local cache directory does not exist. Full download required.")
            files_to_download = list(relevant_files)
        else:
            snapshot_dir = local_cache_dir / 'snapshots'
            if not snapshot_dir.exists():
                print("--> STATUS: Local cache directory exists but is empty. Full download required.")
                files_to_download = list(relevant_files)
            else:
                local_files_in_snapshot = {p.name for p in snapshot_dir.rglob('*') if p.is_file()}
                is_missing = any(
                    Path(f).name not in local_files_in_snapshot for f in relevant_files if not Path(f).is_dir())

                if not is_missing:
                    print("--> STATUS: Cache check passed. All files appear to be present. Skipping download.")
                    is_complete = True
                    files_to_download = []
                else:
                    print(f"--> STATUS: Cache incomplete. Full re-download will be triggered for safety.")
                    files_to_download = list(relevant_files)


    except Exception as e:
        print(f"--> WARNING: Pre-flight check failed. Assuming full download is needed. Error: {e}")
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        files_to_download = [get_filename(info) for info in repo_files_info if get_filename(info).endswith(
            ('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt', '.py')) or "dataset_info.json" in get_filename(
            info) or "README.md" in get_filename(info)]

    if not is_complete:
        print(f"\n--> Step 2/4: Starting intelligent download of {len(files_to_download)} file(s)...")
        max_retries = 5
        initial_wait_time = 2

        for i, filename in enumerate(files_to_download):
            for attempt in range(max_retries):
                try:
                    print(
                        f"    - Downloading file {i + 1}/{len(files_to_download)}: '{filename}' (Attempt {attempt + 1}/{max_retries})...")
                    hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset", resume_download=True)
                    print(f"    - Successfully downloaded '{filename}'.")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = initial_wait_time * (2 ** attempt)
                        print(f"    - FAILED to download '{filename}'. Error: {e}. Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        print(f"    - FATAL: Failed to download '{filename}' after {max_retries} attempts.")
                        raise e
        print("--> Intelligent download complete.")

    try:
        print(f"\n--> Step 3/4: Loading dataset '{repo_id}' from local cache...")
        dataset = load_dataset(repo_id, split=split, download_mode="reuse_dataset_if_exists")
        print(f"\n[Data Engine] Successfully loaded the '{split}' split.")
        print("--> Step 4/4: Data Engine finished.")
        return dataset
    except Exception as e:
        print(
            f"--> FATAL: Failed to load dataset from cache even after download. Cache might be severely corrupted. Error: {e}")
        sys.exit(1)


def run_ppo(config_path: str) -> None:
    """
    Conceptual main function to execute the PPO process.
    """
    print("--- [Bedrock] Initiating Proximal Policy Optimization (PPO) (Conceptual) ---")
    print(f"--> NOTE: Hugging Face endpoint is set to: {os.environ.get('HF_ENDPOINT')}")

    print(f"\n[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    dataset = load_dataset_robustly(config['dataset_name'], split="train")

    if 'dataset_subset_size' in config and int(config['dataset_subset_size']) > 0:
        subset_size = int(config['dataset_subset_size'])
        dataset = dataset.select(range(subset_size))
        print(f"--> Using a subset of {subset_size} samples for this run.")

    def format_ppo_dataset(example: dict) -> dict:
        return {"query": example[config['dataset_text_field']]}

    dataset = dataset.map(format_ppo_dataset, remove_columns=dataset.column_names)

    tokenizer = AutoTokenizer.from_pretrained(config['model_name_or_path'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    def tokenize(example):
        return tokenizer(
            example["query"],
            truncation=True,
            max_length=128,
            padding="max_length"
        )

    dataset = dataset.map(tokenize, batched=False)
    dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "query"])
    print(f"Dataset loaded and formatted with {len(dataset)} prompts.")

    print(f"\n[Model Loading] Loading SFT model as Actor for PPO: {config['model_name_or_path']}")

    peft_config_for_base = PeftConfig.from_pretrained(config['model_name_or_path'])
    base_model_name = peft_config_for_base.base_model_name_or_path

    print("--> Step 1/2: Loading SFT-tuned model and merging into a clean base model...")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float32)
    sft_merged_model = PeftModel.from_pretrained(base_model, config['model_name_or_path']).merge_and_unload()
    print("--> SFT weights merged successfully.")

    ppo_peft_config = LoraConfig(
        r=int(config['lora_r']),
        lora_alpha=int(config['lora_alpha']),
        lora_dropout=float(config['lora_dropout']),
        target_modules=config['lora_target_modules'],
        bias="none",
        task_type="CAUSAL_LM",
    )

    print("--> Step 2/2: Wrapping SFT model with ValueHead AND applying new PEFT adapter in one step...")
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        sft_merged_model,
        peft_config=ppo_peft_config,
    )
    print("--> Model correctly wrapped with ValueHead and PEFT adapter. Ready for PPOTrainer.")
    gc.collect()

    print("\n[Configuration] Setting up PPO training arguments...")

    report_to = config.get('report_to', 'none')
    if report_to == 'none':
        report_to = None

    ppo_config = PPOConfig(
        exp_name=config['run_name'],
        report_to=report_to,
        learning_rate=float(config['learning_rate']),
        batch_size=int(config['batch_size']),
        mini_batch_size=int(config['mini_batch_size']),
        gradient_accumulation_steps=int(config['gradient_accumulation_steps']),
        # [FIX] Removed unexpected 'ppo_epochs' argument. It is controlled by the trainer loop.
        init_kl_coef=float(config['init_kl_coef']),
        target_kl=float(config.get('target_kl', 0.1)),
        adap_kl_ctrl=bool(config['adap_kl_ctrl']),
        seed=int(config['seed']),
        remove_unused_columns=False,
    )

    print("[Reward Model] Initializing conceptual Reward Model (rewards based on length)...")
    def get_dummy_reward(outputs_text):
        rewards = []
        for text in outputs_text:
            unique_words = len(set(text.split()))
            rewards.append(torch.tensor(float(unique_words)))
        return rewards

    print("\n[Trainer Init] Initializing PPOTrainer...")
    ppo_trainer = PPOTrainer(
        config=ppo_config,
        model=model,
        ref_model=None,
        tokenizer=tokenizer,
        dataset=dataset,
    )

    print("\n--- PPO Training Started (Conceptual) ---")
    generation_kwargs = GenerationConfig(
        min_length=-1,
        top_k=0.0,
        top_p=1.0,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens=int(config['max_new_tokens']),
    )

    stats_keys_to_log = [
        "ppo/loss/total", "ppo/loss/policy", "ppo/loss/value",
        "ppo/returns/mean", "ppo/returns/var", "objective/kl", "ppo/policy/approxkl",
    ]

    total_ppo_steps = int(config['ppo_steps'])
    # The number of inner epochs is now controlled by the PPOConfig, not a separate parameter.
    # TRL's PPOTrainer handles this internally during the `step` call.
    for step, batch in enumerate(ppo_trainer.dataloader):
        if step >= total_ppo_steps:
            break

        query_tensors = batch['input_ids']
        queries = [q for q in query_tensors]

        response_tensors = ppo_trainer.generate(queries, **generation_kwargs.to_dict())

        batch['response'] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

        rewards = get_dummy_reward(batch["response"])

        stats = ppo_trainer.step(queries, response_tensors, rewards)
        ppo_trainer.log_stats(stats, batch, rewards)

        log_output = f"Conceptual PPO Step {step + 1}/{total_ppo_steps}:"
        for key in stats_keys_to_log:
            value = stats.get(key)
            if value is not None:
                log_output += f" | {key.split('/')[-1]}: {float(value):.4f}"
        print(log_output)

    print("\n--- PPO Training Finished (Conceptual) ---")

    final_model_path = Path(config['output_dir']) / "final_ppo_model"
    os.makedirs(final_model_path, exist_ok=True)

    print(f"\n[Saving] Saving final PPO-tuned adapter model to {final_model_path}...")
    ppo_trainer.save_pretrained(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))

    print("Model and tokenizer saved successfully.")

    print("\n--- [Bedrock] PPO Process Complete ---")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/ppo_trainer.py <path_to_config.yaml>")
        sys.exit(1)
    config_file_path = sys.argv[1]
    run_ppo(config_file_path)

====================
文件: .\src\trainers\pretrain_trainer.py
====================

# FILE: src/trainers/pretrain_trainer.py
"""
Bedrock Protocol: Main Pre-training Trainer.

This script orchestrates the end-to-end pre-training of a language model,
from loading data and model configuration to running the distributed training loop.
It integrates Hugging Face Accelerate and supports DeepSpeed for advanced parallelism.
Now supports resuming from a checkpoint.
"""

import sys
import os
from pathlib import Path
import yaml
import torch
import torch.nn.functional as F
from datasets import load_from_disk
from transformers import AutoTokenizer, get_scheduler, PreTrainedModel, AutoModelForCausalLM
from accelerate import Accelerator
from accelerate.utils import set_seed
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import wandb
import gc

# Ensure project root is in path for imports
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent.parent
sys.path.append(str(project_root))

from src.models.language_model import BaseLLM, BaseLLMConfig


def get_total_params(model: torch.nn.Module) -> int:
    """Calculates the total number of trainable parameters in a model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def run_pretrain(config_path: str) -> None:
    """
    Main function to execute the pre-training process.
    Supports resuming from a checkpoint.

    Args:
        config_path: Path to the YAML configuration file.
    """
    print("--- [Bedrock] Initiating From-Scratch Pre-training ---")

    # 1. Load Configuration
    print(f"[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # 2. Initialize Accelerator
    accelerator = Accelerator(
        mixed_precision=config.get('mixed_precision', 'no'),
        log_with=config.get('report_to'),
        project_dir=config['output_dir']
    )
    print(f"Accelerator initialized with mixed_precision='{accelerator.mixed_precision}'")

    set_seed(int(config['seed']))

    # 3. Experiment Tracking (WandB)
    if accelerator.is_main_process and config.get('report_to') == "wandb":
        wandb.init(
            project="Awesome-LLM-ZeroToScratch",
            name=config['run_name'],
            config=config
        )
        print("WandB initialized.")

    # 4. Load Tokenizer
    tokenizer_path_str = config['tokenizer_path']
    tokenizer_load_path = project_root / tokenizer_path_str
    if not tokenizer_load_path.exists():
        # Fallback if tokenizer is saved within the model checkpoint directory directly
        tokenizer_load_path = Path(config['model_name_or_path'])
    print(f"Loading tokenizer from: {tokenizer_load_path}")
    tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_load_path), trust_remote_code=True, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("Tokenizer loaded successfully.")

    # 5. Load Model Architecture or Resume from Checkpoint
    model_name_or_path = config.get('model_name_or_path')
    resume_from_checkpoint = config.get('resume_from_checkpoint')

    if resume_from_checkpoint:
        print(f"Resuming pre-training from checkpoint: {resume_from_checkpoint}")
        # When resuming, we load the full model from the checkpoint
        # Assuming the checkpoint contains the full model (not just PEFT adapter for pre-training)
        # Use AutoModelForCausalLM.from_pretrained for full model loading
        model = AutoModelForCausalLM.from_pretrained(str(resume_from_checkpoint), trust_remote_code=True)
        # Ensure config is updated in case tokenizer vocab size was adjusted
        model.config.pad_token_id = tokenizer.pad_token_id
        model.config.vocab_size = tokenizer.vocab_size
        print(f"Model loaded from checkpoint. Total parameters: {get_total_params(model) / 1e9:.2f} Billion")
    else:
        model_config_path = project_root / config['model_config_path']
        print(f"Loading new model architecture configuration from: {model_config_path}")
        with open(model_config_path, 'r') as f:
            model_config_dict = yaml.safe_load(f)

        model_config_dict['model_type_llm'] = model_config_dict.pop('model_type', 'DenseLLM')
        model_config_dict['vocab_size'] = tokenizer.vocab_size
        model_config_dict['pad_token_id'] = tokenizer.pad_token_id

        print("Initializing new model architecture...")
        model_config_obj = BaseLLMConfig(**model_config_dict)
        model = BaseLLM(model_config_obj)
        print(f"New model initialized with {get_total_params(model) / 1e9:.2f} Billion parameters.")

    # Ensure model is in evaluation mode initially, then set to train
    model.train()

    # 6. Load Dataset and Tokenize
    dataset_dir = project_root / config['dataset_dir']
    print(f"Loading dataset from: {dataset_dir}")
    raw_dataset = load_from_disk(str(dataset_dir))

    def tokenize_function(examples):
        return tokenizer(examples[config['dataset_text_field']], max_length=int(config['max_seq_length']),
                         truncation=True, padding="max_length")

    num_processes_for_map = min(os.cpu_count() or 1, 8)
    tokenized_dataset = raw_dataset.map(tokenize_function, batched=True,
                                        remove_columns=raw_dataset['train'].column_names,
                                        num_proc=num_processes_for_map, desc="Tokenizing dataset")
    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])

    # [FIX] DataLoader creation for Accelerate, ensure no 'auto' device_map
    train_dataloader = DataLoader(tokenized_dataset["train"], shuffle=True,
                                  batch_size=int(config['per_device_train_batch_size']))
    eval_dataloader = DataLoader(tokenized_dataset["validation"], batch_size=int(config['per_device_train_batch_size']))
    print("Dataset tokenized and DataLoaders prepared.")

    # 7. Setup Optimizer and Scheduler
    print("Setting up optimizer and learning rate scheduler...")
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=float(config['learning_rate']),
        weight_decay=float(config['weight_decay']),
        betas=(float(config['adam_beta1']), float(config['adam_beta2'])),
        eps=float(config['adam_epsilon'])
    )
    num_training_steps = int(config['max_steps']) if int(config['max_steps']) > 0 else (len(train_dataloader) // int(
        config['gradient_accumulation_steps'])) * int(config['num_train_epochs'])
    lr_scheduler = get_scheduler(
        name=config['lr_scheduler_type'],
        optimizer=optimizer,
        num_warmup_steps=int(num_training_steps * float(config['warmup_ratio'])),
        num_training_steps=num_training_steps
    )
    print(f"Total training steps: {num_training_steps}")

    # 8. Prepare for Distributed Training with Accelerate
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
    )
    print("Model, Optimizer, DataLoaders, and Scheduler prepared for training.")

    # 9. Training Loop
    print("\n--- Training Started ---")
    progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)
    completed_steps = 0

    model.train()
    while completed_steps < num_training_steps:
        for step, batch in enumerate(train_dataloader):
            if completed_steps >= num_training_steps:
                break

            with accelerator.accumulate(model):
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    labels=batch['input_ids']
                )
                loss = outputs.loss

                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), float(
                        config['max_grad_norm']))  # Apply gradient clipping after accumulation
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            if accelerator.sync_gradients:
                progress_bar.update(1)
                completed_steps += 1

                if accelerator.is_main_process:
                    log_dict = {"train_loss": loss.item(), "learning_rate": lr_scheduler.get_last_lr()[0]}
                    accelerator.log(log_dict, step=completed_steps)
                    progress_bar.set_description(f"Loss: {loss.item():.4f}")

                save_steps = int(config.get('save_steps', 0))
                if completed_steps > 0 and save_steps > 0 and completed_steps % save_steps == 0:
                    output_path = Path(config['output_dir']) / f"step_{completed_steps}"
                    accelerator.wait_for_everyone()
                    unwrapped_model = accelerator.unwrap_model(model)

                    if accelerator.is_main_process:
                        os.makedirs(output_path, exist_ok=True)  # Ensure directory exists
                        unwrapped_model.save_pretrained(str(output_path))
                        tokenizer.save_pretrained(str(output_path))
                        print(f"Checkpoint saved at step {completed_steps} to {output_path}")

    progress_bar.close()
    print("\n--- Training Finished ---")

    if accelerator.is_main_process:
        final_model_path = Path(config['output_dir']) / "final_model"
        accelerator.wait_for_everyone()
        unwrapped_model = accelerator.unwrap_model(model)

        os.makedirs(final_model_path, exist_ok=True)  # Ensure directory exists
        unwrapped_model.save_pretrained(str(final_model_path))
        tokenizer.save_pretrained(str(final_model_path))
        print(f"Final model and tokenizer saved to {final_model_path}")
        if config.get('report_to') == "wandb":
            wandb.finish()

    print("\n--- [Bedrock] Pre-training Process Complete ---")
    gc.collect()


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/pretrain_trainer.py <path_to_config.yaml>")
        sys.exit(1)
    config_file_path = sys.argv[1]
    run_pretrain(config_file_path)

# END OF FILE: src/trainers/pretrain_trainer.py

====================
文件: .\src\trainers\reward_model_trainer.py
====================

# FILE: src/trainers/reward_model_trainer.py
"""
Bedrock Protocol: Reward Model Trainer.

This script trains a Reward Model (RM) from human preference data.
A Reward Model is a critical component for Reinforcement Learning from Human Feedback (RLHF)
algorithms like PPO, enabling the model to learn what human preferences look like.
"""

# [HARDCODED MIRROR] Force Hugging Face Hub downloads to go through a domestic mirror
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
from pathlib import Path
import yaml
import torch
import gc
import shutil
import time
from datasets import load_dataset
from huggingface_hub import list_repo_files, hf_hub_download, HfApi
from huggingface_hub.constants import HF_HUB_CACHE
from huggingface_hub.hf_api import RepoFile
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import RewardTrainer, RewardConfig

# Ensure project root is in path for imports
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.utils.data_formatters import format_dpo_dataset_factory


def load_dataset_robustly(repo_id: str, split: str):
    """
    [ULTIMATE DATA ENGINE V12.0] Intelligently validates and downloads datasets.
    """
    print(f"\n[Data Engine] Initializing for dataset '{repo_id}'.")

    print("--> Step 1/4: Performing pre-flight check of local cache...")

    local_cache_dir = Path(HF_HUB_CACHE) / f"datasets--{repo_id.replace('/', '--')}"
    is_complete = False

    try:
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        relevant_files = {
            get_filename(f) for f in repo_files_info
            if get_filename(f).endswith(('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt',
                                         '.py')) or "dataset_info.json" in get_filename(
                f) or "README.md" in get_filename(f)
        }

        if not local_cache_dir.exists():
            print("--> STATUS: Local cache directory does not exist. Full download required.")
            files_to_download = list(relevant_files)
        else:
            snapshot_dir = local_cache_dir / 'snapshots'
            if not snapshot_dir.exists():
                print("--> STATUS: Local cache directory exists but is empty. Full download required.")
                files_to_download = list(relevant_files)
            else:
                local_files_in_snapshot = {p.name for p in snapshot_dir.rglob('*') if p.is_file()}
                is_missing = any(
                    Path(f).name not in local_files_in_snapshot for f in relevant_files if not Path(f).is_dir())

                if not is_missing:
                    print("--> STATUS: Cache check passed. All files appear to be present. Skipping download.")
                    is_complete = True
                    files_to_download = []
                else:
                    print(f"--> STATUS: Cache incomplete. Full re-download will be triggered for safety.")
                    files_to_download = list(relevant_files)

    except Exception as e:
        print(f"--> WARNING: Pre-flight check failed. Assuming full download is needed. Error: {e}")
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        files_to_download = [get_filename(info) for info in repo_files_info if get_filename(info).endswith(
            ('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt', '.py')) or "dataset_info.json" in get_filename(
            info) or "README.md" in get_filename(info)]

    if not is_complete:
        print(f"\n--> Step 2/4: Starting intelligent download of {len(files_to_download)} file(s)...")
        max_retries = 5
        initial_wait_time = 2

        for i, filename in enumerate(files_to_download):
            for attempt in range(max_retries):
                try:
                    print(
                        f"    - Downloading file {i + 1}/{len(files_to_download)}: '{filename}' (Attempt {attempt + 1}/{max_retries})...")
                    hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset", resume_download=True)
                    print(f"    - Successfully downloaded '{filename}'.")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = initial_wait_time * (2 ** attempt)
                        print(f"    - FAILED to download '{filename}'. Error: {e}. Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        print(f"    - FATAL: Failed to download '{filename}' after {max_retries} attempts.")
                        raise e
        print("--> Intelligent download complete.")

    try:
        print(f"\n--> Step 3/4: Loading dataset '{repo_id}' from local cache...")
        dataset = load_dataset(repo_id, split=split, download_mode="reuse_dataset_if_exists")
        print(f"\n[Data Engine] Successfully loaded the '{split}' split.")
        print("--> Step 4/4: Data Engine finished.")
        return dataset
    except Exception as e:
        print(
            f"--> FATAL: Failed to load dataset from cache even after download. Cache might be severely corrupted. Error: {e}")
        sys.exit(1)


def run_reward_model_trainer(config_path: str) -> None:
    """
    Main function to execute the Reward Model training process.
    """
    print("--- [Bedrock] Initiating Reward Model Training ---")
    print(f"--> NOTE: Hugging Face endpoint is set to: {os.environ.get('HF_ENDPOINT')}")

    print(f"\n[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    tokenizer = AutoTokenizer.from_pretrained(config['model_name_or_path'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    print("Tokenizer loaded successfully.")

    dataset = load_dataset_robustly(config['dataset_name'], split="train")

    if 'dataset_subset_size' in config and int(config['dataset_subset_size']) > 0:
        subset_size = int(config['dataset_subset_size'])
        dataset = dataset.select(range(subset_size))
        print(f"--> Using a subset of {subset_size} samples for this run.")

    formatting_function = format_dpo_dataset_factory(tokenizer)
    dataset = dataset.map(formatting_function)

    def preprocess_function(examples):
        """
        Tokenizes the chosen and rejected responses.
        """
        new_examples = {
            "input_ids_chosen": [],
            "attention_mask_chosen": [],
            "input_ids_rejected": [],
            "attention_mask_rejected": [],
        }
        for chosen, rejected in zip(examples["chosen"], examples["rejected"]):
            tokenized_chosen = tokenizer(chosen, truncation=True, max_length=config['max_length'])
            tokenized_rejected = tokenizer(rejected, truncation=True, max_length=config['max_length'])

            new_examples["input_ids_chosen"].append(tokenized_chosen["input_ids"])
            new_examples["attention_mask_chosen"].append(tokenized_chosen["attention_mask"])
            new_examples["input_ids_rejected"].append(tokenized_rejected["input_ids"])
            new_examples["attention_mask_rejected"].append(tokenized_rejected["attention_mask"])

        return new_examples

    tokenized_dataset = dataset.map(
        preprocess_function,
        batched=True,
        num_proc=min(os.cpu_count() or 1, 4),
        remove_columns=dataset.column_names
    )
    print(f"Dataset loaded and formatted with {len(tokenized_dataset)} samples for RM training.")

    print(f"\n[Model Loading] Loading base model for Reward Model: {config['model_name_or_path']}")
    print("--> Using 'force load into RAM' strategy for robust CPU execution.")

    model = AutoModelForSequenceClassification.from_pretrained(
        config['model_name_or_path'],
        num_labels=1,
        torch_dtype=torch.float32,
    )

    print("Reward Model base loaded successfully.")
    gc.collect()

    print("\n[Configuration] Configuring PEFT with LoRA for Reward Model training...")
    peft_config = LoraConfig(
        r=int(config['lora_r']),
        lora_alpha=int(config['lora_alpha']),
        lora_dropout=float(config['lora_dropout']),
        target_modules=config['lora_target_modules'],
        bias="none",
        task_type="SEQ_CLS",
    )
    model = get_peft_model(model, peft_config)
    print("PEFT adapter applied for Reward Model training. Trainable parameters:")
    model.print_trainable_parameters()

    print("[Configuration] Setting up training arguments for Reward Model...")
    # [API UPDATE FIX] Use RewardConfig to wrap arguments
    training_args = RewardConfig(
        output_dir=config['output_dir'],
        num_train_epochs=int(config['num_train_epochs']),
        per_device_train_batch_size=int(config['per_device_train_batch_size']),
        per_device_eval_batch_size=int(config['per_device_eval_batch_size']),
        gradient_accumulation_steps=int(config['gradient_accumulation_steps']),
        optim=config['optim'],
        learning_rate=float(config['learning_rate']),
        weight_decay=float(config.get('weight_decay', 0.0)),
        max_grad_norm=float(config['max_grad_norm']),
        logging_steps=int(config['logging_steps']),
        max_steps=int(config['max_steps']),
        lr_scheduler_type=config['lr_scheduler_type'],
        report_to=config['report_to'],
        run_name=config['run_name'],
        remove_unused_columns=False,
        # Reward-specific arguments
        max_length=int(config['max_length']),
    )

    print("\n[Trainer Init] Initializing RewardTrainer...")
    reward_trainer = RewardTrainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=tokenized_dataset,
        eval_dataset=tokenized_dataset,
    )

    print("\n--- Reward Model Training Started ---")
    reward_trainer.train()
    print("\n--- Reward Model Training Finished ---")

    final_model_path = Path(config['output_dir']) / "final_model"
    os.makedirs(final_model_path, exist_ok=True)

    print(f"\n[Saving] Saving final Reward Model to {final_model_path}...")
    reward_trainer.save_model(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))
    print("Reward Model and tokenizer saved successfully.")

    print("\n--- [Bedrock] Reward Model Training Process Complete ---")
    gc.collect()


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/reward_model_trainer.py <path_to_config.yaml>")
        sys.exit(1)
    config_file_path = sys.argv[1]
    run_reward_model_trainer(config_file_path)

# END OF FILE: src/trainers/reward_model_trainer.py

====================
文件: .\src\trainers\sft_trainer.py
====================

# FILE: src/trainers/sft_trainer.py
"""
Bedrock Protocol: Supervised Fine-Tuning (SFT) Trainer.

This script uses the Hugging Face TRL library to perform SFT on a language model
using Parameter-Efficient Fine-Tuning (PEFT) with LoRA. It is designed to be
driven by a YAML configuration file.
"""

# [HARDCODED MIRROR] Force Hugging Face Hub downloads to go through a domestic mirror
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
from pathlib import Path
import yaml
import torch
import gc
import shutil
import time
from datasets import load_dataset
from huggingface_hub import list_repo_files, hf_hub_download, HfApi
from huggingface_hub.constants import HF_HUB_CACHE
from huggingface_hub.hf_api import RepoFile
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig


def load_dataset_robustly(repo_id: str, split: str):
    """
    [ULTIMATE DATA ENGINE V12.0] Intelligently validates and downloads datasets.
    """
    print(f"\n[Data Engine] Initializing for dataset '{repo_id}'.")

    print("--> Step 1/4: Performing pre-flight check of local cache...")

    local_cache_dir = Path(HF_HUB_CACHE) / f"datasets--{repo_id.replace('/', '--')}"
    is_complete = False

    try:
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        relevant_files = {
            get_filename(f) for f in repo_files_info
            if get_filename(f).endswith(('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt',
                                         '.py')) or "dataset_info.json" in get_filename(
                f) or "README.md" in get_filename(f)
        }

        if not local_cache_dir.exists():
            print("--> STATUS: Local cache directory does not exist. Full download required.")
            files_to_download = list(relevant_files)
        else:
            snapshot_dir = local_cache_dir / 'snapshots'
            if not snapshot_dir.exists():
                print("--> STATUS: Local cache directory exists but is empty. Full download required.")
                files_to_download = list(relevant_files)
            else:
                local_files_in_snapshot = {p.name for p in snapshot_dir.rglob('*') if p.is_file()}
                is_missing = any(
                    Path(f).name not in local_files_in_snapshot for f in relevant_files if not Path(f).is_dir())

                if not is_missing:
                    print("--> STATUS: Cache check passed. All files appear to be present. Skipping download.")
                    is_complete = True
                    files_to_download = []
                else:
                    print(f"--> STATUS: Cache incomplete. Full re-download will be triggered for safety.")
                    files_to_download = list(relevant_files)

    except Exception as e:
        print(f"--> WARNING: Pre-flight check failed. Assuming full download is needed. Error: {e}")
        api = HfApi()
        repo_files_info = api.list_repo_files(repo_id=repo_id, repo_type="dataset")

        def get_filename(file_info):
            return file_info.rfilename if isinstance(file_info, RepoFile) else file_info

        files_to_download = [get_filename(info) for info in repo_files_info if get_filename(info).endswith(
            ('.json', '.jsonl', '.parquet', '.arrow', '.csv', '.txt', '.py')) or "dataset_info.json" in get_filename(
            info) or "README.md" in get_filename(info)]

    if not is_complete:
        print(f"\n--> Step 2/4: Starting intelligent download of {len(files_to_download)} file(s)...")
        max_retries = 5
        initial_wait_time = 2

        for i, filename in enumerate(files_to_download):
            for attempt in range(max_retries):
                try:
                    print(
                        f"    - Downloading file {i + 1}/{len(files_to_download)}: '{filename}' (Attempt {attempt + 1}/{max_retries})...")
                    hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset", resume_download=True)
                    print(f"    - Successfully downloaded '{filename}'.")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = initial_wait_time * (2 ** attempt)
                        print(f"    - FAILED to download '{filename}'. Error: {e}. Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        print(f"    - FATAL: Failed to download '{filename}' after {max_retries} attempts.")
                        raise e
        print("--> Intelligent download complete.")

    try:
        print(f"\n--> Step 3/4: Loading dataset '{repo_id}' from local cache...")
        dataset = load_dataset(repo_id, split=split, download_mode="reuse_dataset_if_exists")
        print(f"\n[Data Engine] Successfully loaded the '{split}' split.")
        print("--> Step 4/4: Data Engine finished.")
        return dataset
    except Exception as e:
        print(
            f"--> FATAL: Failed to load dataset from cache even after download. Cache might be severely corrupted. Error: {e}")
        sys.exit(1)


def run_sft(config_path: str) -> None:
    """
    Main function to execute the SFT process.
    """
    print("--- [Bedrock] Initiating Supervised Fine-Tuning (SFT) ---")
    print(f"--> NOTE: Hugging Face endpoint is set to: {os.environ.get('HF_ENDPOINT')}")

    print(f"\n[Configuration] Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    dataset = load_dataset_robustly(config['dataset_name'], split="train")

    if 'dataset_subset_size_cpu' in config and int(config['dataset_subset_size_cpu']) > 0:
        subset_size = int(config['dataset_subset_size_cpu'])
        print(f"--> Using a subset of {subset_size} samples for this run.")
        dataset = dataset.select(range(subset_size))

    print(f"Dataset loaded with {len(dataset)} samples.")

    print(f"\n[Model Loading] Loading base model for SFT: {config['model_name_or_path']}")

    device_map = {"": "cpu"}
    torch_dtype = torch.float32
    attn_implementation = "sdpa"

    print("--> No GPU detected. Configuring for CPU-only execution.")

    model = AutoModelForCausalLM.from_pretrained(
        config['model_name_or_path'],
        device_map=device_map,
        trust_remote_code=True,
        attn_implementation=attn_implementation,
        torch_dtype=torch_dtype,
    )

    model.config.use_cache = False
    model.config.pretraining_tp = 1

    tokenizer = AutoTokenizer.from_pretrained(
        config['model_name_or_path'],
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    print("Model and tokenizer loaded successfully.")
    gc.collect()

    print("\n[Configuration] Configuring PEFT with LoRA...")
    peft_config = LoraConfig(
        r=int(config['lora_r']),
        lora_alpha=int(config['lora_alpha']),
        lora_dropout=float(config['lora_dropout']),
        target_modules=config['lora_target_modules'],
        bias="none",
        task_type="CAUSAL_LM",
    )
    print("PEFT configured.")

    print("[Configuration] Setting up training arguments...")
    # [API UPDATE FIX] Use SFTConfig to wrap training arguments for TRL
    training_args = SFTConfig(
        output_dir=config['output_dir'],
        num_train_epochs=int(config['num_train_epochs']),
        per_device_train_batch_size=int(config['per_device_train_batch_size']),
        gradient_accumulation_steps=int(config['gradient_accumulation_steps']),
        optim=config['optim'],
        save_steps=int(config['save_steps']),
        logging_steps=int(config['logging_steps']),
        learning_rate=float(config['learning_rate']),
        weight_decay=float(config['weight_decay']),
        fp16=False,
        bf16=False,
        max_grad_norm=float(config['max_grad_norm']),
        max_steps=int(config['max_steps']),
        warmup_ratio=float(config['warmup_ratio']),
        lr_scheduler_type=config['lr_scheduler_type'],
        report_to=config['report_to'],
        run_name=config['run_name'],
        # SFT-specific arguments now go into SFTConfig
        dataset_text_field=config['dataset_text_field'],
        max_seq_length=int(config['max_seq_length']),
        packing=True,
    )
    print("SFTConfig and training arguments set.")

    print("\n[Trainer Init] Initializing SFTTrainer...")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        peft_config=peft_config,
        args=training_args, # Pass the SFTConfig object here
    )

    print("\n--- SFT Training Started ---")
    trainer.train()
    print("\n--- SFT Training Finished ---")

    final_model_path = Path(config['output_dir']) / "final_model"
    os.makedirs(final_model_path, exist_ok=True)

    print(f"\n[Saving] Saving final adapter model to {final_model_path}...")
    trainer.save_model(str(final_model_path))
    tokenizer.save_pretrained(str(final_model_path))
    print("Model and tokenizer saved successfully.")

    print("\n--- [Bedrock] SFT Process Complete ---")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python src/trainers/sft_trainer.py <path_to_config.yaml>")
        sys.exit(1)

    config_file_path = sys.argv[1]
    run_sft(config_file_path)

# END OF FILE: src/trainers/sft_trainer.py

====================
文件: .\src\trainers\__init__.py
====================

# FILE: src/trainers/__init__.py
# Bedrock: This file makes the directory a Python package.

# To maintain modularity and prevent cascading import errors,
# this file is kept minimal. Individual trainer functions should be
# imported directly from their respective modules, e.g.,
# `from .sft_trainer import run_sft`.

====================
文件: .\src\utils\data_formatters.py
====================

# FILE: src/utils/data_formatters.py
"""
Bedrock Protocol: Centralized Data Formatting Utilities.
This module provides reusable functions for formatting datasets for various
training paradigms (SFT, DPO, ORPO, etc.), promoting code reuse and
decoupling trainers.
"""
from transformers import AutoTokenizer

def format_dpo_dataset_factory(tokenizer: AutoTokenizer):
    """
    Creates a formatting function for preference datasets (DPO, ORPO, RM).
    This function is designed to work with datasets that follow the TRL
    chat format, where 'chosen' and 'rejected' fields contain a list of
    message dictionaries.

    Args:
        tokenizer: The tokenizer to use for applying the chat template.

    Returns:
        A function that takes a dataset example and returns a dictionary
        with 'prompt', 'chosen', and 'rejected' as formatted strings.
    """
    def format_dpo_dataset(example: dict) -> dict:
        # The prompt is the conversation history *before* the final turn.
        prompt_messages = example['chosen'][:-1]
        # The chosen response is the full conversation including the final preferred response.
        chosen_messages = example['chosen']
        # The rejected response is the full conversation including the final rejected response.
        rejected_messages = example['rejected']

        prompt_str = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)
        chosen_str = tokenizer.apply_chat_template(chosen_messages, tokenize=False)
        rejected_str = tokenizer.apply_chat_template(rejected_messages, tokenize=False)

        return {
            "prompt": prompt_str,
            "chosen": chosen_str,
            "rejected": rejected_str,
        }
    return format_dpo_dataset

# END OF FILE: src/utils/data_formatters.py

====================
文件: .\src\utils\__init__.py
====================

# Bedrock: This file makes the directory a Python package.


====================
文件: .\wandb\run-20250720_183054-bhxdr6dx\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753007454
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_184140-mbn6jttr\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753008100
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_184718-zfwgmbv2\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753008438
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_190257-hfv4nedu\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753009377
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_190641-2a5id9w2\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753009601
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_191553-pqoo5wmq\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753010153
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_191928-thzchan2\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753010368
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_192023-z801iw56\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753010423
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_192329-n8tk764y\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1000
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 50000
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753010609
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_192600-y0ve7qr8\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753010760
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_194354-441nrq5u\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753011834
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_203907-vmzh55uw\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753015147
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_204438-nu3adish\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753015478
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_205009-xfjkvm1a\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753015809
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_205536-8liu1pgd\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753016136
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_205835-2c6j2k1i\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753016315
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_210329-halx3kur\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753016609
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_210850-g6m3yvak\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 11
eval_steps:
  desc: null
  value: 1
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753016930
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_211612-jwjidwgh\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 10
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 10
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753017372
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_212529-5w6zkgmp\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 10
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753017929
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_212839-hlxp0b33\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 10
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753018119
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_213223-nhe1sjl8\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 10
max_steps:
  desc: null
  value: 100
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753018343
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_213428-utsqymf3\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 10
max_steps:
  desc: null
  value: 10
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753018468
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 2
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


====================
文件: .\wandb\run-20250720_215055-2vr4s5bn\files\config.yaml
====================

wandb_version: 1

seed:
  desc: null
  value: 42
output_dir:
  desc: null
  value: ./checkpoints/pretrain_llm_demo
save_steps:
  desc: null
  value: 1
logging_steps:
  desc: null
  value: 10
eval_steps:
  desc: null
  value: 10
max_steps:
  desc: null
  value: 10
num_train_epochs:
  desc: null
  value: -1
model_config_path:
  desc: null
  value: configs/model/0.5B_dense.yaml
dataset_dir:
  desc: null
  value: ./data/processed/wikitext
dataset_text_field:
  desc: null
  value: text
max_seq_length:
  desc: null
  value: 1024
optimizer:
  desc: null
  value: adamw_torch
learning_rate:
  desc: null
  value: 3e-4
weight_decay:
  desc: null
  value: 0.01
adam_beta1:
  desc: null
  value: 0.9
adam_beta2:
  desc: null
  value: 0.95
adam_epsilon:
  desc: null
  value: 1e-8
max_grad_norm:
  desc: null
  value: 1.0
lr_scheduler_type:
  desc: null
  value: cosine
warmup_ratio:
  desc: null
  value: 0.01
per_device_train_batch_size:
  desc: null
  value: 2
gradient_accumulation_steps:
  desc: null
  value: 16
mixed_precision:
  desc: null
  value: bf16
deepspeed_config:
  desc: null
  value: ''
use_torch_compile:
  desc: null
  value: true
report_to:
  desc: null
  value: wandb
run_name:
  desc: null
  value: pretrain-0.5b-dense-demo
_wandb:
  desc: null
  value:
    python_version: 3.11.13
    cli_version: 0.17.0
    framework: huggingface
    huggingface_version: 4.41.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1753019455
    t:
      1:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      2:
      - 1
      - 11
      - 49
      - 51
      - 55
      - 71
      - 84
      - 98
      3:
      - 2
      - 13
      - 16
      - 23
      4: 3.11.13
      5: 0.17.0
      6: 4.41.2
      8:
      - 3
      - 5
      13: windows-amd64


