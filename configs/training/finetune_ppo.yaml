# FILE: configs/training/finetune_ppo.yaml
# Bedrock Protocol: Configuration for Proximal Policy Optimization (PPO).
# MODIFIED FOR CPU-ONLY EXECUTION & SPEED & MINIMAL MEMORY

# --- Model & Tokenizer ---
# PPO starts from a supervised fine-tuned (SFT) model.
model_name_or_path: "./checkpoints/sft-tinyllama-guanaco-cpu/final_model" # Points to the CPU-trained adapter
use_fast_tokenizer: true

# --- Dataset ---
# PPO requires a dataset of prompts (queries).
dataset_name: "lvwerra/stack-exchange-paired"
dataset_text_field: "question" # Column containing prompts/questions
dataset_subset_size: 10 # [MODIFIED] Reduced to absolute minimum for memory test

# --- PPO Specific Parameters (TRL PPOConfig) ---
ppo_steps: 2 # [MODIFIED] Run only 2 steps for a quick test
learning_rate: 1.41e-5 # Common starting point for PPO
batch_size: 1 # [MODIFIED] Batch size per device for collected experiences.
mini_batch_size: 1
gradient_accumulation_steps: 1
ppo_epochs: 4
init_kl_coef: 0.2
target_kl: 0.1
adap_kl_ctrl: true

# --- Generation Parameters for PPO Rollouts ---
max_new_tokens: 32 # Reduced for CPU run
min_output_length: 8
max_output_length: 64

# --- General Training & Experiment Tracking ---
output_dir: "./checkpoints/ppo-tinyllama-guanaco-cpu" # Changed output dir
seed: 42
bf16: false # [MODIFIED] Disabled for CPU
fp16: false # [MODIFIED] Disabled for CPU

# --- PEFT Configuration (LoRA for PPO Actor) ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Experiment Tracking ---
report_to: "none" # Set to "none" for quick CPU test runs.
run_name: "ppo-tinyllama-guanaco-demo-cpu"

# END OF FILE: configs/training/finetune_ppo.yaml