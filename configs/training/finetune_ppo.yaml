# FILE: configs/training/finetune_ppo.yaml
# Bedrock Protocol: Configuration for Proximal Policy Optimization (PPO).
# MODIFIED FOR CLEAN LOGS & CPU EXECUTION

# --- Model & Tokenizer ---
model_name_or_path: "./checkpoints/sft-tinyllama-guanaco-cpu/final_model" # Points to the CPU-trained adapter
use_fast_tokenizer: true

# --- Dataset ---
dataset_name: "imdb"
dataset_text_field: "text"
# [MODIFIED] Reduced subset size to accommodate larger batch size in RAM
dataset_subset_size: 20

# --- PPO Specific Parameters (TRL PPOConfig) ---
# [MODIFIED] Increased steps slightly for more meaningful run
ppo_steps: 4
learning_rate: 1.41e-5
# [MODIFIED] Increased batch_size to >= 2 to eliminate the 'std' UserWarning
batch_size: 2
mini_batch_size: 1
gradient_accumulation_steps: 1
ppo_epochs: 4
init_kl_coef: 0.2
target_kl: 0.1
adap_kl_ctrl: true

# --- Generation Parameters for PPO Rollouts ---
max_new_tokens: 32
min_output_length: 8
max_output_length: 64

# --- General Training & Experiment Tracking ---
output_dir: "./checkpoints/ppo-tinyllama-guanaco-cpu"
seed: 42
bf16: false
fp16: false

# --- PEFT Configuration (LoRA for PPO Actor) ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Experiment Tracking ---
run_name: "ppo-tinyllama-guanaco-demo-cpu"

# END OF FILE: configs/training/finetune_ppo.yaml