# FILE: configs/training/finetune_dpo.yaml
# Bedrock Protocol: Configuration for Direct Preference Optimization (DPO).
# MODIFIED FOR CPU-ONLY EXECUTION & SPEED & MINIMAL MEMORY

# --- Model & Tokenizer ---
# DPO starts from a supervised fine-tuned (SFT) model.
# This path should point to the SFT model trained on CPU.
model_name_or_path: "./checkpoints/sft-tinyllama-guanaco-cpu/final_model" # Points to the CPU-trained adapter
use_fast_tokenizer: true

# --- Dataset ---
dataset_name: "trl-internal-testing/hh-rlhf-trl-style"
dataset_subset_size: 10 # [MODIFIED] Reduced to absolute minimum for memory test

# --- DPO Specific Parameters ---
beta: 0.1

# --- Training Arguments ---
output_dir: "./checkpoints/dpo-tinyllama-guanaco-cpu" # Changed output dir for CPU run
num_train_epochs: 1
per_device_train_batch_size: 1      # [MODIFIED] Reduced for CPU memory
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1      # [MODIFIED] Reduced to absolute minimum
optim: "adamw_torch"                # [MODIFIED] Switched to standard CPU-compatible optimizer
learning_rate: 5e-5
weight_decay: 0.001
fp16: false                         # [MODIFIED] Disabled for CPU
bf16: false                         # [MODIFIED] Disabled for CPU
max_grad_norm: 1.0
logging_steps: 1                    # Log every step
# [MODIFIED FOR CPU SPEED] Stop after only 2 training steps.
max_steps: 2
max_length: 256                     # Reduced for memory
max_prompt_length: 128              # Reduced for memory
max_target_length: 128              # Reduced for memory
warmup_ratio: 0.1
lr_scheduler_type: "linear"

# --- PEFT is still used for DPO ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# --- Experiment Tracking ---
report_to: "none" # Set to "none" for quick CPU test runs.
run_name: "dpo-tinyllama-guanaco-demo-cpu"

# END OF FILE: configs/training/finetune_dpo.yaml