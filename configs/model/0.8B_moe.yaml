# FILE: configs/model/0.8B_moe.yaml
# Bedrock Protocol: Configuration for a 0.8 Billion (approx) Mixture-of-Experts (MoE) LLM.
# This defines the architectural hyperparameters, showcasing MoE integration.

model_type: "MoELLM" # Custom identifier for our internal MoE model builder

# --- Core Transformer Architecture Parameters (Base for MoE) ---
vocab_size: 32000
hidden_size: 1024
# For MoE, intermediate_size defines the size of each *expert's* FFN.
intermediate_size: 4096
num_hidden_layers: 24
num_attention_heads: 16
num_key_value_heads: 16
hidden_act: "silu"
max_position_embeddings: 2048

# --- Attention Specifics ---
attention_type: "standard" # or "flash"

# --- MoE Specific Parameters ---
num_experts: 8             # Total number of experts in each MoE layer.
num_experts_per_tok: 2     # Number of experts to activate per token. (k)
router_aux_loss_coef: 0.001 # Coefficient for the router auxiliary loss (load balancing).

# --- Tokenizer Configuration (for model loading context) ---
tokenizer_path: "./data/tokenizers/wikitext_spm"

# END OF FILE: configs/model/0.8B_moe.yaml