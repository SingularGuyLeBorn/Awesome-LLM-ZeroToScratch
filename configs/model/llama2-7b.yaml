# FILE: configs/model/llama2-7b.yaml
# Bedrock Protocol: Model configuration template for a LLaMA-class model.
# While we load most configurations directly from the Hugging Face hub,
# this file serves as a template for specifying overrides or custom parameters.

model_name_or_path: "meta-llama/Llama-2-7b-hf" # Placeholder, will be overridden by training config
model_type: "LlamaForCausalLM"

# --- Key Architectural Parameters ---
# These are typically defined by the pre-trained model and not changed during fine-tuning.
vocab_size: 32000
hidden_size: 4096
intermediate_size: 11008
num_hidden_layers: 32
num_attention_heads: 32
num_key_value_heads: 32
hidden_act: "silu"
max_position_embeddings: 4096 # Can be extended via RoPE scaling, etc.
rope_theta: 10000.0
tie_word_embeddings: false

# --- Quantization Configuration ---
# Example for loading with bitsandbytes for lower memory usage.
quantization_config:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16" # torch.bfloat16
  bnb_4bit_use_double_quant: true

# --- Attention Implementation ---
# Use 'flash_attention_2' for significant speedup and memory savings on compatible hardware.
attn_implementation: "flash_attention_2" # or "eager" or "sdpa"

# END OF FILE: configs/model/llama2-7b.yaml